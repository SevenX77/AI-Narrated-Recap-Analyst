"""
Project Migration Workflow
é¡¹ç›®è¿ç§»ä¸æ•°æ®å¤„ç†å·¥ä½œæµ
"""

import os
import json
import shutil
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple

from src.core.interfaces import BaseWorkflow
from src.core.config import config
from src.tools.novel_processor import NovelSegmentationTool
from src.tools.novel_chapter_processor import NovelChapterProcessor, MetadataExtractor
from src.tools.introduction_validator import IntroductionValidator
from src.tools.srt_processor import SrtScriptProcessor

logger = logging.getLogger(__name__)


class ProjectMigrationWorkflow(BaseWorkflow):
    """
    é¡¹ç›®è¿ç§»å·¥ä½œæµ
    
    åŠŸèƒ½ï¼š
    1. ä» åˆ†æèµ„æ–™/ è¿ç§»é¡¹ç›®åˆ°æ ‡å‡†åŒ–çš„ data/projects/ ç»“æ„
    2. é‡æ„ç›®å½•ï¼šwith_novel / without_novel
    3. å¤„ç†å°è¯´æ–‡æœ¬ï¼šåˆ†æ®µã€æ ¼å¼åŒ–
    4. å¤„ç†å­—å¹•æ–‡ä»¶ï¼šç¼–ç è§„èŒƒåŒ–
    5. æ›´æ–° project_index.json
    6. ç”Ÿæˆè¿ç§»æŠ¥å‘Š
    """
    
    name = "project_migration"
    
    # é¡¹ç›®æ˜ å°„é…ç½®
    PROJECT_MAPPING = {
        "with_novel": {
            "æœ«å“¥è¶…å‡¡å…¬è·¯": {
                "old_id": "PROJ_002",
                "source": "åˆ†æèµ„æ–™/æœ‰åŸå°è¯´/01_æœ«å“¥è¶…å‡¡å…¬è·¯",
                "purpose": "alignment_writer_training",
                "is_ground_truth": True,
                "is_explosive": False
            },
            "å¤©å‘½æ¡ƒèŠ±": {
                "old_id": "PROJ_004",
                "source": "åˆ†æèµ„æ–™/æœ‰åŸå°è¯´/02_å¤©å‘½æ¡ƒèŠ±",
                "purpose": "alignment_writer_training",
                "is_ground_truth": False,
                "is_explosive": False
            },
            "æ°¸å¤œæ‚”æ¨å½•": {
                "old_id": "PROJ_003",
                "source": "åˆ†æèµ„æ–™/æœ‰åŸå°è¯´/03_æ°¸å¤œæ‚”æ¨å½•",
                "purpose": "alignment_writer_training",
                "is_ground_truth": False,
                "is_explosive": False
            }
        },
        "without_novel": {
            "è¶…å‰å´›èµ·": {
                "old_id": "PROJ_001",
                "source": "åˆ†æèµ„æ–™/æ²¡æœ‰åŸå°è¯´/04_è¶…å‰å´›èµ·",
                "purpose": "script_analysis_hit_pattern",
                "is_ground_truth": False,
                "is_explosive": False
            },
            "æœ«ä¸–å¯’æ½®": {
                "old_id": "PROJ_005",
                "source": "åˆ†æèµ„æ–™/æ²¡æœ‰åŸå°è¯´/05_æœ«ä¸–å¯’æ½®",
                "purpose": "script_analysis_hit_pattern",
                "is_ground_truth": False,
                "is_explosive": False
            }
        }
    }
    
    def __init__(self, use_llm: bool = True, dry_run: bool = False):
        """
        Args:
            use_llm: æ˜¯å¦åœ¨å°è¯´å¤„ç†ä¸­ä½¿ç”¨ LLM è¾…åŠ©
            dry_run: æ˜¯å¦ä¸ºè¯•è¿è¡Œï¼ˆä¸å®é™…å†™å…¥æ–‡ä»¶ï¼‰
        """
        super().__init__()
        self.use_llm = use_llm
        self.dry_run = dry_run
        self.novel_tool = NovelSegmentationTool(use_llm=use_llm)
        self.chapter_processor = NovelChapterProcessor(chapters_per_file=10)
        self.metadata_extractor = MetadataExtractor(use_llm=True)  # å¯ç”¨LLMæ™ºèƒ½è¿‡æ»¤
        self.intro_validator = IntroductionValidator()  # LLMéªŒè¯å™¨
        self.srt_processor = SrtScriptProcessor(use_llm=use_llm)  # SRTå¤„ç†å™¨
        
        self.base_dir = Path(config.base_dir)
        self.data_dir = Path(config.data_dir)
        self.source_dir = Path(config.analysis_source_dir)
        
        self.migration_report = {
            "start_time": None,
            "end_time": None,
            "projects_migrated": 0,
            "files_processed": {
                "novels": 0,
                "srt_files": 0,
                "srt_scripts_processed": 0,
                "total_size_mb": 0
            },
            "novel_processing": {},
            "errors": []
        }
    
    async def run(self, **kwargs) -> Dict[str, Any]:
        """
        æ‰§è¡Œè¿ç§»å·¥ä½œæµ
        
        Returns:
            è¿ç§»æŠ¥å‘Š
        """
        logger.info(f"Starting migration workflow (use_llm={self.use_llm}, dry_run={self.dry_run})")
        self.migration_report["start_time"] = datetime.now().isoformat()
        
        try:
            # æ­¥éª¤ 1: éªŒè¯æºç›®å½•
            self._validate_sources()
            
            # æ­¥éª¤ 2: è¿ç§» with_novel é¡¹ç›®
            for project_name, project_info in self.PROJECT_MAPPING["with_novel"].items():
                await self._migrate_with_novel(project_name, project_info)
            
            # æ­¥éª¤ 3: è¿ç§» without_novel é¡¹ç›®
            for project_name, project_info in self.PROJECT_MAPPING["without_novel"].items():
                await self._migrate_without_novel(project_name, project_info)
            
            # æ­¥éª¤ 4: æ›´æ–° project_index.json
            self._update_project_index()
            
            # æ­¥éª¤ 5: ç”ŸæˆæŠ¥å‘Š
            self._finalize_report()
            
            logger.info(f"Migration completed: {self.migration_report['projects_migrated']} projects")
            return self.migration_report
        
        except Exception as e:
            logger.error(f"Migration workflow failed: {e}", exc_info=True)
            self.migration_report["errors"].append({
                "stage": "workflow",
                "error": str(e)
            })
            raise
    
    def _validate_sources(self):
        """éªŒè¯æºç›®å½•å­˜åœ¨"""
        if not self.source_dir.exists():
            raise FileNotFoundError(f"Source directory not found: {self.source_dir}")
        
        logger.info(f"Source directory validated: {self.source_dir}")
    
    async def _migrate_with_novel(self, project_name: str, project_info: Dict[str, Any]):
        """
        è¿ç§»æœ‰å°è¯´çš„é¡¹ç›®
        
        Args:
            project_name: é¡¹ç›®åç§°ï¼ˆä¸­æ–‡ï¼‰
            project_info: é¡¹ç›®é…ç½®ä¿¡æ¯
        """
        logger.info(f"Migrating with_novel project: {project_name}")
        
        try:
            # 1. åˆ›å»ºç›®æ ‡ç›®å½•
            target_dir = self.data_dir / "projects" / "with_novel" / project_name
            raw_dir = target_dir / "raw"
            novel_dir = target_dir / "novel"  # æ–°å¢ï¼šå¤„ç†åçš„å°è¯´ç›®å½•
            script_dir = target_dir / "script"  # æ–°å¢ï¼šå¤„ç†åçš„è„šæœ¬ç›®å½•
            
            if not self.dry_run:
                raw_dir.mkdir(parents=True, exist_ok=True)
                novel_dir.mkdir(parents=True, exist_ok=True)  # æ–°å¢
                script_dir.mkdir(parents=True, exist_ok=True)  # æ–°å¢
                (target_dir / "alignment").mkdir(exist_ok=True)
                (target_dir / "analysis").mkdir(exist_ok=True)
                (target_dir / "ground_truth").mkdir(exist_ok=True)
            
            # 2. å¤„ç†å°è¯´æ–‡ä»¶
            source_path = self.base_dir / project_info["source"]
            novel_source = source_path / "novel"
            
            # æŸ¥æ‰¾å°è¯´æ–‡ä»¶
            novel_files = list(novel_source.glob("*.txt"))
            if not novel_files:
                logger.warning(f"No novel file found for {project_name}")
                return
            
            novel_file = novel_files[0]  # å–ç¬¬ä¸€ä¸ª .txt æ–‡ä»¶
            logger.info(f"Processing novel: {novel_file.name}")
            
            # è¯»å–åŸå§‹å°è¯´
            with open(novel_file, "r", encoding="utf-8") as f:
                original_text = f.read()
            
            file_size_mb = len(original_text.encode("utf-8")) / (1024 * 1024)
            
            if not self.dry_run:
                # ä¿å­˜åŸå§‹æ–‡ä»¶åˆ° raw/
                raw_novel_path = raw_dir / "novel.txt"
                with open(raw_novel_path, "w", encoding="utf-8") as f:
                    f.write(original_text)
                logger.info(f"Saved raw novel: {raw_novel_path}")
                
                # å¤„ç†åˆ†æ®µ
                logger.info(f"Starting novel segmentation for {project_name}")
                result = self.novel_tool.execute(original_text, preserve_metadata=True)
                processed_text = result.paragraphs[0]
                
                # å…ˆæå–å…ƒæ•°æ®ï¼ˆåŒ…å«æ™ºèƒ½è¿‡æ»¤çš„ç®€ä»‹ï¼‰
                logger.info(f"Extracting metadata for {project_name}")
                extracted_metadata = self.metadata_extractor.execute(original_text)
                filtered_introduction = extracted_metadata["novel"]["introduction"]
                
                # éªŒè¯è¿‡æ»¤è´¨é‡ï¼ˆæ··åˆç­–ç•¥éªŒè¯ï¼‰
                logger.info(f"Validating introduction quality for {project_name}")
                original_intro = self._extract_original_introduction(original_text)
                validation_result = self.intro_validator.execute(
                    original_introduction=original_intro,
                    filtered_introduction=filtered_introduction,
                    novel_title=extracted_metadata["novel"].get("title", project_name)
                )
                
                # ä½¿ç”¨éªŒè¯åçš„ç®€ä»‹ï¼ˆå¯èƒ½å·²ä¿®å¤criticalé—®é¢˜ï¼‰
                final_introduction = validation_result.filtered_introduction
                
                # è®°å½•éªŒè¯ç»“æœ
                validation_summary = {
                    "is_valid": validation_result.is_valid,
                    "quality_score": validation_result.quality_score,
                    "issues_count": len(validation_result.issues),
                    "critical_issues": sum(1 for i in validation_result.issues if i.severity == "critical"),
                    "rule_suggestions": validation_result.rule_suggestions
                }
                logger.info(f"Validation result: {validation_result}")
                
                # ç« èŠ‚å¤„ç†ï¼šæ‹†åˆ†ä¸º chpt_0000.txt, chpt_0001-0010.txt, ...
                logger.info(f"Starting chapter processing for {project_name}")
                chapter_report = self.chapter_processor.execute(
                    processed_text, 
                    novel_dir,
                    introduction_override=final_introduction  # ä½¿ç”¨éªŒè¯åçš„ç®€ä»‹
                )
                logger.info(f"Created {len(chapter_report['chapter_files'])} chapter files")
                
                # æ›´æ–°è¿ç§»æŠ¥å‘Š
                self.migration_report["novel_processing"][project_name] = {
                    **result.stats,
                    "chapters": chapter_report,
                    "validation": validation_summary  # æ·»åŠ éªŒè¯ç»“æœ
                }
            
            self.migration_report["files_processed"]["novels"] += 1
            self.migration_report["files_processed"]["total_size_mb"] += file_size_mb
            
            # 3. å¤åˆ¶ SRT æ–‡ä»¶åˆ° raw/
            srt_source = source_path / "srt"
            if srt_source.exists():
                srt_count = await self._copy_srt_files(srt_source, raw_dir)
                self.migration_report["files_processed"]["srt_files"] += srt_count
                
                # 4. å¤„ç† SRT æ–‡ä»¶ -> script/
                if not self.dry_run:
                    logger.info(f"Processing SRT files for {project_name}")
                    
                    # è¯»å–å‰3ç« å°è¯´ä½œä¸ºå‚è€ƒ
                    novel_reference = self._load_novel_reference(novel_dir, chapters=3)
                    
                    # å¤„ç†æ¯ä¸ªSRTæ–‡ä»¶
                    srt_files = sorted(raw_dir.glob("*.srt"))
                    for srt_file in srt_files:
                        try:
                            logger.info(f"Processing {srt_file.name}")
                            srt_report = self.srt_processor.execute(
                                srt_file_path=srt_file,
                                output_dir=script_dir,
                                novel_reference=novel_reference,
                                episode_name=srt_file.stem
                            )
                            self.migration_report["files_processed"]["srt_scripts_processed"] += 1
                            logger.info(f"Processed {srt_file.name} -> {srt_report['output_file']}")
                        except Exception as e:
                            logger.error(f"Failed to process SRT {srt_file.name}: {e}", exc_info=True)
                            self.migration_report["errors"].append({
                                "project": project_name,
                                "file": srt_file.name,
                                "error": str(e)
                            })
            
            # 4. åˆ›å»ºå…ƒæ•°æ®æ–‡ä»¶
            if not self.dry_run:
                metadata = {
                    "project_name": project_name,
                    "old_project_id": project_info["old_id"],
                    "category": "with_novel",
                    "source_path": str(source_path),
                    "migrated_at": datetime.now().isoformat(),
                    "raw_novel_file": novel_file.name,
                    **extracted_metadata  # åŒ…å« novel: {title, author, tags, introduction}
                }
                
                # æ·»åŠ ç« èŠ‚æ–‡ä»¶ä¿¡æ¯
                if "novel" in metadata:
                    metadata["novel"]["chapters"] = {
                        "total": chapter_report["total_chapters"],
                        "files": {
                            filename: f"ç¬¬{i*10+1}-{min((i+1)*10, chapter_report['total_chapters'])}ç« "
                            for i, filename in enumerate(chapter_report["chapter_files"])
                        }
                    }
                    # æ·»åŠ ç®€ä»‹æ–‡ä»¶
                    metadata["novel"]["chapters"]["files"]["chpt_0000.txt"] = "ç®€ä»‹"
                
                metadata_path = target_dir / "metadata.json"
                with open(metadata_path, "w", encoding="utf-8") as f:
                    json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            self.migration_report["projects_migrated"] += 1
            logger.info(f"Successfully migrated {project_name}")
        
        except Exception as e:
            logger.error(f"Failed to migrate {project_name}: {e}", exc_info=True)
            self.migration_report["errors"].append({
                "project": project_name,
                "error": str(e)
            })
    
    async def _migrate_without_novel(self, project_name: str, project_info: Dict[str, Any]):
        """
        è¿ç§»æ²¡æœ‰å°è¯´çš„é¡¹ç›®
        
        Args:
            project_name: é¡¹ç›®åç§°
            project_info: é¡¹ç›®é…ç½®
        """
        logger.info(f"Migrating without_novel project: {project_name}")
        
        try:
            # 1. åˆ›å»ºç›®æ ‡ç›®å½•
            target_dir = self.data_dir / "projects" / "without_novel" / project_name
            raw_dir = target_dir / "raw"
            script_dir = target_dir / "script"  # æ–°å¢ï¼šå¤„ç†åçš„è„šæœ¬ç›®å½•
            
            if not self.dry_run:
                raw_dir.mkdir(parents=True, exist_ok=True)
                script_dir.mkdir(parents=True, exist_ok=True)  # æ–°å¢
                (target_dir / "analysis").mkdir(exist_ok=True)
                (target_dir / "ground_truth").mkdir(exist_ok=True)
            
            # 2. å¤åˆ¶ SRT æ–‡ä»¶åˆ° raw/
            source_path = self.base_dir / project_info["source"]
            srt_source = source_path / "srt"
            
            if srt_source.exists():
                srt_count = await self._copy_srt_files(srt_source, raw_dir)
                self.migration_report["files_processed"]["srt_files"] += srt_count
                
                # 3. å¤„ç† SRT æ–‡ä»¶ -> script/ï¼ˆæ— å°è¯´å‚è€ƒæ¨¡å¼ï¼‰
                if not self.dry_run:
                    logger.info(f"Processing SRT files for {project_name} (without novel reference)")
                    
                    # å¤„ç†æ¯ä¸ªSRTæ–‡ä»¶ï¼ˆæ— å°è¯´å‚è€ƒï¼‰
                    srt_files = sorted(raw_dir.glob("*.srt"))
                    for srt_file in srt_files:
                        try:
                            logger.info(f"Processing {srt_file.name}")
                            srt_report = self.srt_processor.execute(
                                srt_file_path=srt_file,
                                output_dir=script_dir,
                                novel_reference=None,  # æ— å°è¯´å‚è€ƒ
                                episode_name=srt_file.stem
                            )
                            self.migration_report["files_processed"]["srt_scripts_processed"] += 1
                            logger.info(f"Processed {srt_file.name} -> {srt_report['output_file']}")
                        except Exception as e:
                            logger.error(f"Failed to process SRT {srt_file.name}: {e}", exc_info=True)
                            self.migration_report["errors"].append({
                                "project": project_name,
                                "file": srt_file.name,
                                "error": str(e)
                            })
            else:
                logger.warning(f"No SRT directory found for {project_name}")
            
            # 3. åˆ›å»ºå…ƒæ•°æ®
            if not self.dry_run:
                metadata = {
                    "project_name": project_name,
                    "old_project_id": project_info["old_id"],
                    "category": "without_novel",
                    "source_path": str(source_path),
                    "migrated_at": datetime.now().isoformat()
                }
                
                metadata_path = target_dir / "metadata.json"
                with open(metadata_path, "w", encoding="utf-8") as f:
                    json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            self.migration_report["projects_migrated"] += 1
            logger.info(f"Successfully migrated {project_name}")
        
        except Exception as e:
            logger.error(f"Failed to migrate {project_name}: {e}", exc_info=True)
            self.migration_report["errors"].append({
                "project": project_name,
                "error": str(e)
            })
    
    async def _copy_srt_files(self, source_dir: Path, target_dir: Path) -> int:
        """
        å¤åˆ¶å¹¶å¤„ç† SRT æ–‡ä»¶
        
        Args:
            source_dir: SRT æºç›®å½•
            target_dir: ç›®æ ‡ç›®å½•
        
        Returns:
            å¤åˆ¶çš„æ–‡ä»¶æ•°é‡
        """
        srt_files = sorted(source_dir.glob("*.srt"))
        
        for srt_file in srt_files:
            if not self.dry_run:
                # è¯»å–åŸå§‹å†…å®¹
                with open(srt_file, "r", encoding="utf-8", errors="ignore") as f:
                    content = f.read()
                
                # è§„èŒƒåŒ–ç¼–ç å’Œæ¢è¡Œç¬¦
                content = content.replace("\r\n", "\n")
                
                # å†™å…¥ç›®æ ‡
                target_path = target_dir / srt_file.name
                with open(target_path, "w", encoding="utf-8") as f:
                    f.write(content)
                
                logger.info(f"Copied SRT: {srt_file.name}")
        
        return len(srt_files)
    
    def _update_project_index(self):
        """æ›´æ–° project_index.json"""
        logger.info("Updating project_index.json")
        
        index_path = self.data_dir / "project_index.json"
        
        # è¯»å–æ—§çš„ç´¢å¼•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        old_index = {}
        if index_path.exists():
            with open(index_path, "r", encoding="utf-8") as f:
                old_index = json.load(f)
        
        # æ„å»ºæ–°ç´¢å¼•
        new_index = {
            "version": "2.0",
            "updated_at": datetime.now().isoformat(),
            "projects": {
                "with_novel": {},
                "without_novel": {}
            },
            "migration_history": {
                "v1_to_v2": {
                    "date": datetime.now().strftime("%Y-%m-%d"),
                    "archived_to": "data/projects_archive_20260205",
                    "reason": "é‡æ„é¡¹ç›®ç»“æ„ï¼Œåˆ†ç¦»æœ‰æ— åŸå°è¯´é¡¹ç›®ï¼Œå®ç°å°è¯´è‡ªç„¶åˆ†æ®µå¤„ç†"
                }
            }
        }
        
        # å¡«å……é¡¹ç›®ä¿¡æ¯
        for category in ["with_novel", "without_novel"]:
            for project_name, project_info in self.PROJECT_MAPPING[category].items():
                old_id = project_info["old_id"]
                
                # ä»æ—§ç´¢å¼•è·å–ä¿¡æ¯
                old_project_data = {}
                if "projects" in old_index and old_id in old_index["projects"]:
                    old_project_data = old_index["projects"][old_id]
                
                # è·å– episodes åˆ—è¡¨
                episodes = self._get_episodes(project_name, category)
                
                new_index["projects"][category][project_name] = {
                    "id": old_id,
                    "name": project_name,
                    "category": category,
                    "purpose": project_info["purpose"],
                    "source_path": str(self.base_dir / project_info["source"]),
                    "created_at": old_project_data.get("created_at", datetime.now().isoformat()),
                    "migrated_at": datetime.now().isoformat(),
                    "status": "migrated",
                    "is_ground_truth": project_info["is_ground_truth"],
                    "is_explosive": project_info["is_explosive"],
                    "heat_score": old_project_data.get("heat_score"),
                    "episodes": episodes,
                    "notes": old_project_data.get("notes", "")
                }
                
                # æ·»åŠ å°è¯´å¤„ç†ä¿¡æ¯
                if category == "with_novel" and project_name in self.migration_report["novel_processing"]:
                    new_index["projects"][category][project_name]["novel_processing"] = {
                        "method": "rule_llm_hybrid" if self.use_llm else "rule_only",
                        "processed_at": datetime.now().isoformat(),
                        "stats": self.migration_report["novel_processing"][project_name]
                    }
        
        # ä¿ç•™å…ƒæ•°æ®å®šä¹‰
        if "heat_score_definition" in old_index:
            new_index["heat_score_definition"] = old_index["heat_score_definition"]
        if "is_explosive_definition" in old_index:
            new_index["is_explosive_definition"] = old_index["is_explosive_definition"]
        
        # å†™å…¥æ–°ç´¢å¼•
        if not self.dry_run:
            with open(index_path, "w", encoding="utf-8") as f:
                json.dump(new_index, f, ensure_ascii=False, indent=2)
            logger.info(f"Updated project_index.json")
    
    def _load_novel_reference(self, novel_dir: Path, chapters: int = 3) -> str:
        """
        åŠ è½½å°è¯´å‚è€ƒæ–‡æœ¬ï¼ˆç”¨äºSRTå¤„ç†ï¼‰
        
        Args:
            novel_dir: å°è¯´ç›®å½•ï¼ˆåŒ…å«chpt_*.txtæ–‡ä»¶ï¼‰
            chapters: åŠ è½½å‰Nç« ä½œä¸ºå‚è€ƒï¼ˆé»˜è®¤3ç« ï¼‰
        
        Returns:
            åˆå¹¶çš„å°è¯´å‚è€ƒæ–‡æœ¬
        """
        try:
            # è¯»å–ç®€ä»‹
            intro_file = novel_dir / "chpt_0000.txt"
            reference_parts = []
            
            if intro_file.exists():
                with open(intro_file, 'r', encoding='utf-8') as f:
                    reference_parts.append(f.read())
            
            # è¯»å–å‰Nç« 
            chapter_files = sorted(novel_dir.glob("chpt_*.txt"))
            loaded = 0
            
            for chapter_file in chapter_files:
                if chapter_file.name == "chpt_0000.txt":
                    continue  # è·³è¿‡ç®€ä»‹
                
                if loaded >= chapters:
                    break
                
                with open(chapter_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    # é™åˆ¶æ¯ç« çš„é•¿åº¦ï¼ˆé¿å…contextè¿‡é•¿ï¼‰
                    if len(content) > 3000:
                        content = content[:3000] + "..."
                    reference_parts.append(content)
                    loaded += 1
            
            reference_text = "\n\n".join(reference_parts)
            logger.info(f"Loaded novel reference: {len(reference_text)} chars from {loaded} chapters")
            return reference_text
        
        except Exception as e:
            logger.warning(f"Failed to load novel reference: {e}")
            return ""
    
    def _extract_original_introduction(self, novel_text: str) -> str:
        """æå–åŸå§‹ç®€ä»‹ï¼ˆåŒ…å«å…ƒä¿¡æ¯ï¼‰"""
        lines = novel_text.split('\n')
        intro_lines = []
        in_introduction = False
        
        for line in lines:
            if line.strip() == 'ç®€ä»‹:':
                in_introduction = True
                continue
            
            if '========' in line or line.startswith('==='):
                if in_introduction:
                    break
                continue
            
            if in_introduction and line.strip():
                intro_lines.append(line.strip())
        
        return '\n'.join(intro_lines)
    
    def _get_episodes(self, project_name: str, category: str) -> List[str]:
        """è·å–é¡¹ç›®çš„é›†æ•°åˆ—è¡¨"""
        project_dir = self.data_dir / "projects" / category / project_name / "raw"
        
        if not project_dir.exists():
            return []
        
        srt_files = sorted(project_dir.glob("ep*.srt"))
        episodes = [f.stem for f in srt_files]  # ep01, ep02, ...
        
        return episodes
    
    def _finalize_report(self):
        """å®ŒæˆæŠ¥å‘Š"""
        self.migration_report["end_time"] = datetime.now().isoformat()
        
        # ä¿å­˜æŠ¥å‘Š
        if not self.dry_run:
            report_path = self.data_dir / "migration_report_20260205.json"
            with open(report_path, "w", encoding="utf-8") as f:
                json.dump(self.migration_report, f, ensure_ascii=False, indent=2)
            logger.info(f"Migration report saved: {report_path}")


async def main():
    """ä¸»å‡½æ•°ï¼šæ‰§è¡Œè¿ç§»"""
    import asyncio
    
    # åˆ›å»ºå·¥ä½œæµ
    workflow = ProjectMigrationWorkflow(use_llm=True, dry_run=False)
    
    # æ‰§è¡Œè¿ç§»
    report = await workflow.run()
    
    # æ‰“å°æ‘˜è¦
    print("\n" + "="*60)
    print("ğŸ“Š è¿ç§»å®Œæˆæ‘˜è¦")
    print("="*60)
    print(f"âœ… é¡¹ç›®è¿ç§»æ•°é‡: {report['projects_migrated']}")
    print(f"ğŸ“– å°è¯´æ–‡ä»¶å¤„ç†: {report['files_processed']['novels']}")
    print(f"ğŸ“ å­—å¹•æ–‡ä»¶å¤åˆ¶: {report['files_processed']['srt_files']}")
    print(f"ğŸ’¾ æ€»æ•°æ®å¤§å°: {report['files_processed']['total_size_mb']:.2f} MB")
    
    if report["errors"]:
        print(f"\nâš ï¸  é”™è¯¯æ•°é‡: {len(report['errors'])}")
        for error in report["errors"]:
            print(f"  - {error}")
    
    print("\n" + "="*60)


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
