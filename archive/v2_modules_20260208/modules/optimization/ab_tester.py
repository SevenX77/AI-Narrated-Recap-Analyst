"""
A/Bæµ‹è¯•æ¡†æ¶ (A/B Tester)

ç”¨äºå¯¹æ¯”æ–°æ—§Promptçš„æ€§èƒ½ï¼š
1. ä½¿ç”¨æ—§Promptè¿è¡Œå¯¹é½ â†’ ç»“æœA
2. ä½¿ç”¨æ–°Promptè¿è¡Œå¯¹é½ â†’ ç»“æœB
3. å¯¹æ¯”ç»“æœAå’Œç»“æœB
4. å†³ç­–æ˜¯å¦é‡‡ç”¨æ–°Prompt
"""

import json
import logging
from typing import Dict, List, Optional
from pathlib import Path
from datetime import datetime
from dataclasses import dataclass

from src.core.schemas import PromptVersion, OptimizationRound, AlignmentAnnotation
from src.modules.optimization.heat_calculator import HeatCalculator

logger = logging.getLogger(__name__)


@dataclass
class ABTestResult:
    """A/Bæµ‹è¯•ç»“æœ"""
    old_version: str
    new_version: str
    
    # å¯¹é½ç»“æœè·¯å¾„
    old_alignment_path: str
    new_alignment_path: str
    
    # æ€§èƒ½æŒ‡æ ‡
    old_overall_score: float
    new_overall_score: float
    old_layer_scores: Dict[str, float]
    new_layer_scores: Dict[str, float]
    
    # æ ‡æ³¨æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
    old_annotations: Optional[List[AlignmentAnnotation]] = None
    new_annotations: Optional[List[AlignmentAnnotation]] = None
    old_total_heat: Optional[float] = None
    new_total_heat: Optional[float] = None
    
    # å†³ç­–
    is_better: bool = False
    improvement_percentage: float = 0.0
    adoption_reason: str = ""
    
    def to_dict(self) -> Dict:
        return {
            "old_version": self.old_version,
            "new_version": self.new_version,
            "old_alignment_path": self.old_alignment_path,
            "new_alignment_path": self.new_alignment_path,
            "performance": {
                "old": {
                    "overall_score": self.old_overall_score,
                    "layer_scores": self.old_layer_scores,
                    "total_heat": self.old_total_heat
                },
                "new": {
                    "overall_score": self.new_overall_score,
                    "layer_scores": self.new_layer_scores,
                    "total_heat": self.new_total_heat
                }
            },
            "decision": {
                "is_better": self.is_better,
                "improvement_percentage": self.improvement_percentage,
                "adoption_reason": self.adoption_reason
            }
        }


class ABTester:
    """
    A/Bæµ‹è¯•æ¡†æ¶
    
    å·¥ä½œæµç¨‹ï¼š
        1. åŠ è½½æ—§Promptå’Œæ–°Prompt
        2. åˆ†åˆ«è¿è¡Œå¯¹é½
        3. å¯¹æ¯”æ€§èƒ½æŒ‡æ ‡
        4. å†³ç­–æ˜¯å¦é‡‡ç”¨æ–°Prompt
    """
    
    def __init__(
        self,
        layered_aligner,
        heat_calculator: Optional[HeatCalculator] = None,
        output_dir: str = "data/alignment_optimization/ab_tests"
    ):
        """
        åˆå§‹åŒ–A/Bæµ‹è¯•å™¨
        
        Args:
            layered_aligner: LayeredAlignmentEngineå®ä¾‹
            heat_calculator: HeatCalculatorå®ä¾‹ï¼ˆå¯é€‰ï¼‰
            output_dir: A/Bæµ‹è¯•ç»“æœè¾“å‡ºç›®å½•
        """
        self.layered_aligner = layered_aligner
        self.heat_calculator = heat_calculator or HeatCalculator()
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info("âœ… ABTester åˆå§‹åŒ–å®Œæˆ")
    
    async def run_ab_test(
        self,
        project_id: str,
        episode: str,
        script_text: str,
        novel_text: str,
        script_time_range: str,
        old_prompt_version: str,
        new_prompt_version: str,
        layer: str
    ) -> ABTestResult:
        """
        è¿è¡ŒA/Bæµ‹è¯•
        
        Args:
            project_id: é¡¹ç›®ID
            episode: é›†æ•°
            script_text: Scriptæ–‡æœ¬
            novel_text: Novelæ–‡æœ¬
            script_time_range: Scriptæ—¶é—´èŒƒå›´
            old_prompt_version: æ—§Promptç‰ˆæœ¬
            new_prompt_version: æ–°Promptç‰ˆæœ¬
            layer: æµ‹è¯•çš„å±‚çº§
        
        Returns:
            ABTestResult
        """
        logger.info(f"ğŸ§ª å¼€å§‹A/Bæµ‹è¯•: {project_id}/{episode} - {layer}")
        logger.info(f"   æ—§ç‰ˆæœ¬: {old_prompt_version}")
        logger.info(f"   æ–°ç‰ˆæœ¬: {new_prompt_version}")
        
        # Step 1: ä½¿ç”¨æ—§Promptè¿è¡Œå¯¹é½
        logger.info("   â†’ ä½¿ç”¨æ—§Promptè¿è¡Œå¯¹é½...")
        self._load_prompt_version(layer, old_prompt_version)
        
        old_result = await self.layered_aligner.align(
            episode=episode,
            script_text=script_text,
            novel_text=novel_text,
            script_time_range=script_time_range
        )
        
        old_alignment_path = self._save_alignment_result(
            old_result,
            project_id,
            episode,
            f"old_{old_prompt_version}"
        )
        
        logger.info(f"     Old Overall Score: {old_result.overall_score:.3f}")
        
        # Step 2: ä½¿ç”¨æ–°Promptè¿è¡Œå¯¹é½
        logger.info("   â†’ ä½¿ç”¨æ–°Promptè¿è¡Œå¯¹é½...")
        self._load_prompt_version(layer, new_prompt_version)
        
        new_result = await self.layered_aligner.align(
            episode=episode,
            script_text=script_text,
            novel_text=novel_text,
            script_time_range=script_time_range
        )
        
        new_alignment_path = self._save_alignment_result(
            new_result,
            project_id,
            episode,
            f"new_{new_prompt_version}"
        )
        
        logger.info(f"     New Overall Score: {new_result.overall_score:.3f}")
        
        # Step 3: å¯¹æ¯”æ€§èƒ½
        is_better, improvement, reason = self._compare_results(
            old_result,
            new_result,
            layer
        )
        
        # Step 4: åˆ›å»ºæµ‹è¯•ç»“æœ
        ab_result = ABTestResult(
            old_version=old_prompt_version,
            new_version=new_prompt_version,
            old_alignment_path=old_alignment_path,
            new_alignment_path=new_alignment_path,
            old_overall_score=old_result.overall_score,
            new_overall_score=new_result.overall_score,
            old_layer_scores=old_result.layer_scores,
            new_layer_scores=new_result.layer_scores,
            is_better=is_better,
            improvement_percentage=improvement,
            adoption_reason=reason
        )
        
        # Step 5: ä¿å­˜æµ‹è¯•ç»“æœ
        self._save_ab_result(ab_result, project_id, episode, layer)
        
        logger.info(f"âœ… A/Bæµ‹è¯•å®Œæˆ: {'âœ… é‡‡ç”¨æ–°ç‰ˆæœ¬' if is_better else 'âŒ ä¿ç•™æ—§ç‰ˆæœ¬'}")
        logger.info(f"   æ”¹è¿›å¹…åº¦: {improvement:+.2f}%")
        logger.info(f"   ç†ç”±: {reason}")
        
        return ab_result
    
    def _load_prompt_version(self, layer: str, version: str):
        """åŠ è½½æŒ‡å®šç‰ˆæœ¬çš„Prompt"""
        # TODO: å®ç°Promptç‰ˆæœ¬åˆ‡æ¢é€»è¾‘
        # è¿™éœ€è¦ä¿®æ”¹promptsçš„åŠ è½½æœºåˆ¶
        logger.debug(f"   åŠ è½½Prompt: {layer}/{version}")
        pass
    
    def _save_alignment_result(
        self,
        result,
        project_id: str,
        episode: str,
        suffix: str
    ) -> str:
        """ä¿å­˜å¯¹é½ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{project_id}_{episode}_{suffix}_{timestamp}.json"
        filepath = self.output_dir / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(result.to_dict(), f, ensure_ascii=False, indent=2)
        
        logger.debug(f"   å·²ä¿å­˜å¯¹é½ç»“æœ: {filepath}")
        return str(filepath)
    
    def _compare_results(
        self,
        old_result,
        new_result,
        layer: str
    ) -> tuple[bool, float, str]:
        """
        å¯¹æ¯”æ–°æ—§ç»“æœ
        
        Returns:
            (is_better, improvement_percentage, reason)
        """
        # å¯¹æ¯”Overall Score
        old_score = old_result.overall_score
        new_score = new_result.overall_score
        
        if old_score == 0:
            improvement = 100.0 if new_score > 0 else 0.0
        else:
            improvement = ((new_score - old_score) / old_score) * 100
        
        # å¯¹æ¯”ç‰¹å®šå±‚çš„Score
        old_layer_score = old_result.layer_scores.get(layer, 0)
        new_layer_score = new_result.layer_scores.get(layer, 0)
        
        if old_layer_score == 0:
            layer_improvement = 100.0 if new_layer_score > 0 else 0.0
        else:
            layer_improvement = ((new_layer_score - old_layer_score) / old_layer_score) * 100
        
        # å†³ç­–é€»è¾‘
        is_better = False
        reason = ""
        
        if new_score > old_score * 1.05:  # æ”¹è¿›è¶…è¿‡5%
            is_better = True
            reason = f"Overall Scoreæå‡{improvement:.2f}%ï¼Œæ˜¾è‘—æ”¹è¿›"
        elif new_score > old_score and new_layer_score > old_layer_score * 1.10:
            is_better = True
            reason = f"{layer}å±‚Scoreæå‡{layer_improvement:.2f}%ï¼Œç›®æ ‡å±‚æ”¹è¿›æ˜¾è‘—"
        elif new_score >= old_score * 0.95:  # ä¸‹é™å°äº5%
            if new_layer_score > old_layer_score:
                is_better = True
                reason = f"{layer}å±‚Scoreæå‡ï¼ŒOverall Scoreå˜åŒ–å¯æ¥å—"
            else:
                is_better = False
                reason = f"Overall Scoreä¸‹é™{-improvement:.2f}%æˆ–æŒå¹³ï¼Œä¸é‡‡ç”¨"
        else:
            is_better = False
            reason = f"Overall Scoreä¸‹é™{-improvement:.2f}%ï¼Œæ‹’ç»æ–°ç‰ˆæœ¬"
        
        return is_better, improvement, reason
    
    def _save_ab_result(
        self,
        ab_result: ABTestResult,
        project_id: str,
        episode: str,
        layer: str
    ):
        """ä¿å­˜A/Bæµ‹è¯•ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"ab_test_{project_id}_{episode}_{layer}_{timestamp}.json"
        filepath = self.output_dir / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(ab_result.to_dict(), f, ensure_ascii=False, indent=2)
        
        logger.info(f"   A/Bæµ‹è¯•ç»“æœå·²ä¿å­˜: {filepath}")
    
    async def run_optimization_round(
        self,
        project_id: str,
        episode: str,
        script_text: str,
        novel_text: str,
        script_time_range: str,
        annotations: List[AlignmentAnnotation],
        round_number: int
    ) -> OptimizationRound:
        """
        è¿è¡Œå®Œæ•´çš„ä¼˜åŒ–è½®æ¬¡
        
        Args:
            project_id: é¡¹ç›®ID
            episode: é›†æ•°
            script_text: Scriptæ–‡æœ¬
            novel_text: Novelæ–‡æœ¬
            script_time_range: Scriptæ—¶é—´èŒƒå›´
            annotations: æ ‡æ³¨æ•°æ®
            round_number: è½®æ¬¡ç¼–å·
        
        Returns:
            OptimizationRound
        """
        logger.info(f"ğŸ”„ å¼€å§‹ä¼˜åŒ–è½®æ¬¡ {round_number}...")
        
        # è®¡ç®—Heat
        annotations_with_heat = self.heat_calculator.calculate_batch_heat(annotations)
        summary = self.heat_calculator.get_heat_summary(annotations_with_heat)
        
        logger.info(f"   æ ‡æ³¨æ•°æ®: {len(annotations)}æ¡")
        logger.info(f"   é”™è¯¯æ•°é‡: {len([a for a in annotations if not a.is_correct_match])}ä¸ª")
        logger.info(f"   æ€»Heat: {summary['total_heat']:.1f}")
        
        # è¿è¡Œå¯¹é½ï¼ˆå½“å‰ç‰ˆæœ¬ï¼‰
        current_result = await self.layered_aligner.align(
            episode=episode,
            script_text=script_text,
            novel_text=novel_text,
            script_time_range=script_time_range
        )
        
        current_alignment_path = self._save_alignment_result(
            current_result,
            project_id,
            episode,
            f"round_{round_number}"
        )
        
        # åˆ›å»ºOptimizationRound
        round_data = OptimizationRound(
            round_number=round_number,
            project_id=project_id,
            episode=episode,
            prompt_versions={
                "world_building": "v1.0",  # TODO: åŠ¨æ€è·å–
                "game_mechanics": "v1.0",
                "items_equipment": "v1.0",
                "plot_events": "v1.0"
            },
            alignment_result_path=current_alignment_path,
            overall_score=current_result.overall_score,
            layer_scores=current_result.layer_scores,
            annotations=annotations_with_heat,
            total_annotations=len(annotations_with_heat),
            error_count=len([a for a in annotations_with_heat if not a.is_correct_match]),
            total_heat=summary['total_heat'],
            avg_heat=summary['avg_heat'],
            adopted=False,  # é»˜è®¤ä¸ºFalseï¼ŒA/Bæµ‹è¯•åæ›´æ–°
            adoption_reason=None
        )
        
        logger.info(f"âœ… ä¼˜åŒ–è½®æ¬¡ {round_number} å®Œæˆ")
        logger.info(f"   Overall Score: {current_result.overall_score:.3f}")
        
        return round_data
