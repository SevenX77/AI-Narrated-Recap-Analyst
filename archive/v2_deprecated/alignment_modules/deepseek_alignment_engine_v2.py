"""
DeepSeekå¯¹é½å¼•æ“ v2 - åŸºäºä¸¤çº§åŒ¹é…ç­–ç•¥

æ–°çš„ä¸‰å±‚æ•°æ®æ¨¡å‹ï¼šSentence -> SemanticBlock -> Event
ä¸¤çº§åŒ¹é…ç­–ç•¥ï¼šEventçº§ç²—åŒ¹é… + SemanticBlocké“¾ç»†éªŒè¯
"""

import json
import re
import asyncio
from typing import List, Dict, Tuple, Optional, Any
from src.core.schemas import (
    Sentence, SemanticBlock, Event, EventAlignment,
    BlockChainValidation, TimeRange, AlignmentQualityReport
)
from src.utils.logger import logger
from src.utils.prompt_loader import load_prompts
from src.modules.alignment.hook_detector import HookDetector
from .alignment_engine import AlignmentEngine


class DeepSeekAlignmentEngineV2(AlignmentEngine):
    """
    DeepSeekå¯¹é½å¼•æ“ v2
    
    å®ç°æ–°çš„ä¸¤çº§åŒ¹é…ç­–ç•¥ï¼š
    1. Level 1: Eventçº§ç²—åŒ¹é…ï¼ˆå¿«é€Ÿå®šä½å€™é€‰ï¼‰
    2. Level 2: SemanticBlocké“¾ç»†éªŒè¯ï¼ˆç²¾ç¡®ç¡®è®¤ï¼‰
    """
    
    def __init__(self, client=None, model_name: str = "deepseek-chat"):
        super().__init__(client, model_name)
        self.prompts = load_prompts("alignment")
        self.hook_detector = HookDetector(client, model_name)
        self.semaphore = asyncio.Semaphore(5)  # å¹¶å‘æ§åˆ¶
        self.last_hook_result = None  # ä¿å­˜æœ€åä¸€æ¬¡Hookæ£€æµ‹ç»“æœ
    
    # ==================== Step 1: æ–‡æœ¬é¢„å¤„ç† ====================
    
    async def restore_sentences_from_srt_async(self, srt_blocks: List[Dict]) -> List[Sentence]:
        """
        å°†SRTå­—å¹•å—è¿˜åŸä¸ºå®Œæ•´å¥å­ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰
        
        Args:
            srt_blocks: SRTå—åˆ—è¡¨ï¼Œæ¯ä¸ªå—åŒ…å« {index, start, end, text}
            
        Returns:
            List[Sentence]: å¥å­åˆ—è¡¨
        """
        logger.info(f"ğŸ“ è¿˜åŸSRTå¥å­: {len(srt_blocks)} blocks")
        logger.info(f"   â†’ è°ƒç”¨LLMè¿›è¡Œå¥å­è¿˜åŸ...")
        
        # æ ¼å¼åŒ–SRT blocksä¸ºæ–‡æœ¬
        srt_text = self._format_srt_blocks(srt_blocks)
        
        # æ„é€ prompt
        system_prompt = self.prompts["srt_sentence_restoration"]["system"]
        user_prompt = self.prompts["srt_sentence_restoration"]["user"].format(
            srt_blocks=srt_text
        )
        
        try:
            async with self.semaphore:
                response = await asyncio.to_thread(
                    self.client.chat.completions.create,
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=0.1,
                    response_format={"type": "json_object"}
                )
            
            content = response.choices[0].message.content
            
            # å°è¯•è§£æJSON
            try:
                data = json.loads(content)
            except json.JSONDecodeError as json_err:
                logger.error(f"âŒ SRTå¥å­è¿˜åŸ - JSONè§£æå¤±è´¥")
                logger.error(f"   é”™è¯¯ç±»å‹: {type(json_err).__name__}")
                logger.error(f"   é”™è¯¯ä¿¡æ¯: {json_err}")
                logger.error(f"   é”™è¯¯ä½ç½®: line {json_err.lineno}, column {json_err.colno}")
                logger.error(f"   LLMè¿”å›å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                logger.error(f"   LLMè¿”å›å†…å®¹å‰500å­—ç¬¦:")
                logger.error(f"   {content[:500]}")
                logger.error(f"   LLMè¿”å›å†…å®¹å500å­—ç¬¦:")
                logger.error(f"   {content[-500:]}")
                logger.warning(f"   â†’ ä½¿ç”¨é™çº§æ–¹æ¡ˆï¼šæ¯ä¸ªSRTå—ä½œä¸ºä¸€ä¸ªå¥å­")
                return self._fallback_srt_to_sentences(srt_blocks)
            
            # è§£æä¸ºSentenceå¯¹è±¡
            sentences_list = data.get("sentences", data) if isinstance(data, dict) else data
            
            if not isinstance(sentences_list, list):
                logger.error(f"âŒ SRTå¥å­è¿˜åŸ - è¿”å›æ•°æ®æ ¼å¼é”™è¯¯")
                logger.error(f"   æœŸæœ›: list")
                logger.error(f"   å®é™…: {type(sentences_list).__name__}")
                logger.error(f"   æ•°æ®å†…å®¹: {sentences_list}")
                logger.warning(f"   â†’ ä½¿ç”¨é™çº§æ–¹æ¡ˆï¼šæ¯ä¸ªSRTå—ä½œä¸ºä¸€ä¸ªå¥å­")
                return self._fallback_srt_to_sentences(srt_blocks)
            
            sentences = []
            
            for idx, item in enumerate(sentences_list):
                try:
                    time_range = None
                    if "time_range" in item:
                        time_range = TimeRange(**item["time_range"])
                    
                    sentences.append(Sentence(
                        text=item["text"],
                        time_range=time_range,
                        index=item.get("index", len(sentences))
                    ))
                except Exception as item_err:
                    logger.error(f"âŒ è§£æç¬¬ {idx+1} ä¸ªå¥å­å¤±è´¥")
                    logger.error(f"   é”™è¯¯: {item_err}")
                    logger.error(f"   æ•°æ®: {item}")
                    # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª
                    continue
            
            if not sentences:
                logger.error(f"âŒ SRTå¥å­è¿˜åŸ - æœªæˆåŠŸè§£æä»»ä½•å¥å­")
                logger.warning(f"   â†’ ä½¿ç”¨é™çº§æ–¹æ¡ˆï¼šæ¯ä¸ªSRTå—ä½œä¸ºä¸€ä¸ªå¥å­")
                return self._fallback_srt_to_sentences(srt_blocks)
            
            logger.info(f"âœ… è¿˜åŸå®Œæˆ: {len(sentences)} å¥å­")
            return sentences
            
        except Exception as e:
            logger.error(f"âŒ SRTå¥å­è¿˜åŸå¤±è´¥ - æœªé¢„æœŸçš„é”™è¯¯")
            logger.error(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
            logger.error(f"   é”™è¯¯ä¿¡æ¯: {e}")
            import traceback
            logger.error(f"   å †æ ˆè¿½è¸ª:\n{traceback.format_exc()}")
            logger.warning(f"   â†’ ä½¿ç”¨é™çº§æ–¹æ¡ˆï¼šæ¯ä¸ªSRTå—ä½œä¸ºä¸€ä¸ªå¥å­")
            # é™çº§ï¼šæ¯ä¸ªSRTå—ä½œä¸ºä¸€ä¸ªå¥å­
            return self._fallback_srt_to_sentences(srt_blocks)
    
    def restore_sentences_from_novel(self, novel_text: str, chapter_id: str) -> List[Sentence]:
        """
        å°†å°è¯´æ–‡æœ¬åˆ†å‰²ä¸ºå¥å­
        
        Args:
            novel_text: å°è¯´æ–‡æœ¬
            chapter_id: ç« èŠ‚ID
            
        Returns:
            List[Sentence]: å¥å­åˆ—è¡¨
        """
        # ç®€å•å®ç°ï¼šæŒ‰å¥å·ã€é—®å·ã€æ„Ÿå¹å·åˆ†å‰²
        sentences = []
        pattern = r'([^ã€‚ï¼ï¼Ÿ]+[ã€‚ï¼ï¼Ÿ])'
        matches = re.findall(pattern, novel_text)
        
        for i, text in enumerate(matches):
            if text.strip():
                sentences.append(Sentence(
                    text=text.strip(),
                    time_range=None,
                    index=i
                ))
        
        logger.info(f"ğŸ“– å°è¯´å¥å­åˆ†å‰²: {chapter_id} -> {len(sentences)} å¥å­")
        return sentences
    
    def _format_srt_blocks(self, srt_blocks: List[Dict]) -> str:
        """æ ¼å¼åŒ–SRT blocksä¸ºå¯è¯»æ–‡æœ¬"""
        lines = []
        for block in srt_blocks:
            lines.append(f"[{block['start']} --> {block['end']}]")
            lines.append(f"{block['text']}")
            lines.append("")
        return "\n".join(lines)
    
    def _fallback_srt_to_sentences(self, srt_blocks: List[Dict]) -> List[Sentence]:
        """é™çº§æ–¹æ¡ˆï¼šæ¯ä¸ªSRTå—ä½œä¸ºä¸€ä¸ªå¥å­"""
        sentences = []
        for i, block in enumerate(srt_blocks):
            sentences.append(Sentence(
                text=block['text'],
                time_range=TimeRange(start=block['start'], end=block['end']),
                index=i
            ))
        return sentences
    
    # ==================== Step 2: æ„æ€å—åˆ’åˆ† ====================
    
    async def segment_semantic_blocks_async(
        self,
        sentences: List[Sentence],
        source_type: str,
        context_info: str = ""
    ) -> List[SemanticBlock]:
        """
        å°†å¥å­åˆ’åˆ†ä¸ºæ„æ€å—ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰
        
        Args:
            sentences: å¥å­åˆ—è¡¨
            source_type: "script" æˆ– "novel"
            context_info: é¢å¤–ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Returns:
            List[SemanticBlock]: æ„æ€å—åˆ—è¡¨
        """
        logger.info(f"ğŸ§© åˆ’åˆ†æ„æ€å—: {len(sentences)} å¥å­ ({source_type})")
        logger.info(f"   â†’ è°ƒç”¨LLMè¿›è¡Œæ„æ€å—åˆ’åˆ†...")
        
        # æ ¼å¼åŒ–sentencesä¸ºæ–‡æœ¬
        sentences_text = self._format_sentences(sentences)
        
        # æ„é€ prompt
        system_prompt = self.prompts["semantic_block_segmentation"]["system"]
        user_prompt = self.prompts["semantic_block_segmentation"]["user"].format(
            sentences=sentences_text,
            source_type=source_type,
            context_info=context_info
        )
        
        try:
            async with self.semaphore:
                response = await asyncio.to_thread(
                    self.client.chat.completions.create,
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=0.3,
                    response_format={"type": "json_object"}
                )
            
            content = response.choices[0].message.content
            data = json.loads(content)
            
            # è§£æä¸ºSemanticBlockå¯¹è±¡
            blocks_list = data.get("blocks", data) if isinstance(data, dict) else data
            semantic_blocks = []
            
            for item in blocks_list:
                # æå–å¯¹åº”çš„å¥å­
                sentence_indices = item.get("sentence_indices", [])
                block_sentences = [sentences[i] for i in sentence_indices if i < len(sentences)]
                
                # è®¡ç®—time_rangeï¼ˆä»…scriptæœ‰ï¼‰
                time_range = None
                if source_type == "script" and block_sentences:
                    first_sentence = block_sentences[0]
                    last_sentence = block_sentences[-1]
                    if first_sentence.time_range and last_sentence.time_range:
                        time_range = TimeRange(
                            start=first_sentence.time_range.start,
                            end=last_sentence.time_range.end
                        )
                
                semantic_blocks.append(SemanticBlock(
                    block_id=item["block_id"],
                    theme=item["theme"],
                    sentences=block_sentences,
                    characters=item.get("characters", []),
                    location=item.get("location"),
                    time_context=item.get("time_context"),
                    summary=item["summary"],
                    time_range=time_range
                ))
            
            logger.info(f"âœ… æ„æ€å—åˆ’åˆ†å®Œæˆ: {len(semantic_blocks)} blocks")
            return semantic_blocks
            
        except json.JSONDecodeError as json_err:
            logger.error(f"âŒ æ„æ€å—åˆ’åˆ† - JSONè§£æå¤±è´¥")
            logger.error(f"   é”™è¯¯: {json_err}")
            logger.error(f"   ä½ç½®: line {json_err.lineno}, column {json_err.colno}")
            if 'content' in locals():
                logger.error(f"   LLMè¿”å›å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                logger.error(f"   å‰500å­—ç¬¦: {content[:500]}")
            return []
        except Exception as e:
            logger.error(f"âŒ æ„æ€å—åˆ’åˆ†å¤±è´¥")
            logger.error(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
            logger.error(f"   é”™è¯¯ä¿¡æ¯: {e}")
            import traceback
            logger.error(f"   å †æ ˆè¿½è¸ª:\n{traceback.format_exc()}")
            return []
    
    def _format_sentences(self, sentences: List[Sentence]) -> str:
        """æ ¼å¼åŒ–å¥å­ä¸ºå¯è¯»æ–‡æœ¬"""
        lines = []
        for i, sentence in enumerate(sentences):
            if sentence.time_range:
                lines.append(f"{i}. [{sentence.time_range.start}] {sentence.text}")
            else:
                lines.append(f"{i}. {sentence.text}")
        return "\n".join(lines)
    
    # ==================== Step 3: äº‹ä»¶èšåˆ ====================
    
    async def aggregate_events_async(
        self,
        semantic_blocks: List[SemanticBlock],
        source_type: str,
        context_info: str = ""
    ) -> List[Event]:
        """
        å°†æ„æ€å—èšåˆä¸ºäº‹ä»¶ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰
        
        Args:
            semantic_blocks: æ„æ€å—åˆ—è¡¨
            source_type: "script" æˆ– "novel"
            context_info: é¢å¤–ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Returns:
            List[Event]: äº‹ä»¶åˆ—è¡¨
        """
        logger.info(f"ğŸ“¦ èšåˆäº‹ä»¶: {len(semantic_blocks)} blocks ({source_type})")
        logger.info(f"   â†’ è°ƒç”¨LLMè¿›è¡Œäº‹ä»¶èšåˆ...")
        
        # æ ¼å¼åŒ–blocksä¸ºæ–‡æœ¬
        blocks_text = self._format_blocks_for_aggregation(semantic_blocks)
        
        # æ„é€ prompt
        system_prompt = self.prompts["event_aggregation"]["system"]
        user_prompt = self.prompts["event_aggregation"]["user"].format(
            semantic_blocks=blocks_text,
            source_type=source_type,
            context_info=context_info
        )
        
        try:
            async with self.semaphore:
                response = await asyncio.to_thread(
                    self.client.chat.completions.create,
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=0.3,
                    response_format={"type": "json_object"}
                )
            
            content = response.choices[0].message.content
            data = json.loads(content)
            
            # è§£æä¸ºEventå¯¹è±¡
            events_list = data.get("events", data) if isinstance(data, dict) else data
            events = []
            
            # åˆ›å»ºblock_idåˆ°blockå¯¹è±¡çš„æ˜ å°„
            block_map = {block.block_id: block for block in semantic_blocks}
            
            for item in events_list:
                # æå–å¯¹åº”çš„blocks
                block_ids = item.get("block_ids", [])
                event_blocks = [block_map[bid] for bid in block_ids if bid in block_map]
                
                # è®¡ç®—time_rangeï¼ˆä»…scriptæœ‰ï¼‰
                time_range = None
                if source_type == "script" and event_blocks:
                    first_block = event_blocks[0]
                    last_block = event_blocks[-1]
                    if first_block.time_range and last_block.time_range:
                        time_range = TimeRange(
                            start=first_block.time_range.start,
                            end=last_block.time_range.end
                        )
                
                # è§£æchapter_range
                chapter_range = None
                if "chapter_range" in item and item["chapter_range"]:
                    cr = item["chapter_range"]
                    if isinstance(cr, list) and len(cr) == 2:
                        chapter_range = tuple(cr)
                
                events.append(Event(
                    event_id=item["event_id"],
                    title=item["title"],
                    semantic_blocks=event_blocks,
                    characters=item.get("characters", []),
                    location=item.get("location"),
                    time_context=item.get("time_context"),
                    chapter_range=chapter_range,
                    time_range=time_range,
                    episode=item.get("episode")
                ))
            
            logger.info(f"âœ… äº‹ä»¶èšåˆå®Œæˆ: {len(events)} events")
            return events
            
        except json.JSONDecodeError as json_err:
            logger.error(f"âŒ äº‹ä»¶èšåˆ - JSONè§£æå¤±è´¥")
            logger.error(f"   é”™è¯¯: {json_err}")
            logger.error(f"   ä½ç½®: line {json_err.lineno}, column {json_err.colno}")
            if 'content' in locals():
                logger.error(f"   LLMè¿”å›å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                logger.error(f"   å‰500å­—ç¬¦: {content[:500]}")
            return []
        except Exception as e:
            logger.error(f"âŒ äº‹ä»¶èšåˆå¤±è´¥")
            logger.error(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
            logger.error(f"   é”™è¯¯ä¿¡æ¯: {e}")
            import traceback
            logger.error(f"   å †æ ˆè¿½è¸ª:\n{traceback.format_exc()}")
            return []
    
    def _format_blocks_for_aggregation(self, blocks: List[SemanticBlock]) -> str:
        """æ ¼å¼åŒ–æ„æ€å—ä¸ºå¯è¯»æ–‡æœ¬"""
        lines = []
        for block in blocks:
            lines.append(f"Block ID: {block.block_id}")
            lines.append(f"  ä¸»é¢˜: {block.theme}")
            lines.append(f"  æ¦‚æ‹¬: {block.summary}")
            lines.append(f"  è§’è‰²: {', '.join(block.characters) if block.characters else 'æ— '}")
            lines.append(f"  åœ°ç‚¹: {block.location or 'æœªçŸ¥'}")
            lines.append(f"  æ—¶é—´: {block.time_context or 'æœªçŸ¥'}")
            lines.append("")
        return "\n".join(lines)
    
    # ==================== Step 4: ä¸¤çº§åŒ¹é… ====================
    
    async def match_events_two_level_async(
        self,
        script_events: List[Event],
        novel_events: List[Event],
        episode_name: str = "ep01"
    ) -> List[EventAlignment]:
        """
        ä¸¤çº§åŒ¹é…ï¼ˆä¼˜åŒ–ç‰ˆï¼‰ï¼šæ‰¹é‡Eventçº§åŒ¹é… + æ‰¹é‡Blocké“¾éªŒè¯
        
        Args:
            script_events: Scriptçš„äº‹ä»¶åˆ—è¡¨
            novel_events: Novelçš„äº‹ä»¶åˆ—è¡¨
            episode_name: é›†æ•°åç§°
            
        Returns:
            List[EventAlignment]: å¯¹é½ç»“æœåˆ—è¡¨
        """
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ”— å¼€å§‹ä¸¤çº§åŒ¹é…: {episode_name}")
        logger.info(f"   Script Events: {len(script_events)}")
        logger.info(f"   Novel Events: {len(novel_events)}")
        logger.info(f"{'='*60}\n")
        
        # å¦‚æœæ˜¯ep01ï¼Œå…ˆè¿›è¡ŒHookæ£€æµ‹
        linear_start_index = 0
        if episode_name == "ep01" and script_events:
            logger.info("ğŸ£ æ‰§è¡ŒHookæ£€æµ‹...")
            # ä½¿ç”¨SemanticBlocksè¿›è¡ŒHookæ£€æµ‹
            script_blocks = []
            for event in script_events[:5]:  # åªæ£€æŸ¥å‰5ä¸ªäº‹ä»¶
                script_blocks.extend(event.semantic_blocks)
            
            novel_blocks = []
            for event in novel_events[:10]:  # æ£€æŸ¥å‰10ä¸ªäº‹ä»¶
                novel_blocks.extend(event.semantic_blocks)
            
            hook_result = self.hook_detector.detect_hook_boundary(
                script_blocks[:20],
                novel_blocks[:50]
            )
            
            # ä¿å­˜Hookæ£€æµ‹ç»“æœä¾›workflowä½¿ç”¨
            self.last_hook_result = hook_result
            
            logger.info(f"   ç»“æœ: has_hook={hook_result.has_hook}, confidence={hook_result.confidence:.2f}")
            logger.info(f"   æ¨ç†: {hook_result.reasoning}")
            
            # æ‰¾åˆ°linear_start_indexå¯¹åº”çš„eventç´¢å¼•
            if hook_result.has_hook:
                block_count = 0
                for i, event in enumerate(script_events):
                    block_count += len(event.semantic_blocks)
                    if block_count > hook_result.linear_start_index:
                        linear_start_index = i
                        break
                logger.info(f"   âœ… çº¿æ€§å™äº‹èµ·ç‚¹: Eventç´¢å¼• {linear_start_index}\n")
        
        # ä»çº¿æ€§èµ·ç‚¹å¼€å§‹åŒ¹é…
        alignments = []
        last_matched_novel_index = 0
        events_to_match = script_events[linear_start_index:]
        
        logger.info(f"ğŸ“Š Level 1: æ‰¹é‡Eventçº§ç²—åŒ¹é…")
        logger.info(f"   å¾…åŒ¹é…Script Events: {len(events_to_match)}")
        
        # æ‰¹é‡Eventçº§åŒ¹é…
        batch_matching_results = await self._batch_match_events_async(
            events_to_match,
            novel_events,
            last_matched_novel_index
        )
        
        logger.info(f"   âœ… æ‰¹é‡åŒ¹é…å®Œæˆ\n")
        
        # å¤„ç†æ¯ä¸ªScript Eventçš„åŒ¹é…ç»“æœ
        for i, script_event in enumerate(events_to_match):
            matching_result = batch_matching_results.get(script_event.event_id, {})
            candidates = matching_result.get("candidates", [])
            
            logger.info(f"{'â”€'*60}")
            logger.info(f"ğŸ“Œ å¤„ç†Script Event #{i+1}: {script_event.event_id}")
            logger.info(f"   æ ‡é¢˜: {script_event.title}")
            logger.info(f"   è§’è‰²: {', '.join(script_event.characters) if script_event.characters else 'æ— '}")
            logger.info(f"   åœ°ç‚¹: {script_event.location or 'æœªçŸ¥'}")
            logger.info(f"   æ„æ€å—æ•°: {len(script_event.semantic_blocks)}")
            logger.info(f"   å€™é€‰æ•°: {len(candidates)}")
            
            if not candidates:
                logger.warning(f"   âš ï¸  æœªæ‰¾åˆ°å€™é€‰\n")
                continue
            
            # è¿‡æ»¤é«˜åˆ†å€™é€‰ï¼ˆé˜ˆå€¼0.75ï¼‰
            EVENT_THRESHOLD_HIGH = 0.75
            EVENT_THRESHOLD_LOW = 0.6
            
            high_score_candidates = [c for c in candidates if c["match_score"] >= EVENT_THRESHOLD_HIGH]
            
            logger.info(f"   ğŸ“ Eventçº§é˜ˆå€¼: {EVENT_THRESHOLD_HIGH} (é™çº§é˜ˆå€¼: {EVENT_THRESHOLD_LOW})")
            
            if not high_score_candidates:
                max_score = max([c['match_score'] for c in candidates]) if candidates else 0
                logger.info(f"   âš ï¸  æ— å€™é€‰è¾¾åˆ°é˜ˆå€¼{EVENT_THRESHOLD_HIGH}ï¼ˆæœ€é«˜åˆ†: {max_score:.3f}ï¼‰")
                # é™çº§ï¼šä½¿ç”¨é˜ˆå€¼0.6
                high_score_candidates = [c for c in candidates if c["match_score"] >= EVENT_THRESHOLD_LOW]
                if high_score_candidates:
                    logger.info(f"   â†’ ä½¿ç”¨é™çº§é˜ˆå€¼{EVENT_THRESHOLD_LOW}ï¼Œæ‰¾åˆ° {len(high_score_candidates)} ä¸ªå€™é€‰")
            
            if not high_score_candidates:
                logger.warning(f"   âš ï¸  æœªæ‰¾åˆ°åˆé€‚å€™é€‰ï¼ˆæœ€é«˜åˆ† < 0.6ï¼‰\n")
                continue
            
            # æ˜¾ç¤ºå€™é€‰ä¿¡æ¯
            for j, cand in enumerate(high_score_candidates[:3]):  # æœ€å¤šæ˜¾ç¤º3ä¸ª
                logger.info(f"   å€™é€‰#{j+1}: {cand['novel_event_id']} (è¯„åˆ†: {cand['match_score']:.3f})")
                logger.info(f"      æ¨ç†: {cand.get('reasoning', 'N/A')}")
            
            # Level 2: æ‰¹é‡Blocké“¾éªŒè¯
            logger.info(f"\n   ğŸ” Level 2: æ‰¹é‡Blocké“¾éªŒè¯ï¼ˆ{len(high_score_candidates)}ä¸ªå€™é€‰ï¼‰")
            
            # è·å–Novel Eventå¯¹è±¡
            novel_event_objs = []
            for cand in high_score_candidates:
                novel_event = next(
                    (e for e in novel_events if e.event_id == cand["novel_event_id"]),
                    None
                )
                if novel_event:
                    novel_event_objs.append((cand, novel_event))
            
            if not novel_event_objs:
                logger.warning(f"   âš ï¸  æ— æ³•æ‰¾åˆ°Novel Eventå¯¹è±¡\n")
                continue
            
            # æ‰¹é‡éªŒè¯
            validation_results = await self._batch_validate_block_chains_async(
                script_event,
                novel_event_objs
            )
            
            # é€‰æ‹©æœ€ä½³åŒ¹é…
            best_alignment = None
            best_score = 0.0
            
            for (cand, novel_event), validation in zip(novel_event_objs, validation_results):
                # è®¡ç®—æœ€ç»ˆç½®ä¿¡åº¦
                final_confidence = (
                    cand["match_score"] * 0.4 +
                    validation.validation_score * 0.6
                )
                
                logger.info(f"      Novel Event: {novel_event.title}")
                logger.info(f"         EventåŒ¹é…åˆ†: {cand['match_score']:.3f}")
                logger.info(f"         Blocké“¾éªŒè¯åˆ†: {validation.validation_score:.3f}")
                logger.info(f"         æœ€ç»ˆç½®ä¿¡åº¦: {final_confidence:.3f}")
                logger.info(f"         è¦†ç›–ç‡: {validation.coverage_rate:.1%}, é¡ºåºä¸€è‡´æ€§: {validation.order_consistency:.1%}")
                
                if final_confidence > best_score:
                    best_score = final_confidence
                    best_alignment = EventAlignment(
                        script_event=script_event,
                        novel_event=novel_event,
                        event_match_score=cand["match_score"],
                        block_chain_validation=validation,
                        final_confidence=final_confidence,
                        reasoning=f"EventåŒ¹é…: {cand.get('reasoning', '')}; Blocké“¾éªŒè¯: {validation.reasoning}"
                    )
                    
                    # æ›´æ–°last_matched_novel_index
                    novel_index = novel_events.index(novel_event)
                    last_matched_novel_index = max(last_matched_novel_index, novel_index)
            
            if best_alignment:
                alignments.append(best_alignment)
                logger.info(f"\n   âœ… æœ€ä½³åŒ¹é…: {best_alignment.novel_event.title}")
                logger.info(f"      æœ€ç»ˆç½®ä¿¡åº¦: {best_score:.3f}\n")
            else:
                logger.warning(f"   âš ï¸  éªŒè¯å¤±è´¥ï¼Œæ— æœ€ä½³åŒ¹é…\n")
        
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ¯ ä¸¤çº§åŒ¹é…å®Œæˆ: {episode_name}")
        logger.info(f"   æˆåŠŸåŒ¹é…: {len(alignments)} / {len(events_to_match)}")
        logger.info(f"   åŒ¹é…ç‡: {len(alignments)/len(events_to_match)*100:.1f}%" if events_to_match else "   åŒ¹é…ç‡: N/A")
        avg_conf = sum(a.final_confidence for a in alignments)/len(alignments) if alignments else 0.0
        logger.info(f"   å¹³å‡ç½®ä¿¡åº¦: {avg_conf:.3f}")
        logger.info(f"{'='*60}\n")
        
        return alignments
    
    async def _batch_match_events_async(
        self,
        script_events: List[Event],
        novel_events: List[Event],
        last_matched_index: int
    ) -> Dict[str, Dict]:
        """
        æ‰¹é‡Eventçº§ç²—åŒ¹é…ï¼ˆä¸€æ¬¡LLMè°ƒç”¨å¤„ç†æ‰€æœ‰Script Eventsï¼‰
        
        Args:
            script_events: Scriptäº‹ä»¶åˆ—è¡¨
            novel_events: Noveläº‹ä»¶åˆ—è¡¨
            last_matched_index: ä¸Šæ¬¡åŒ¹é…çš„Novelç´¢å¼•
            
        Returns:
            Dict[str, Dict]: {script_event_id: {"candidates": [...]}, ...}
        """
        # æ ¼å¼åŒ–æ‰€æœ‰Script Events
        script_events_text = "\n\n".join([
            f"Script Event #{i+1}:\n" + self._format_event_for_matching(e)
            for i, e in enumerate(script_events)
        ])
        
        # æ ¼å¼åŒ–Novel Eventsï¼ˆä»last_matched_indexå¾€åï¼‰
        novel_events_window = novel_events[last_matched_index:last_matched_index+100]  # é™åˆ¶çª—å£
        novel_events_text = "\n\n".join([
            f"Novel Event #{i+last_matched_index}:\n" + self._format_event_for_matching(e)
            for i, e in enumerate(novel_events_window)
        ])
        
        # æ„é€ prompt
        system_prompt = self.prompts["event_level_matching"]["system"]
        user_prompt = self.prompts["event_level_matching"]["user"].format(
            script_events=script_events_text,
            novel_events=novel_events_text,
            last_matched_index=last_matched_index
        )
        
        logger.info(f"   â†’ è°ƒç”¨LLMæ‰¹é‡åŒ¹é… {len(script_events)} ä¸ªScript Events (Novelçª—å£: {len(novel_events_window)} events)...")
        
        try:
            async with self.semaphore:
                response = await asyncio.to_thread(
                    self.client.chat.completions.create,
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=0.2,
                    response_format={"type": "json_object"}
                )
            
            content = response.choices[0].message.content
            data = json.loads(content)
            
            # è§£æresults
            results = data.get("results", [])
            
            # æ„å»ºè¿”å›å­—å…¸
            result_dict = {}
            for result in results:
                script_event_id = result.get("script_event_id")
                if script_event_id:
                    result_dict[script_event_id] = {
                        "candidates": result.get("candidates", [])
                    }
            
            return result_dict
            
        except json.JSONDecodeError as json_err:
            logger.error(f"âŒ æ‰¹é‡Eventçº§åŒ¹é… - JSONè§£æå¤±è´¥")
            logger.error(f"   é”™è¯¯: {json_err}")
            logger.error(f"   ä½ç½®: line {json_err.lineno}, column {json_err.colno}")
            if 'content' in locals():
                logger.error(f"   LLMè¿”å›å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                logger.error(f"   å‰500å­—ç¬¦: {content[:500]}")
                logger.error(f"   å500å­—ç¬¦: {content[-500:]}")
            return {}
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡Eventçº§åŒ¹é…å¤±è´¥")
            logger.error(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
            logger.error(f"   é”™è¯¯ä¿¡æ¯: {e}")
            import traceback
            logger.error(f"   å †æ ˆè¿½è¸ª:\n{traceback.format_exc()}")
            return {}
    
    async def _batch_validate_block_chains_async(
        self,
        script_event: Event,
        novel_candidates: List[Tuple[Dict, Event]]
    ) -> List[BlockChainValidation]:
        """
        æ‰¹é‡Blocké“¾éªŒè¯ï¼ˆä¸€æ¬¡LLMè°ƒç”¨éªŒè¯å¤šä¸ªå€™é€‰ï¼‰
        
        Args:
            script_event: Scriptäº‹ä»¶
            novel_candidates: [(candidate_dict, novel_event), ...] åˆ—è¡¨
            
        Returns:
            List[BlockChainValidation]: éªŒè¯ç»“æœåˆ—è¡¨ï¼ˆä¸è¾“å…¥é¡ºåºä¸€è‡´ï¼‰
        """
        if not novel_candidates:
            return []
        
        # æ ¼å¼åŒ–Script Eventçš„blocks
        script_blocks_text = self._format_blocks_for_validation(script_event.semantic_blocks)
        
        # æ ¼å¼åŒ–æ‰€æœ‰Novel Eventå€™é€‰
        novel_candidates_text = []
        for i, (cand, novel_event) in enumerate(novel_candidates):
            novel_text = f"""
Candidate #{i+1}:
Novel Event ID: {novel_event.event_id}
Title: {novel_event.title}
Semantic Blocks:
{self._format_blocks_for_validation(novel_event.semantic_blocks)}
"""
            novel_candidates_text.append(novel_text.strip())
        
        # æ„é€ prompt
        system_prompt = self.prompts["block_chain_validation_batch"]["system"]
        user_prompt = self.prompts["block_chain_validation_batch"]["user"].format(
            script_event_id=script_event.event_id,
            script_event_title=script_event.title,
            script_blocks=script_blocks_text,
            novel_candidates="\n\n".join(novel_candidates_text)
        )
        
        logger.info(f"      â†’ è°ƒç”¨LLMæ‰¹é‡éªŒè¯Blocké“¾ ({len(novel_candidates)} ä¸ªå€™é€‰)...")
        
        try:
            async with self.semaphore:
                response = await asyncio.to_thread(
                    self.client.chat.completions.create,
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=0.1,
                    response_format={"type": "json_object"}
                )
            
            content = response.choices[0].message.content
            data = json.loads(content)
            
            # è§£ævalidations
            validations = data.get("validations", [])
            
            # æ„å»ºè¿”å›åˆ—è¡¨ï¼ˆæŒ‰novel_event_idåŒ¹é…ï¼‰
            results = []
            for cand, novel_event in novel_candidates:
                # æŸ¥æ‰¾å¯¹åº”çš„validation
                validation_data = next(
                    (v for v in validations if v.get("novel_event_id") == novel_event.event_id),
                    None
                )
                
                if validation_data:
                    matched_pairs = [tuple(pair) for pair in validation_data.get("matched_pairs", [])]
                    results.append(BlockChainValidation(
                        script_chain=validation_data.get("script_chain", []),
                        novel_chain=validation_data.get("novel_chain", []),
                        matched_pairs=matched_pairs,
                        coverage_rate=validation_data.get("coverage_rate", 0.0),
                        order_consistency=validation_data.get("order_consistency", 0.0),
                        validation_score=validation_data.get("validation_score", 0.0),
                        reasoning=validation_data.get("reasoning", "")
                    ))
                else:
                    # é»˜è®¤å¤±è´¥ç»“æœ
                    script_chain = [b.theme for b in script_event.semantic_blocks]
                    novel_chain = [b.theme for b in novel_event.semantic_blocks]
                    results.append(BlockChainValidation(
                        script_chain=script_chain,
                        novel_chain=novel_chain,
                        matched_pairs=[],
                        coverage_rate=0.0,
                        order_consistency=0.0,
                        validation_score=0.0,
                        reasoning="æœªæ‰¾åˆ°éªŒè¯ç»“æœ"
                    ))
            
            return results
            
        except json.JSONDecodeError as json_err:
            logger.error(f"âŒ æ‰¹é‡Blocké“¾éªŒè¯ - JSONè§£æå¤±è´¥")
            logger.error(f"   é”™è¯¯: {json_err}")
            logger.error(f"   ä½ç½®: line {json_err.lineno}, column {json_err.colno}")
            if 'content' in locals():
                logger.error(f"   LLMè¿”å›å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                logger.error(f"   å‰500å­—ç¬¦: {content[:500]}")
                logger.error(f"   å500å­—ç¬¦: {content[-500:]}")
            # è¿”å›é»˜è®¤å¤±è´¥ç»“æœ
            results = []
            for cand, novel_event in novel_candidates:
                script_chain = [b.theme for b in script_event.semantic_blocks]
                novel_chain = [b.theme for b in novel_event.semantic_blocks]
                results.append(BlockChainValidation(
                    script_chain=script_chain,
                    novel_chain=novel_chain,
                    matched_pairs=[],
                    coverage_rate=0.0,
                    order_consistency=0.0,
                    validation_score=0.0,
                    reasoning=f"JSONè§£æå¤±è´¥: {str(json_err)}"
                ))
            return results
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡Blocké“¾éªŒè¯å¤±è´¥")
            logger.error(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
            logger.error(f"   é”™è¯¯ä¿¡æ¯: {e}")
            import traceback
            logger.error(f"   å †æ ˆè¿½è¸ª:\n{traceback.format_exc()}")
            # è¿”å›é»˜è®¤å¤±è´¥ç»“æœ
            results = []
            for cand, novel_event in novel_candidates:
                script_chain = [b.theme for b in script_event.semantic_blocks]
                novel_chain = [b.theme for b in novel_event.semantic_blocks]
                results.append(BlockChainValidation(
                    script_chain=script_chain,
                    novel_chain=novel_chain,
                    matched_pairs=[],
                    coverage_rate=0.0,
                    order_consistency=0.0,
                    validation_score=0.0,
                    reasoning=f"éªŒè¯å¤±è´¥: {str(e)}"
                ))
            return results
    
    
    def _format_event_for_matching(self, event: Event) -> str:
        """æ ¼å¼åŒ–äº‹ä»¶ä¸ºå¯è¯»æ–‡æœ¬ï¼ˆç”¨äºåŒ¹é…ï¼‰"""
        lines = [
            f"Event ID: {event.event_id}",
            f"Title: {event.title}",
            f"Characters: {', '.join(event.characters) if event.characters else 'æ— '}",
            f"Location: {event.location or 'æœªçŸ¥'}",
            f"Time: {event.time_context or 'æœªçŸ¥'}",
            f"Blocks: {len(event.semantic_blocks)} blocks",
            "Block Themes: " + " â†’ ".join([b.theme for b in event.semantic_blocks])
        ]
        return "\n".join(lines)
    
    def _format_blocks_for_validation(self, blocks: List[SemanticBlock]) -> str:
        """æ ¼å¼åŒ–æ„æ€å—ä¸ºå¯è¯»æ–‡æœ¬ï¼ˆç”¨äºéªŒè¯ï¼‰"""
        lines = []
        for i, block in enumerate(blocks):
            lines.append(f"{i}. {block.theme}")
            lines.append(f"   æ¦‚æ‹¬: {block.summary}")
        return "\n".join(lines)
    
    # ==================== å…¼å®¹æ—§æ¥å£ ====================
    
    def align_script_with_novel(
        self,
        novel_events_data: List[Dict],
        script_events_data: List[Dict]
    ) -> List[EventAlignment]:
        """
        å…¼å®¹æ—§æ¥å£ï¼ˆæš‚æ—¶ä¸å®ç°ï¼Œéœ€è¦åœ¨workflowä¸­ä½¿ç”¨æ–°çš„æµç¨‹ï¼‰
        """
        raise NotImplementedError("è¯·ä½¿ç”¨æ–°çš„ä¸¤çº§åŒ¹é…æµç¨‹")
    
    def aggregate_context(self, alignment_results, novel_chapters):
        """å…¼å®¹æ—§æ¥å£"""
        raise NotImplementedError("ä½¿ç”¨æ–°çš„Eventå¯¹é½ç»“æœ")
    
    def evaluate_alignment_quality(
        self,
        alignment_results: List[EventAlignment],
        quality_threshold: float = 70.0
    ) -> AlignmentQualityReport:
        """
        è¯„ä¼°å¯¹é½è´¨é‡ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼ŒåŸºäºæ–°çš„EventAlignmentï¼‰
        
        TODO: å®Œå–„è¯„ä¼°é€»è¾‘
        """
        if not alignment_results:
            return AlignmentQualityReport(
                overall_score=0.0,
                avg_confidence=0.0,
                coverage_ratio=0.0,
                continuity_score=0.0,
                episode_coverage=[],
                is_qualified=False,
                needs_more_chapters=True,
                details={}
            )
        
        # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
        avg_confidence = sum(a.final_confidence for a in alignment_results) / len(alignment_results)
        
        # ç®€åŒ–çš„è¯„åˆ†
        overall_score = avg_confidence * 100
        
        return AlignmentQualityReport(
            overall_score=overall_score,
            avg_confidence=avg_confidence,
            coverage_ratio=1.0,  # TODO: è®¡ç®—å®é™…è¦†ç›–ç‡
            continuity_score=1.0,  # TODO: è®¡ç®—ç« èŠ‚è¿ç»­æ€§
            episode_coverage=[],  # TODO: è®¡ç®—å„é›†è¦†ç›–
            is_qualified=overall_score >= quality_threshold,
            needs_more_chapters=overall_score < quality_threshold,
            details={
                "total_alignments": len(alignment_results),
                "avg_event_match_score": sum(a.event_match_score for a in alignment_results) / len(alignment_results),
                "avg_validation_score": sum(a.block_chain_validation.validation_score for a in alignment_results) / len(alignment_results)
            }
        )
