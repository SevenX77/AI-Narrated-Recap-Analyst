import os
import re
import json
import asyncio
from typing import List, Dict, Tuple
from src.core.interfaces import BaseWorkflow
from src.core.project_manager import project_manager
from src.core.artifact_manager import artifact_manager
from src.core.config import config
from src.agents.deepseek_analyst import DeepSeekAnalyst, get_llm_client
from src.modules.alignment.deepseek_alignment_engine import DeepSeekAlignmentEngine
from src.utils.logger import logger, op_logger

class IngestionWorkflow(BaseWorkflow):
    """
    Workflow 1: Ingestion & Alignment (åŠ¨æ€ç« èŠ‚æå–)
    
    å®ç°ç­–ç•¥ï¼š
    1. æ ¹æ®SRTæ•°é‡é¢„ä¼°åˆå§‹ç« èŠ‚æ•°
    2. æ‰¹é‡æå–å¹¶è¯„ä¼°å¯¹é½è´¨é‡
    3. æ ¹æ®è¦†ç›–ç‡å’Œè´¨é‡åŠ¨æ€å†³å®šæ˜¯å¦ç»§ç»­æå–
    4. æœ€åæ·»åŠ å®‰å…¨ç¼“å†²ï¼Œé˜²æ­¢é—æ¼æ›´å¥½çš„åŒ¹é…
    """
    def __init__(self, project_id: str):
        super().__init__()
        self.project_id = project_id
        self.paths = project_manager.get_project_paths(project_id)
        self.client = get_llm_client()
        self.analyst = DeepSeekAnalyst(self.client)
        self.aligner = DeepSeekAlignmentEngine(self.client)
        self.cfg = config.ingestion

    async def run(self, **kwargs):
        """
        æ‰§è¡Œæ•°æ®æ‘„å…¥ä¸å¯¹é½æµç¨‹
        
        Args:
            **kwargs: å¯é€‰å‚æ•°
                - max_chapters: å¼ºåˆ¶æŒ‡å®šæœ€å¤§ç« èŠ‚æ•°ï¼ˆè¦†ç›–åŠ¨æ€ç­–ç•¥ï¼‰
        """
        logger.info(f"ğŸš€ å¯åŠ¨æ•°æ®æ‘„å…¥ä¸å¯¹é½æµç¨‹: {self.project_id}")
        
        novel_path = os.path.join(self.paths['raw'], "novel.txt")
        if not os.path.exists(novel_path):
            logger.error(f"Novel file not found: {novel_path}")
            return

        # 1. è¯»å–å°è¯´å¹¶åˆ†ç« 
        logger.info("1. è¯»å–å°è¯´å†…å®¹...")
        with open(novel_path, 'r', encoding='utf-8') as f:
            novel_text = f.read()
        
        all_chapters = self._split_chapters(novel_text)
        logger.info(f"   - å°è¯´å…± {len(all_chapters)} ç« ")
        
        # 2. è·å–SRTæ–‡ä»¶åˆ—è¡¨
        import glob
        srt_files = sorted(glob.glob(os.path.join(self.paths['raw'], "*.srt")))
        logger.info(f"   - æ‰¾åˆ° {len(srt_files)} ä¸ªSRTæ–‡ä»¶")
        
        if not srt_files:
            logger.error("æœªæ‰¾åˆ°SRTæ–‡ä»¶")
            return
        
        # 3. è§£ææ‰€æœ‰SRTäº‹ä»¶ï¼ˆä¸€æ¬¡æ€§å®Œæˆï¼‰
        logger.info("2. è§£æè§£è¯´å­—å¹•...")
        all_srt_events = await self._parse_all_srt_files(srt_files)
        
        # 4. åŠ¨æ€ç« èŠ‚æå–ä¸å¯¹é½
        logger.info("3. åŠ¨æ€ç« èŠ‚æå–ä¸å¯¹é½...")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¼ºåˆ¶æŒ‡å®šçš„æœ€å¤§ç« èŠ‚æ•°
        forced_max_chapters = kwargs.get('max_chapters')
        
        if forced_max_chapters:
            logger.info(f"   - ä½¿ç”¨å¼ºåˆ¶æŒ‡å®šçš„ç« èŠ‚æ•°: {forced_max_chapters}")
            novel_events_db = await self._extract_chapters(
                all_chapters[:forced_max_chapters]
            )
        else:
            novel_events_db = await self._adaptive_chapter_extraction(
                all_chapters,
                all_srt_events,
                len(srt_files)
            )
        
        # 5. ä¿å­˜ç»“æœ
        logger.info("4. ä¿å­˜æ•°æ®...")
        
        # Save Novel Events
        artifact_manager.save_artifact(
            novel_events_db,
            "novel_events",
            self.project_id,
            self.paths['alignment']
        )
        
        # Save Script Events (per episode)
        for episode_name, srt_events_data in all_srt_events.items():
            artifact_manager.save_artifact(
                srt_events_data,
                f"{episode_name}_script_events",
                self.project_id,
                self.paths['alignment']
            )
        
        # Final alignment with all extracted chapters
        logger.info("5. æ‰§è¡Œæœ€ç»ˆå¯¹é½...")
        final_alignment = await self._align_all_episodes(
            novel_events_db,
            all_srt_events
        )
        
        # Save Alignment
        alignment_path = artifact_manager.save_artifact(
            [item.model_dump() for item in final_alignment],
            "alignment",
            self.project_id,
            self.paths['alignment']
        )
        
        # Evaluate final quality
        quality_report = self.aligner.evaluate_alignment_quality(
            final_alignment,
            self.cfg.quality_threshold
        )
        
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š æœ€ç»ˆå¯¹é½è´¨é‡æŠ¥å‘Š")
        logger.info("="*60)
        logger.info(f"   ç»¼åˆå¾—åˆ†: {quality_report.overall_score:.2f}/100")
        logger.info(f"   å¹³å‡ç½®ä¿¡åº¦: {quality_report.avg_confidence:.2%}")
        logger.info(f"   æ•´ä½“è¦†ç›–ç‡: {quality_report.coverage_ratio:.2%}")
        logger.info(f"   ç« èŠ‚è¿ç»­æ€§: {quality_report.continuity_score:.2%}")
        logger.info(f"   æ˜¯å¦åˆæ ¼: {'âœ… æ˜¯' if quality_report.is_qualified else 'âŒ å¦'}")
        logger.info("\n   å„é›†è¦†ç›–æƒ…å†µ:")
        for ep_cov in quality_report.episode_coverage:
            logger.info(f"     - {ep_cov.episode_name}: "
                       f"{ep_cov.matched_events}/{ep_cov.total_events} "
                       f"({ep_cov.coverage_ratio:.1%}) "
                       f"[{ep_cov.min_matched_chapter} - {ep_cov.max_matched_chapter}]")
        logger.info("="*60 + "\n")
        
        # Save quality report
        quality_report_path = artifact_manager.save_artifact(
            quality_report.model_dump(),
            "alignment_quality_report",
            self.project_id,
            self.paths['alignment']
        )
        
        # Log operation
        output_files = [alignment_path, quality_report_path]
        op_logger.log_operation(
            project_id=self.project_id,
            action="Ingestion & Alignment (Adaptive)",
            output_files=output_files,
            details=f"Processed {len(srt_files)} SRT files, extracted {len(novel_events_db)} chapters, quality score: {quality_report.overall_score:.1f}"
        )
        
        logger.info("âœ… æ•°æ®æ‘„å…¥ä¸å¯¹é½å®Œæˆï¼")

    def _split_chapters(self, novel_text: str) -> List[Tuple[str, str]]:
        """
        åˆ†å‰²å°è¯´ç« èŠ‚
        
        Returns:
            List[Tuple[str, str]]: [(ç« èŠ‚æ ‡é¢˜, ç« èŠ‚å†…å®¹), ...]
        """
        chapters = re.split(r"(ç¬¬[0-9]+ç« \s+[^\n]+)", novel_text)
        parts = chapters
        if parts and not re.match(r"(ç¬¬[0-9]+ç« \s+[^\n]+)", parts[0]):
            parts = parts[1:]
        
        result = []
        for i in range(0, len(parts), 2):
            if i+1 < len(parts):
                title = parts[i].strip()
                content = parts[i+1].strip()
                result.append((title, content))
        
        return result

    async def _parse_all_srt_files(self, srt_files: List[str]) -> Dict[str, List[Dict]]:
        """
        è§£ææ‰€æœ‰SRTæ–‡ä»¶ï¼Œæå–äº‹ä»¶æµ
        
        Returns:
            Dict[str, List[Dict]]: {episode_name: srt_events_data, ...}
        """
        all_srt_events = {}
        
        for srt_path in srt_files:
            filename = os.path.basename(srt_path)
            episode_name = os.path.splitext(filename)[0]
            logger.info(f"   - è§£æ: {filename}")
            
            with open(srt_path, 'r', encoding='utf-8') as f:
                srt_content = f.read()
            
            # Parse SRT into chunks
            srt_chunks = self._parse_srt_content(srt_content)
            
            # Extract events concurrently
            if self.cfg.enable_concurrent:
                srt_events_data = await self._extract_srt_chunks_concurrent(
                    srt_chunks, filename
                )
            else:
                srt_events_data = []
                for chunk in srt_chunks:
                    events = self.analyst.extract_events(
                        chunk['content'], 
                        f"{filename} {chunk['time']}"
                    )
                    srt_events_data.append({
                        "time": chunk['time'], 
                        "events": [e.model_dump() for e in events]
                    })
            
            all_srt_events[episode_name] = srt_events_data
        
        return all_srt_events

    def _parse_srt_content(self, srt_content: str) -> List[Dict]:
        """
        è§£æSRTå†…å®¹ä¸ºchunks
        
        Returns:
            List[Dict]: [{"time": "00:00:12", "content": "..."}, ...]
        """
        blocks = srt_content.strip().split('\n\n')
        srt_chunks = []
        current_text = []
        start_time = ""
        
        for i, block in enumerate(blocks):
            lines = block.split('\n')
            if len(lines) >= 3:
                if not start_time:
                    start_time = lines[1].split(' --> ')[0]
                current_text.append(" ".join(lines[2:]))
                if (i+1) % 10 == 0:
                    srt_chunks.append({
                        "time": start_time, 
                        "content": " ".join(current_text)
                    })
                    current_text = []
                    start_time = ""
        
        if current_text:
            srt_chunks.append({
                "time": start_time, 
                "content": " ".join(current_text)
            })
        
        return srt_chunks

    async def _adaptive_chapter_extraction(
        self,
        all_chapters: List[Tuple[str, str]],
        all_srt_events: Dict[str, List[Dict]],
        srt_count: int
    ) -> List[Dict]:
        """
        åŠ¨æ€é€‚åº”å¼ç« èŠ‚æå–
        
        ç­–ç•¥ï¼š
        1. åˆå§‹æå– srt_count * multiplier ç« 
        2. å¯¹é½å¹¶è¯„ä¼°è´¨é‡
        3. å¦‚æœè¦†ç›–ç‡ä¸è¶³ï¼Œç»§ç»­æå–
        4. å¦‚æœè´¨é‡åˆæ ¼ï¼Œå†æå–å®‰å…¨ç¼“å†²
        
        Returns:
            List[Dict]: novel_events_db
        """
        # è®¡ç®—åˆå§‹ç« èŠ‚æ•°
        initial_chapters = min(
            srt_count * self.cfg.initial_chapter_multiplier,
            len(all_chapters)
        )
        
        logger.info(f"   - åˆå§‹ç­–ç•¥: æå–å‰ {initial_chapters} ç«  "
                   f"(SRTæ•° {srt_count} Ã— å€æ•° {self.cfg.initial_chapter_multiplier})")
        
        extracted_chapters = []
        current_index = 0
        iteration = 1
        
        while current_index < len(all_chapters):
            # ç¡®å®šæœ¬æ‰¹æ¬¡æå–çš„ç« èŠ‚èŒƒå›´
            if iteration == 1:
                batch_end = initial_chapters
            else:
                batch_end = min(
                    current_index + self.cfg.batch_size,
                    len(all_chapters)
                )
            
            # æå–æœ¬æ‰¹æ¬¡ç« èŠ‚
            batch_chapters = all_chapters[current_index:batch_end]
            logger.info(f"\n   ğŸ“– ç¬¬ {iteration} è½®æå–: ç« èŠ‚ {current_index+1}-{batch_end}")
            
            batch_events = await self._extract_chapters(batch_chapters)
            extracted_chapters.extend(batch_events)
            
            # å¯¹é½å¹¶è¯„ä¼°
            logger.info(f"   ğŸ”— æ‰§è¡Œå¯¹é½è¯„ä¼°...")
            alignment = await self._align_all_episodes(
                extracted_chapters,
                all_srt_events
            )
            
            quality_report = self.aligner.evaluate_alignment_quality(
                alignment,
                self.cfg.quality_threshold
            )
            
            logger.info(f"   ğŸ“Š è´¨é‡è¯„ä¼°:")
            logger.info(f"      - ç»¼åˆå¾—åˆ†: {quality_report.overall_score:.1f}/100")
            logger.info(f"      - è¦†ç›–ç‡: {quality_report.coverage_ratio:.1%}")
            logger.info(f"      - æ˜¯å¦åˆæ ¼: {'âœ…' if quality_report.is_qualified else 'âŒ'}")
            logger.info(f"      - éœ€è¦æ›´å¤šç« èŠ‚: {'æ˜¯' if quality_report.needs_more_chapters else 'å¦'}")
            
            # åˆ¤æ–­æ˜¯å¦ç»§ç»­
            if not quality_report.needs_more_chapters:
                # è´¨é‡åˆæ ¼ä¸”è¦†ç›–å……åˆ†ï¼Œæ·»åŠ å®‰å…¨ç¼“å†²åé€€å‡º
                logger.info(f"\n   âœ… å¯¹é½è´¨é‡åˆæ ¼ï¼Œæ·»åŠ å®‰å…¨ç¼“å†²...")
                
                buffer_start = batch_end
                buffer_end = min(
                    buffer_start + self.cfg.safety_buffer_chapters,
                    len(all_chapters)
                )
                
                if buffer_start < len(all_chapters):
                    logger.info(f"   ğŸ“– å®‰å…¨ç¼“å†²: ç« èŠ‚ {buffer_start+1}-{buffer_end}")
                    buffer_chapters = all_chapters[buffer_start:buffer_end]
                    buffer_events = await self._extract_chapters(buffer_chapters)
                    extracted_chapters.extend(buffer_events)
                
                break
            
            # æ›´æ–°ç´¢å¼•ï¼Œç»§ç»­ä¸‹ä¸€è½®
            current_index = batch_end
            iteration += 1
            
            if current_index >= len(all_chapters):
                logger.info(f"\n   âš ï¸  å·²æå–æ‰€æœ‰ç« èŠ‚ï¼Œä½†è´¨é‡ä»æœªè¾¾æ ‡")
                break
        
        logger.info(f"\n   âœ… ç« èŠ‚æå–å®Œæˆ: å…±æå– {len(extracted_chapters)} ç« ")
        return extracted_chapters

    async def _extract_chapters(
        self,
        chapters: List[Tuple[str, str]]
    ) -> List[Dict]:
        """
        å¹¶å‘æå–ç« èŠ‚äº‹ä»¶
        
        ä½¿ç”¨ asyncio.Semaphore é™åˆ¶å¹¶å‘æ•°ï¼Œé¿å… API rate limitã€‚
        
        Args:
            chapters: [(title, content), ...]
            
        Returns:
            List[Dict]: [{"id": title, "events": [...]}, ...]
        """
        if not self.cfg.enable_concurrent or len(chapters) == 1:
            # å•ç« èŠ‚æˆ–ç¦ç”¨å¹¶å‘æ—¶ä½¿ç”¨ä¸²è¡Œ
            novel_events = []
            for title, content in chapters:
                logger.info(f"      - åˆ†æ: {title}")
                events = self.analyst.extract_events(content, title)
                novel_events.append({
                    "id": title,
                    "events": [e.model_dump() for e in events]
                })
            return novel_events
        
        # å¹¶å‘æå–
        logger.info(f"      - å¹¶å‘åˆ†æ {len(chapters)} ç«  (æœ€å¤§å¹¶å‘: {self.cfg.max_concurrent_requests})")
        
        semaphore = asyncio.Semaphore(self.cfg.max_concurrent_requests)
        
        async def extract_with_limit(title: str, content: str) -> Tuple[str, List]:
            async with semaphore:
                logger.info(f"        â†’ å¼€å§‹: {title}")
                events = await self.analyst.extract_events_async(content, title)
                logger.info(f"        âœ“ å®Œæˆ: {title}")
                return (title, events)
        
        # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
        tasks = [extract_with_limit(title, content) for title, content in chapters]
        
        # å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†ç»“æœ
        novel_events = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                title = chapters[i][0]
                logger.error(f"      âœ— æå–å¤±è´¥: {title} - {result}")
                # æ·»åŠ ç©ºäº‹ä»¶åˆ—è¡¨
                novel_events.append({"id": title, "events": []})
            else:
                title, events = result
                novel_events.append({
                    "id": title,
                    "events": [e.model_dump() for e in events]
                })
        
        return novel_events
    
    async def _extract_srt_chunks_concurrent(
        self,
        srt_chunks: List[Dict],
        filename: str
    ) -> List[Dict]:
        """
        å¹¶å‘æå–SRT chunksçš„äº‹ä»¶
        
        Args:
            srt_chunks: [{"time": "00:00:12", "content": "..."}, ...]
            filename: SRTæ–‡ä»¶å
            
        Returns:
            List[Dict]: [{"time": "...", "events": [...]}, ...]
        """
        if len(srt_chunks) == 1:
            # å•chunkæ—¶ç›´æ¥è°ƒç”¨
            chunk = srt_chunks[0]
            events = self.analyst.extract_events(
                chunk['content'],
                f"{filename} {chunk['time']}"
            )
            return [{"time": chunk['time'], "events": [e.model_dump() for e in events]}]
        
        semaphore = asyncio.Semaphore(self.cfg.max_concurrent_requests)
        
        async def extract_chunk_with_limit(chunk: Dict) -> Tuple[str, List]:
            async with semaphore:
                events = await self.analyst.extract_events_async(
                    chunk['content'],
                    f"{filename} {chunk['time']}"
                )
                return (chunk['time'], events)
        
        # å¹¶å‘æ‰§è¡Œ
        tasks = [extract_chunk_with_limit(chunk) for chunk in srt_chunks]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†ç»“æœ
        srt_events_data = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"      âœ— SRT chunkæå–å¤±è´¥: {srt_chunks[i]['time']} - {result}")
                srt_events_data.append({
                    "time": srt_chunks[i]['time'],
                    "events": []
                })
            else:
                time_point, events = result
                srt_events_data.append({
                    "time": time_point,
                    "events": [e.model_dump() for e in events]
                })
        
        return srt_events_data

    async def _align_all_episodes(
        self,
        novel_events_db: List[Dict],
        all_srt_events: Dict[str, List[Dict]]
    ) -> List:
        """
        å¯¹æ‰€æœ‰episodeæ‰§è¡Œå¯¹é½
        
        Returns:
            List[AlignmentItem]
        """
        all_alignment_results = []
        
        for episode_name, srt_events_data in all_srt_events.items():
            alignment = self.aligner.align_script_with_novel(
                novel_events_db,
                srt_events_data
            )
            all_alignment_results.extend(alignment)
        
        return all_alignment_results
