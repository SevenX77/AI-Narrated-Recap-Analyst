"""
Ingestion Workflow v2 - åŸºäºæ–°çš„ä¸‰å±‚æ•°æ®æ¨¡å‹å’Œä¸¤çº§åŒ¹é…ç­–ç•¥

æ•°æ®æµï¼š
1. SRT blocks â†’ Sentences (å¥å­è¿˜åŸ)
2. Sentences â†’ Semantic Blocks (æ„æ€å—åˆ’åˆ†)
3. Semantic Blocks â†’ Events (äº‹ä»¶èšåˆ)
4. Events (Script) vs Events (Novel) â†’ Two-Level Matching

ä¸¤çº§åŒ¹é…ï¼š
- Level 1: Eventçº§ç²—åŒ¹é…ï¼ˆå¿«é€Ÿå®šä½å€™é€‰ï¼‰
- Level 2: SemanticBlocké“¾ç»†éªŒè¯ï¼ˆç²¾ç¡®ç¡®è®¤ï¼‰
"""

import os
import re
import json
import asyncio
from typing import List, Dict, Tuple
from src.core.interfaces import BaseWorkflow
from src.core.project_manager import project_manager
from src.core.artifact_manager import artifact_manager
from src.core.config import config
from src.agents.deepseek_analyst import get_llm_client
from src.modules.alignment.deepseek_alignment_engine_v2 import DeepSeekAlignmentEngineV2
from src.core.schemas import Sentence, SemanticBlock, Event, EventAlignment
from src.utils.logger import logger, op_logger


class IngestionWorkflow(BaseWorkflow):
    """
    Ingestion Workflow v2
    
    ä½¿ç”¨æ–°çš„ä¸‰å±‚æ•°æ®æ¨¡å‹å’Œä¸¤çº§åŒ¹é…ç­–ç•¥
    """
    
    def __init__(self, project_id: str):
        super().__init__()
        self.project_id = project_id
        self.paths = project_manager.get_project_paths(project_id)
        self.client = get_llm_client()
        self.aligner = DeepSeekAlignmentEngineV2(self.client)
        self.cfg = config.ingestion
    
    async def run(self, **kwargs):
        """
        æ‰§è¡Œæ•°æ®æ‘„å…¥ä¸å¯¹é½æµç¨‹
        
        Args:
            **kwargs: å¯é€‰å‚æ•°
                - max_chapters: å¼ºåˆ¶æŒ‡å®šæœ€å¤§ç« èŠ‚æ•°
                - force_reprocess: å¼ºåˆ¶é‡æ–°å¤„ç†ï¼ˆå¿½ç•¥å·²æœ‰ç»“æœï¼‰ï¼Œé»˜è®¤False
        """
        self.force_reprocess = kwargs.get('force_reprocess', False)
        if self.force_reprocess:
            logger.warning("âš ï¸  å¼ºåˆ¶é‡æ–°å¤„ç†æ¨¡å¼ï¼šå°†å¿½ç•¥æ‰€æœ‰å·²æœ‰ç»“æœ")
        
        logger.info(f"ğŸš€ å¯åŠ¨æ•°æ®æ‘„å…¥ä¸å¯¹é½æµç¨‹ v2: {self.project_id}")
        
        novel_path = os.path.join(self.paths['raw'], "novel.txt")
        if not os.path.exists(novel_path):
            logger.error(f"Novel file not found: {novel_path}")
            return
        
        # 1. è¯»å–å°è¯´å¹¶åˆ†ç« 
        logger.info("=" * 60)
        logger.info("Step 1: è¯»å–å°è¯´å†…å®¹")
        logger.info("=" * 60)
        
        with open(novel_path, 'r', encoding='utf-8') as f:
            novel_text = f.read()
        
        all_chapters = self._split_chapters(novel_text)
        logger.info(f"âœ… å°è¯´å…± {len(all_chapters)} ç« \n")
        
        # 2. è·å–SRTæ–‡ä»¶åˆ—è¡¨
        import glob
        srt_files = sorted(glob.glob(os.path.join(self.paths['raw'], "*.srt")))
        logger.info(f"âœ… æ‰¾åˆ° {len(srt_files)} ä¸ªSRTæ–‡ä»¶\n")
        
        if not srt_files:
            logger.error("æœªæ‰¾åˆ°SRTæ–‡ä»¶")
            return
        
        # 3. å¤„ç†SRTæ–‡ä»¶ â†’ Sentences â†’ Semantic Blocks â†’ Events
        logger.info("=" * 60)
        logger.info("Step 2: å¤„ç†SRTæ–‡ä»¶ (Script)")
        logger.info("=" * 60)
        logger.info("ğŸ”„ æ–­ç‚¹ç»­ä¼ åŠŸèƒ½å·²å¯ç”¨ï¼šå°†è‡ªåŠ¨è·³è¿‡å·²å¤„ç†çš„æ–‡ä»¶\n")
        
        script_events_by_episode = await self._process_srt_files(srt_files)
        
        # 4. å¤„ç†Novel â†’ Sentences â†’ Semantic Blocks â†’ Events
        logger.info("\n" + "=" * 60)
        logger.info("Step 3: å¤„ç†å°è¯´ç« èŠ‚ (Novel)")
        logger.info("=" * 60)
        
        # ç¡®å®šè¦æå–çš„ç« èŠ‚æ•°
        forced_max_chapters = kwargs.get('max_chapters')
        if forced_max_chapters:
            chapters_to_process = all_chapters[:forced_max_chapters]
            logger.info(f"ä½¿ç”¨å¼ºåˆ¶æŒ‡å®šçš„ç« èŠ‚æ•°: {forced_max_chapters}")
        else:
            # ç®€åŒ–ç‰ˆï¼šå…ˆæå– srt_count * multiplier ç« 
            initial_chapters = min(
                len(srt_files) * self.cfg.initial_chapter_multiplier,
                len(all_chapters)
            )
            chapters_to_process = all_chapters[:initial_chapters]
            logger.info(f"åˆå§‹æå–ç­–ç•¥: {initial_chapters} ç«  "
                       f"(SRTæ•° {len(srt_files)} Ã— å€æ•° {self.cfg.initial_chapter_multiplier})")
        
        novel_events = await self._process_novel_chapters(chapters_to_process)
        
        # 5. æ‰§è¡Œä¸¤çº§åŒ¹é…
        logger.info("\n" + "=" * 60)
        logger.info("Step 4: ä¸¤çº§åŒ¹é… (Eventçº§ç²—åŒ¹é… + Blocké“¾ç»†éªŒè¯)")
        logger.info("=" * 60)
        
        all_alignments = []
        for episode_name, script_events in script_events_by_episode.items():
            logger.info(f"\nå¤„ç†é›†æ•°: {episode_name}")
            logger.info(f"  Script Events: {len(script_events)}")
            logger.info(f"  Novel Events: {len(novel_events)}")
            
            alignments = await self.aligner.match_events_two_level_async(
                script_events,
                novel_events,
                episode_name
            )
            
            all_alignments.extend(alignments)
            logger.info(f"  âœ… å®ŒæˆåŒ¹é…: {len(alignments)} ä¸ªå¯¹é½")
            
            # å¦‚æœæ˜¯ep01ï¼Œä¿å­˜Hookéƒ¨åˆ†ï¼ˆå¦‚æœæœ‰ï¼‰
            if episode_name == "ep01" and hasattr(self.aligner, 'last_hook_result'):
                hook_result = self.aligner.last_hook_result
                if hook_result and hook_result.has_hook:
                    hook_data = {
                        "episode": episode_name,
                        "has_hook": hook_result.has_hook,
                        "hook_end_index": hook_result.hook_end_index,
                        "linear_start_index": hook_result.linear_start_index,
                        "confidence": hook_result.confidence,
                        "reasoning": hook_result.reasoning,
                        "hook_blocks": [
                            {
                                "block_id": block.block_id,
                                "theme": block.theme,
                                "summary": block.summary,
                                "time_range": {
                                    "start": block.time_range.start if block.time_range else None,
                                    "end": block.time_range.end if block.time_range else None
                                } if block.time_range else None
                            }
                            for block in hook_result.hook_blocks
                        ]
                    }
                    
                    # ä¿å­˜Hookæ•°æ®
                    artifact_manager.save_artifact(
                        hook_data,
                        f"{episode_name}_hook_detection",
                        self.project_id,
                        self.paths['alignment']
                    )
                    logger.info(f"  ğŸ’¾ ä¿å­˜Hookæ£€æµ‹ç»“æœ: {len(hook_result.hook_blocks)} ä¸ªæ„æ€å—")
        
        # 6. ä¿å­˜æœ€ç»ˆç»“æœ
        logger.info("\n" + "=" * 60)
        logger.info("Step 5: ä¿å­˜æœ€ç»ˆå¯¹é½ç»“æœ")
        logger.info("=" * 60)
        logger.info("(Script Eventså’ŒNovel Eventså·²åœ¨å¤„ç†è¿‡ç¨‹ä¸­å¢é‡ä¿å­˜)")
        
        # Save Alignments
        alignment_path = artifact_manager.save_artifact(
            [a.model_dump() for a in all_alignments],
            "alignment_v2",
            self.project_id,
            self.paths['alignment']
        )
        logger.info(f"âœ… ä¿å­˜: alignment_v2")
        
        # 7. è¯„ä¼°è´¨é‡
        logger.info("\n" + "=" * 60)
        logger.info("Step 6: è´¨é‡è¯„ä¼°")
        logger.info("=" * 60)
        
        quality_report = self.aligner.evaluate_alignment_quality(
            all_alignments,
            self.cfg.quality_threshold
        )
        
        logger.info(f"\nğŸ“Š å¯¹é½è´¨é‡æŠ¥å‘Š:")
        logger.info(f"  ç»¼åˆå¾—åˆ†: {quality_report.overall_score:.2f}/100")
        logger.info(f"  å¹³å‡ç½®ä¿¡åº¦: {quality_report.avg_confidence:.2%}")
        logger.info(f"  æ˜¯å¦åˆæ ¼: {'âœ… æ˜¯' if quality_report.is_qualified else 'âŒ å¦'}")
        logger.info(f"\n  è¯¦ç»†ä¿¡æ¯:")
        for key, value in quality_report.details.items():
            logger.info(f"    - {key}: {value}")
        
        # Save quality report
        quality_report_path = artifact_manager.save_artifact(
            quality_report.model_dump(),
            "alignment_quality_report_v2",
            self.project_id,
            self.paths['alignment']
        )
        logger.info(f"\nâœ… ä¿å­˜: alignment_quality_report_v2")
        
        # Log operation
        op_logger.log_operation(
            project_id=self.project_id,
            action="Ingestion & Alignment v2 (Two-Level Matching)",
            output_files=[alignment_path, quality_report_path],
            details=f"Processed {len(srt_files)} SRT files, {len(chapters_to_process)} chapters, "
                   f"{len(all_alignments)} alignments, quality: {quality_report.overall_score:.1f}"
        )
        
        logger.info("\n" + "=" * 60)
        logger.info("âœ… æ•°æ®æ‘„å…¥ä¸å¯¹é½å®Œæˆï¼")
        logger.info("=" * 60 + "\n")
    
    # ==================== SRTå¤„ç†æµç¨‹ ====================
    
    async def _process_srt_files(
        self,
        srt_files: List[str]
    ) -> Dict[str, List[Event]]:
        """
        å¤„ç†SRTæ–‡ä»¶ï¼šSRT blocks â†’ Sentences â†’ Semantic Blocks â†’ Events
        
        Returns:
            Dict[str, List[Event]]: {episode_name: [Event, ...], ...}
        """
        script_events_by_episode = {}
        
        for idx, srt_path in enumerate(srt_files, 1):
            filename = os.path.basename(srt_path)
            episode_name = os.path.splitext(filename)[0]
            logger.info(f"\n{'â”'*60}")
            logger.info(f"ğŸ“º å¤„ç†SRT [{idx}/{len(srt_files)}]: {filename}")
            logger.info(f"{'â”'*60}")
            
            # ğŸ”„ æ–­ç‚¹ç»­ä¼ ï¼šæ£€æŸ¥æ˜¯å¦å·²å¤„ç†
            existing_file = os.path.join(
                self.paths['alignment'], 
                f"{episode_name}_script_events_v2_latest.json"
            )
            if os.path.exists(existing_file) and not self.force_reprocess:
                logger.info(f"  â™»ï¸  å‘ç°å·²å­˜åœ¨çš„ç»“æœï¼Œè·³è¿‡å¤„ç†")
                try:
                    with open(existing_file, 'r', encoding='utf-8') as f:
                        events_data = json.load(f)
                    events = [Event(**e) for e in events_data]
                    script_events_by_episode[episode_name] = events
                    logger.info(f"  âœ… åŠ è½½å·²æœ‰ç»“æœ: {len(events)} events\n")
                    continue
                except Exception as e:
                    logger.warning(f"  âš ï¸  åŠ è½½å·²æœ‰ç»“æœå¤±è´¥: {e}ï¼Œå°†é‡æ–°å¤„ç†")
                    pass
            
            with open(srt_path, 'r', encoding='utf-8') as f:
                srt_content = f.read()
            
            logger.info(f"ğŸ“„ SRTæ–‡ä»¶å¤§å°: {len(srt_content)} å­—ç¬¦")
            
            # Step 2.1: è§£æSRT blocks
            srt_blocks = self._parse_srt_blocks(srt_content)
            logger.info(f"  âœ“ è§£æSRT: {len(srt_blocks)} blocks")
            
            # Step 2.2: SRT blocks â†’ Sentences
            sentences = await self.aligner.restore_sentences_from_srt_async(srt_blocks)
            logger.info(f"  âœ“ å¥å­è¿˜åŸ: {len(sentences)} å¥å­")
            
            # Step 2.3: Sentences â†’ Semantic Blocks
            semantic_blocks = await self.aligner.segment_semantic_blocks_async(
                sentences,
                source_type="script",
                context_info=f"Episode: {episode_name}"
            )
            logger.info(f"  âœ“ æ„æ€å—åˆ’åˆ†: {len(semantic_blocks)} blocks")
            
            # Step 2.4: Semantic Blocks â†’ Events
            events = await self.aligner.aggregate_events_async(
                semantic_blocks,
                source_type="script",
                context_info=f"Episode: {episode_name}"
            )
            logger.info(f"  âœ“ äº‹ä»¶èšåˆ: {len(events)} events")
            
            # ä¸ºæ¯ä¸ªeventè®¾ç½®episode
            for event in events:
                event.episode = episode_name
            
            script_events_by_episode[episode_name] = events
            
            # ğŸ’¾ å¢é‡ä¿å­˜ï¼šç«‹å³ä¿å­˜Script Events
            artifact_manager.save_artifact(
                [e.model_dump() for e in events],
                f"{episode_name}_script_events_v2",
                self.project_id,
                self.paths['alignment']
            )
            logger.info(f"  ğŸ’¾ å·²ä¿å­˜: {episode_name}_script_events_v2.json")
            logger.info(f"  âœ… å®Œæˆ: {episode_name} ({len(events)} events)\n")
        
        return script_events_by_episode
    
    def _parse_srt_blocks(self, srt_content: str) -> List[Dict]:
        """
        è§£æSRTå†…å®¹ä¸ºblocks
        
        Returns:
            List[Dict]: [{"index": 1, "start": "00:00:00,000", "end": "00:00:02,000", "text": "..."}, ...]
        """
        blocks = srt_content.strip().split('\n\n')
        srt_blocks = []
        
        for block in blocks:
            lines = block.split('\n')
            if len(lines) >= 3:
                try:
                    index = int(lines[0])
                    time_parts = lines[1].split(' --> ')
                    start = time_parts[0]
                    end = time_parts[1]
                    text = " ".join(lines[2:])
                    
                    srt_blocks.append({
                        "index": index,
                        "start": start,
                        "end": end,
                        "text": text
                    })
                except:
                    continue
        
        return srt_blocks
    
    # ==================== Novelå¤„ç†æµç¨‹ ====================
    
    async def _process_novel_chapters(
        self,
        chapters: List[Tuple[str, str]]
    ) -> List[Event]:
        """
        å¤„ç†Novelç« èŠ‚ï¼šText â†’ Sentences â†’ Semantic Blocks â†’ Events
        
        Returns:
            List[Event]: Novelçš„äº‹ä»¶åˆ—è¡¨
        """
        all_novel_events = []
        
        # ğŸ”„ æ–­ç‚¹ç»­ä¼ ï¼šæ£€æŸ¥æ˜¯å¦æœ‰å·²å¤„ç†çš„Novel Events
        existing_novel_file = os.path.join(
            self.paths['alignment'], 
            "novel_events_v2_latest.json"
        )
        start_chapter_idx = 0
        
        if os.path.exists(existing_novel_file) and not self.force_reprocess:
            logger.info(f"\nâ™»ï¸  å‘ç°å·²å­˜åœ¨çš„Novel Eventsæ–‡ä»¶ï¼Œå°è¯•åŠ è½½...")
            try:
                with open(existing_novel_file, 'r', encoding='utf-8') as f:
                    events_data = json.load(f)
                all_novel_events = [Event(**e) for e in events_data]
                
                # è®¡ç®—å·²å¤„ç†åˆ°ç¬¬å‡ ç« 
                if all_novel_events:
                    max_chapter = max(
                        e.chapter_range[1] if e.chapter_range else 0 
                        for e in all_novel_events
                    )
                    start_chapter_idx = max_chapter
                    logger.info(f"  âœ… åŠ è½½å·²æœ‰ç»“æœ: {len(all_novel_events)} events (å·²å¤„ç†åˆ°ç¬¬ {max_chapter} ç« )")
                    logger.info(f"  ğŸ”„ å°†ä»ç¬¬ {start_chapter_idx + 1} ç« ç»§ç»­å¤„ç†...\n")
            except Exception as e:
                logger.warning(f"  âš ï¸  åŠ è½½å·²æœ‰ç»“æœå¤±è´¥: {e}ï¼Œå°†ä»å¤´å¼€å§‹å¤„ç†")
                all_novel_events = []
                start_chapter_idx = 0
        
        # åªå¤„ç†å‰©ä½™çš„ç« èŠ‚
        remaining_chapters = chapters[start_chapter_idx:]
        if not remaining_chapters:
            logger.info(f"\nâœ… æ‰€æœ‰ç« èŠ‚å·²å¤„ç†å®Œæˆï¼Œå…± {len(all_novel_events)} ä¸ªäº‹ä»¶")
            return all_novel_events
        
        logger.info(f"\nğŸ“š å¤„ç†å‰©ä½™ {len(remaining_chapters)} ä¸ªç« èŠ‚ (å…± {len(chapters)} ç« )...")
        
        for i, (chapter_title, chapter_content) in enumerate(remaining_chapters):
            actual_chapter_num = start_chapter_idx + i + 1
            logger.info(f"\n{'â”'*60}")
            logger.info(f"ğŸ“– å¤„ç†ç« èŠ‚ [{actual_chapter_num}/{len(chapters)}]: {chapter_title}")
            logger.info(f"{'â”'*60}")
            logger.info(f"ğŸ“„ ç« èŠ‚å†…å®¹é•¿åº¦: {len(chapter_content)} å­—ç¬¦")
            
            # Step 3.1: Text â†’ Sentences
            sentences = self.aligner.restore_sentences_from_novel(
                chapter_content,
                chapter_title
            )
            logger.info(f"  âœ“ å¥å­åˆ†å‰²: {len(sentences)} å¥å­")
            
            # Step 3.2: Sentences â†’ Semantic Blocks
            semantic_blocks = await self.aligner.segment_semantic_blocks_async(
                sentences,
                source_type="novel",
                context_info=f"Chapter: {chapter_title}"
            )
            logger.info(f"  âœ“ æ„æ€å—åˆ’åˆ†: {len(semantic_blocks)} blocks")
            
            # Step 3.3: Semantic Blocks â†’ Events
            events = await self.aligner.aggregate_events_async(
                semantic_blocks,
                source_type="novel",
                context_info=f"Chapter: {chapter_title}"
            )
            logger.info(f"  âœ“ äº‹ä»¶èšåˆ: {len(events)} events")
            
            # ä¸ºæ¯ä¸ªeventè®¾ç½®chapter_rangeï¼ˆä½¿ç”¨å®é™…ç« èŠ‚ç¼–å·ï¼‰
            for event in events:
                event.chapter_range = (actual_chapter_num, actual_chapter_num)
            
            all_novel_events.extend(events)
            
            # ğŸ’¾ å¢é‡ä¿å­˜ï¼šæ¯å¤„ç†å®Œä¸€ç« å°±ä¿å­˜å½“å‰æ‰€æœ‰Novel Events
            artifact_manager.save_artifact(
                [e.model_dump() for e in all_novel_events],
                "novel_events_v2",
                self.project_id,
                self.paths['alignment']
            )
            logger.info(f"  ğŸ’¾ å·²ä¿å­˜: novel_events_v2.json (ç´¯è®¡ {len(all_novel_events)} events)")
            logger.info(f"  âœ… å®Œæˆ: {chapter_title} (æœ¬ç«  {len(events)} events)\n")
        
        logger.info(f"\nâœ… Novelå¤„ç†å®Œæˆ: å…± {len(all_novel_events)} ä¸ªäº‹ä»¶ (å·²å¤„ç† {len(chapters)} ç« )")
        return all_novel_events
    
    # ==================== è¾…åŠ©æ–¹æ³• ====================
    
    def _split_chapters(self, novel_text: str) -> List[Tuple[str, str]]:
        """
        åˆ†å‰²å°è¯´ç« èŠ‚
        
        Returns:
            List[Tuple[str, str]]: [(ç« èŠ‚æ ‡é¢˜, ç« èŠ‚å†…å®¹), ...]
        """
        chapters = re.split(r"(ç¬¬[0-9]+ç« \s+[^\n]+)", novel_text)
        parts = chapters
        if parts and not re.match(r"(ç¬¬[0-9]+ç« \s+[^\n]+)", parts[0]):
            parts = parts[1:]
        
        result = []
        for i in range(0, len(parts), 2):
            if i+1 < len(parts):
                title = parts[i].strip()
                content = parts[i+1].strip()
                result.append((title, content))
        
        return result
