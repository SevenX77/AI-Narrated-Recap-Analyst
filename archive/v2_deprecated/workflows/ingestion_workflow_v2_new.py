"""
Ingestion Workflow v2 - åŸºäºæ–°çš„ä¸‰å±‚æ•°æ®æ¨¡å‹å’Œä¸¤çº§åŒ¹é…ç­–ç•¥

æ•°æ®æµï¼š
1. SRT blocks â†’ Sentences (å¥å­è¿˜åŸ)
2. Sentences â†’ Semantic Blocks (æ„æ€å—åˆ’åˆ†)
3. Semantic Blocks â†’ Events (äº‹ä»¶èšåˆ)
4. Events (Script) vs Events (Novel) â†’ Two-Level Matching

ä¸¤çº§åŒ¹é…ï¼š
- Level 1: Eventçº§ç²—åŒ¹é…ï¼ˆå¿«é€Ÿå®šä½å€™é€‰ï¼‰
- Level 2: SemanticBlocké“¾ç»†éªŒè¯ï¼ˆç²¾ç¡®ç¡®è®¤ï¼‰
"""

import os
import re
import json
import asyncio
from typing import List, Dict, Tuple
from src.core.interfaces import BaseWorkflow
from src.core.project_manager import project_manager
from src.core.artifact_manager import artifact_manager
from src.core.config import config
from src.agents.deepseek_analyst import get_llm_client
from src.modules.alignment.deepseek_alignment_engine_v2 import DeepSeekAlignmentEngineV2
from src.core.schemas import Sentence, SemanticBlock, Event, EventAlignment
from src.utils.logger import logger, op_logger


class IngestionWorkflowV2(BaseWorkflow):
    """
    Ingestion Workflow v2
    
    ä½¿ç”¨æ–°çš„ä¸‰å±‚æ•°æ®æ¨¡å‹å’Œä¸¤çº§åŒ¹é…ç­–ç•¥
    """
    
    def __init__(self, project_id: str):
        super().__init__()
        self.project_id = project_id
        self.paths = project_manager.get_project_paths(project_id)
        self.client = get_llm_client()
        self.aligner = DeepSeekAlignmentEngineV2(self.client)
        self.cfg = config.ingestion
    
    async def run(self, **kwargs):
        """
        æ‰§è¡Œæ•°æ®æ‘„å…¥ä¸å¯¹é½æµç¨‹
        
        Args:
            **kwargs: å¯é€‰å‚æ•°
                - max_chapters: å¼ºåˆ¶æŒ‡å®šæœ€å¤§ç« èŠ‚æ•°
        """
        logger.info(f"ğŸš€ å¯åŠ¨æ•°æ®æ‘„å…¥ä¸å¯¹é½æµç¨‹ v2: {self.project_id}")
        
        novel_path = os.path.join(self.paths['raw'], "novel.txt")
        if not os.path.exists(novel_path):
            logger.error(f"Novel file not found: {novel_path}")
            return
        
        # 1. è¯»å–å°è¯´å¹¶åˆ†ç« 
        logger.info("=" * 60)
        logger.info("Step 1: è¯»å–å°è¯´å†…å®¹")
        logger.info("=" * 60)
        
        with open(novel_path, 'r', encoding='utf-8') as f:
            novel_text = f.read()
        
        all_chapters = self._split_chapters(novel_text)
        logger.info(f"âœ… å°è¯´å…± {len(all_chapters)} ç« \n")
        
        # 2. è·å–SRTæ–‡ä»¶åˆ—è¡¨
        import glob
        srt_files = sorted(glob.glob(os.path.join(self.paths['raw'], "*.srt")))
        logger.info(f"âœ… æ‰¾åˆ° {len(srt_files)} ä¸ªSRTæ–‡ä»¶\n")
        
        if not srt_files:
            logger.error("æœªæ‰¾åˆ°SRTæ–‡ä»¶")
            return
        
        # 3. å¤„ç†SRTæ–‡ä»¶ â†’ Sentences â†’ Semantic Blocks â†’ Events
        logger.info("=" * 60)
        logger.info("Step 2: å¤„ç†SRTæ–‡ä»¶ (Script)")
        logger.info("=" * 60)
        
        script_events_by_episode = await self._process_srt_files(srt_files)
        
        # 4. å¤„ç†Novel â†’ Sentences â†’ Semantic Blocks â†’ Events
        logger.info("\n" + "=" * 60)
        logger.info("Step 3: å¤„ç†å°è¯´ç« èŠ‚ (Novel)")
        logger.info("=" * 60)
        
        # ç¡®å®šè¦æå–çš„ç« èŠ‚æ•°
        forced_max_chapters = kwargs.get('max_chapters')
        if forced_max_chapters:
            chapters_to_process = all_chapters[:forced_max_chapters]
            logger.info(f"ä½¿ç”¨å¼ºåˆ¶æŒ‡å®šçš„ç« èŠ‚æ•°: {forced_max_chapters}")
        else:
            # ç®€åŒ–ç‰ˆï¼šå…ˆæå– srt_count * multiplier ç« 
            initial_chapters = min(
                len(srt_files) * self.cfg.initial_chapter_multiplier,
                len(all_chapters)
            )
            chapters_to_process = all_chapters[:initial_chapters]
            logger.info(f"åˆå§‹æå–ç­–ç•¥: {initial_chapters} ç«  "
                       f"(SRTæ•° {len(srt_files)} Ã— å€æ•° {self.cfg.initial_chapter_multiplier})")
        
        novel_events = await self._process_novel_chapters(chapters_to_process)
        
        # 5. æ‰§è¡Œä¸¤çº§åŒ¹é…
        logger.info("\n" + "=" * 60)
        logger.info("Step 4: ä¸¤çº§åŒ¹é… (Eventçº§ç²—åŒ¹é… + Blocké“¾ç»†éªŒè¯)")
        logger.info("=" * 60)
        
        all_alignments = []
        for episode_name, script_events in script_events_by_episode.items():
            logger.info(f"\nå¤„ç†é›†æ•°: {episode_name}")
            logger.info(f"  Script Events: {len(script_events)}")
            logger.info(f"  Novel Events: {len(novel_events)}")
            
            alignments = await self.aligner.match_events_two_level_async(
                script_events,
                novel_events,
                episode_name
            )
            
            all_alignments.extend(alignments)
            logger.info(f"  âœ… å®ŒæˆåŒ¹é…: {len(alignments)} ä¸ªå¯¹é½")
        
        # 6. ä¿å­˜ç»“æœ
        logger.info("\n" + "=" * 60)
        logger.info("Step 5: ä¿å­˜æ•°æ®")
        logger.info("=" * 60)
        
        # Save Script Events (per episode)
        for episode_name, script_events in script_events_by_episode.items():
            artifact_manager.save_artifact(
                [e.model_dump() for e in script_events],
                f"{episode_name}_script_events_v2",
                self.project_id,
                self.paths['alignment']
            )
            logger.info(f"âœ… ä¿å­˜: {episode_name}_script_events_v2")
        
        # Save Novel Events
        artifact_manager.save_artifact(
            [e.model_dump() for e in novel_events],
            "novel_events_v2",
            self.project_id,
            self.paths['alignment']
        )
        logger.info(f"âœ… ä¿å­˜: novel_events_v2")
        
        # Save Alignments
        alignment_path = artifact_manager.save_artifact(
            [a.model_dump() for a in all_alignments],
            "alignment_v2",
            self.project_id,
            self.paths['alignment']
        )
        logger.info(f"âœ… ä¿å­˜: alignment_v2")
        
        # 7. è¯„ä¼°è´¨é‡
        logger.info("\n" + "=" * 60)
        logger.info("Step 6: è´¨é‡è¯„ä¼°")
        logger.info("=" * 60)
        
        quality_report = self.aligner.evaluate_alignment_quality(
            all_alignments,
            self.cfg.quality_threshold
        )
        
        logger.info(f"\nğŸ“Š å¯¹é½è´¨é‡æŠ¥å‘Š:")
        logger.info(f"  ç»¼åˆå¾—åˆ†: {quality_report.overall_score:.2f}/100")
        logger.info(f"  å¹³å‡ç½®ä¿¡åº¦: {quality_report.avg_confidence:.2%}")
        logger.info(f"  æ˜¯å¦åˆæ ¼: {'âœ… æ˜¯' if quality_report.is_qualified else 'âŒ å¦'}")
        logger.info(f"\n  è¯¦ç»†ä¿¡æ¯:")
        for key, value in quality_report.details.items():
            logger.info(f"    - {key}: {value}")
        
        # Save quality report
        quality_report_path = artifact_manager.save_artifact(
            quality_report.model_dump(),
            "alignment_quality_report_v2",
            self.project_id,
            self.paths['alignment']
        )
        logger.info(f"\nâœ… ä¿å­˜: alignment_quality_report_v2")
        
        # Log operation
        op_logger.log_operation(
            project_id=self.project_id,
            action="Ingestion & Alignment v2 (Two-Level Matching)",
            output_files=[alignment_path, quality_report_path],
            details=f"Processed {len(srt_files)} SRT files, {len(chapters_to_process)} chapters, "
                   f"{len(all_alignments)} alignments, quality: {quality_report.overall_score:.1f}"
        )
        
        logger.info("\n" + "=" * 60)
        logger.info("âœ… æ•°æ®æ‘„å…¥ä¸å¯¹é½å®Œæˆï¼")
        logger.info("=" * 60 + "\n")
    
    # ==================== SRTå¤„ç†æµç¨‹ ====================
    
    async def _process_srt_files(
        self,
        srt_files: List[str]
    ) -> Dict[str, List[Event]]:
        """
        å¤„ç†SRTæ–‡ä»¶ï¼šSRT blocks â†’ Sentences â†’ Semantic Blocks â†’ Events
        
        Returns:
            Dict[str, List[Event]]: {episode_name: [Event, ...], ...}
        """
        script_events_by_episode = {}
        
        for srt_path in srt_files:
            filename = os.path.basename(srt_path)
            episode_name = os.path.splitext(filename)[0]
            logger.info(f"\nå¤„ç†: {filename}")
            
            with open(srt_path, 'r', encoding='utf-8') as f:
                srt_content = f.read()
            
            # Step 2.1: è§£æSRT blocks
            srt_blocks = self._parse_srt_blocks(srt_content)
            logger.info(f"  âœ“ è§£æSRT: {len(srt_blocks)} blocks")
            
            # Step 2.2: SRT blocks â†’ Sentences
            sentences = await self.aligner.restore_sentences_from_srt_async(srt_blocks)
            logger.info(f"  âœ“ å¥å­è¿˜åŸ: {len(sentences)} å¥å­")
            
            # Step 2.3: Sentences â†’ Semantic Blocks
            semantic_blocks = await self.aligner.segment_semantic_blocks_async(
                sentences,
                source_type="script",
                context_info=f"Episode: {episode_name}"
            )
            logger.info(f"  âœ“ æ„æ€å—åˆ’åˆ†: {len(semantic_blocks)} blocks")
            
            # Step 2.4: Semantic Blocks â†’ Events
            events = await self.aligner.aggregate_events_async(
                semantic_blocks,
                source_type="script",
                context_info=f"Episode: {episode_name}"
            )
            logger.info(f"  âœ“ äº‹ä»¶èšåˆ: {len(events)} events")
            
            # ä¸ºæ¯ä¸ªeventè®¾ç½®episode
            for event in events:
                event.episode = episode_name
            
            script_events_by_episode[episode_name] = events
        
        return script_events_by_episode
    
    def _parse_srt_blocks(self, srt_content: str) -> List[Dict]:
        """
        è§£æSRTå†…å®¹ä¸ºblocks
        
        Returns:
            List[Dict]: [{"index": 1, "start": "00:00:00,000", "end": "00:00:02,000", "text": "..."}, ...]
        """
        blocks = srt_content.strip().split('\n\n')
        srt_blocks = []
        
        for block in blocks:
            lines = block.split('\n')
            if len(lines) >= 3:
                try:
                    index = int(lines[0])
                    time_parts = lines[1].split(' --> ')
                    start = time_parts[0]
                    end = time_parts[1]
                    text = " ".join(lines[2:])
                    
                    srt_blocks.append({
                        "index": index,
                        "start": start,
                        "end": end,
                        "text": text
                    })
                except:
                    continue
        
        return srt_blocks
    
    # ==================== Novelå¤„ç†æµç¨‹ ====================
    
    async def _process_novel_chapters(
        self,
        chapters: List[Tuple[str, str]]
    ) -> List[Event]:
        """
        å¤„ç†Novelç« èŠ‚ï¼šText â†’ Sentences â†’ Semantic Blocks â†’ Events
        
        Returns:
            List[Event]: Novelçš„äº‹ä»¶åˆ—è¡¨
        """
        all_novel_events = []
        
        logger.info(f"\nå¤„ç† {len(chapters)} ä¸ªç« èŠ‚...")
        
        for i, (chapter_title, chapter_content) in enumerate(chapters):
            logger.info(f"\nå¤„ç†: {chapter_title}")
            
            # Step 3.1: Text â†’ Sentences
            sentences = self.aligner.restore_sentences_from_novel(
                chapter_content,
                chapter_title
            )
            logger.info(f"  âœ“ å¥å­åˆ†å‰²: {len(sentences)} å¥å­")
            
            # Step 3.2: Sentences â†’ Semantic Blocks
            semantic_blocks = await self.aligner.segment_semantic_blocks_async(
                sentences,
                source_type="novel",
                context_info=f"Chapter: {chapter_title}"
            )
            logger.info(f"  âœ“ æ„æ€å—åˆ’åˆ†: {len(semantic_blocks)} blocks")
            
            # Step 3.3: Semantic Blocks â†’ Events
            events = await self.aligner.aggregate_events_async(
                semantic_blocks,
                source_type="novel",
                context_info=f"Chapter: {chapter_title}"
            )
            logger.info(f"  âœ“ äº‹ä»¶èšåˆ: {len(events)} events")
            
            # ä¸ºæ¯ä¸ªeventè®¾ç½®chapter_range
            chapter_num = i + 1
            for event in events:
                event.chapter_range = (chapter_num, chapter_num)
            
            all_novel_events.extend(events)
        
        logger.info(f"\nâœ… Novelå¤„ç†å®Œæˆ: å…± {len(all_novel_events)} ä¸ªäº‹ä»¶")
        return all_novel_events
    
    # ==================== è¾…åŠ©æ–¹æ³• ====================
    
    def _split_chapters(self, novel_text: str) -> List[Tuple[str, str]]:
        """
        åˆ†å‰²å°è¯´ç« èŠ‚
        
        Returns:
            List[Tuple[str, str]]: [(ç« èŠ‚æ ‡é¢˜, ç« èŠ‚å†…å®¹), ...]
        """
        chapters = re.split(r"(ç¬¬[0-9]+ç« \s+[^\n]+)", novel_text)
        parts = chapters
        if parts and not re.match(r"(ç¬¬[0-9]+ç« \s+[^\n]+)", parts[0]):
            parts = parts[1:]
        
        result = []
        for i in range(0, len(parts), 2):
            if i+1 < len(parts):
                title = parts[i].strip()
                content = parts[i+1].strip()
                result.append((title, content))
        
        return result
