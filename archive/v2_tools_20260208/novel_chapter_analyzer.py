"""
Novel Chapter Functional Analyzer
å°è¯´ç« èŠ‚åŠŸèƒ½æ®µåˆ†æå·¥å…· - ä½¿ç”¨LLMè¿›è¡Œå™äº‹åŠŸèƒ½çº§åˆ«çš„åˆ†æ®µå’Œæ ‡æ³¨
"""

import json
import logging
from pathlib import Path
from typing import Dict, Any, Optional, List
from openai import OpenAI

from src.core.interfaces import BaseTool
from src.core.config import config
from src.core.schemas_novel_analysis import (
    ChapterFunctionalAnalysis,
    FunctionalSegment,
    ChapterSummary,
    ChapterStructureInsight
)
from src.utils.prompt_loader import load_prompts

logger = logging.getLogger(__name__)


class NovelChapterAnalyzer(BaseTool):
    """
    å°è¯´ç« èŠ‚åŠŸèƒ½æ®µåˆ†æå·¥å…·
    
    åŠŸèƒ½ï¼š
    1. ä½¿ç”¨LLMæŒ‰å™äº‹åŠŸèƒ½å°†ç« èŠ‚åˆ†æ®µï¼ˆåŠŸèƒ½æ®µçº§åˆ«ï¼Œéè‡ªç„¶æ®µï¼‰
    2. ä¸ºæ¯ä¸ªåŠŸèƒ½æ®µæ ‡æ³¨å¤šç»´åº¦æ ‡ç­¾ï¼ˆå™äº‹åŠŸèƒ½ã€ç»“æ„ã€è§’è‰²ã€ä¼˜å…ˆçº§ï¼‰
    3. æä¾›æµ“ç¼©å»ºè®®
    4. ç”Ÿæˆç« èŠ‚æ‘˜è¦å’Œç»“æ„æ´å¯Ÿ
    5. è¾“å‡ºMarkdownå’ŒJSONä¸¤ç§æ ¼å¼
    
    ä¸ NovelSegmentationAnalyzer çš„åŒºåˆ«ï¼š
    - NovelSegmentationAnalyzerï¼šè‡ªç„¶æ®µçº§åˆ«ï¼ˆ24ä¸ªæ®µè½/ç« ï¼‰ï¼Œé€‚åˆç²¾ç¡®å¯¹é½
    - NovelChapterAnalyzerï¼šåŠŸèƒ½æ®µçº§åˆ«ï¼ˆ11ä¸ªæ®µè½/ç« ï¼‰ï¼Œé€‚åˆäººç±»ç†è§£
    """
    
    name = "novel_chapter_analyzer"
    description = "Analyze novel chapters by narrative function (functional segments)"
    
    def __init__(self):
        """åˆå§‹åŒ–å·¥å…·"""
        self.llm_client = OpenAI(
            api_key=config.llm.api_key,
            base_url=config.llm.base_url
        )
        self.prompt_config = load_prompts("novel_chapter_functional_analysis")
        logger.info(f"Initialized {self.name} with model: {config.llm.model}")
    
    def execute(self,
                chapter_content: str,
                chapter_number: int,
                chapter_title: str,
                novel_title: str = "",
                known_characters: List[str] = None,
                known_world_settings: Dict[str, str] = None,
                previous_foreshadowing: List[str] = None) -> ChapterFunctionalAnalysis:
        """
        æ‰§è¡Œç« èŠ‚åŠŸèƒ½æ®µåˆ†æ
        
        Args:
            chapter_content: ç« èŠ‚åŸæ–‡å†…å®¹
            chapter_number: ç« èŠ‚åºå·
            chapter_title: ç« èŠ‚æ ‡é¢˜
            novel_title: å°è¯´æ ‡é¢˜
            known_characters: å·²çŸ¥è§’è‰²åˆ—è¡¨ï¼ˆå¯é€‰ï¼Œç”¨äºä¸Šä¸‹æ–‡ï¼‰
            known_world_settings: å·²çŸ¥ä¸–ç•Œè§‚è®¾å®šï¼ˆå¯é€‰ï¼‰
            previous_foreshadowing: å‰æ–‡ä¼ç¬”åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            ChapterFunctionalAnalysis: ç« èŠ‚åŠŸèƒ½æ®µåˆ†æç»“æœ
        """
        logger.info(f"Starting functional analysis for Chapter {chapter_number}: {chapter_title}")
        
        # 1. å‡†å¤‡ä¸Šä¸‹æ–‡
        context = self._prepare_context(
            known_characters or [],
            known_world_settings or {},
            previous_foreshadowing or []
        )
        
        # 2. æ„å»ºPrompt
        prompt = self._build_prompt(
            novel_title=novel_title,
            chapter_number=chapter_number,
            chapter_title=chapter_title,
            chapter_content=chapter_content,
            **context
        )
        
        # 3. è°ƒç”¨LLMï¼ˆæ”¯æŒV3 -> R1 fallbackï¼‰
        logger.info("Calling LLM for functional analysis...")
        analysis_result, used_model = self._call_llm_with_fallback(prompt)
        
        # 4. è§£æç»“æœ
        try:
            chapter_analysis = self._parse_result(analysis_result, chapter_number)
            logger.info(f"Analysis successful with {used_model}: {len(chapter_analysis.segments)} functional segments")
            return chapter_analysis
        except Exception as e:
            logger.error(f"Failed to parse LLM result for chapter {chapter_number}: {e}")
            raise RuntimeError(f"Result parsing failed: {e}")
    
    def _prepare_context(self,
                        known_characters: List[str],
                        known_world_settings: Dict[str, str],
                        previous_foreshadowing: List[str]) -> Dict[str, str]:
        """å‡†å¤‡ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        return {
            "known_characters": ", ".join(known_characters) if known_characters else "æ— ",
            "known_world_settings": json.dumps(known_world_settings, ensure_ascii=False) if known_world_settings else "æ— ",
            "previous_foreshadowing": ", ".join(previous_foreshadowing) if previous_foreshadowing else "æ— "
        }
    
    def _build_prompt(self,
                     novel_title: str,
                     chapter_number: int,
                     chapter_title: str,
                     chapter_content: str,
                     **context) -> str:
        """æ„å»ºLLM Prompt"""
        system_prompt = self.prompt_config.get("novel_chapter_functional_analysis", {}).get("system", "")
        user_template = self.prompt_config.get("novel_chapter_functional_analysis", {}).get("user", "")
        
        user_prompt = user_template.format(
            novel_title=novel_title or "æœªçŸ¥",
            chapter_number=chapter_number,
            chapter_title=chapter_title,
            chapter_content=chapter_content,
            chapter_id=f"{chapter_number:04d}",
            **context
        )
        
        return f"{system_prompt}\n\n{user_prompt}"
    
    def _call_llm_with_fallback(self, prompt: str) -> tuple[str, str]:
        """
        è°ƒç”¨LLMè¿›è¡Œåˆ†æï¼Œæ”¯æŒå¤šæä¾›å•†å’Œfallback
        
        - Claude: ç›´æ¥è°ƒç”¨ï¼Œæ— fallback
        - DeepSeek: æ”¯æŒ V3 -> R1 fallback
        
        Returns:
            tuple: (LLMè¾“å‡º, ä½¿ç”¨çš„æ¨¡å‹åç§°)
        """
        # å¦‚æœæ˜¯ Claudeï¼Œç›´æ¥è°ƒç”¨ï¼ˆä¸ä½¿ç”¨ fallback é€»è¾‘ï¼‰
        if config.llm.provider == "claude":
            return self._call_claude_model(prompt)
        
        # DeepSeek: ä½¿ç”¨åŒæ¨¡å‹é€»è¾‘
        # å°è¯•ä¸»æ¨¡å‹ (R1)
        try:
            logger.info(f"Trying primary model: {config.llm.primary_model}")
            response = self.llm_client.chat.completions.create(
                model=config.llm.primary_model,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=1.0,  # R1 æ¨èä½¿ç”¨ 1.0
                max_tokens=8000
            )
            result = response.choices[0].message.content
            
            # éªŒè¯ç»“æœï¼ˆæ£€æŸ¥æ˜¯å¦è¿‡åº¦èšåˆï¼‰
            if config.llm.enable_fallback and config.llm.fallback_on_validation_fail:
                if self._should_fallback(result):
                    logger.warning(f"Primary model result failed validation, falling back to {config.llm.fallback_model}")
                    return self._call_fallback_model(prompt)
            
            return result, config.llm.primary_model
            
        except Exception as e:
            logger.error(f"Primary model ({config.llm.primary_model}) failed: {e}")
            
            if config.llm.enable_fallback and config.llm.fallback_on_error:
                logger.info(f"Falling back to {config.llm.fallback_model}")
                return self._call_fallback_model(prompt)
            else:
                raise RuntimeError(f"LLM API error: {e}")
    
    def _call_claude_model(self, prompt: str) -> tuple[str, str]:
        """è°ƒç”¨ Claude æ¨¡å‹"""
        try:
            logger.info(f"Calling Claude model: {config.llm.model_name}")
            response = self.llm_client.chat.completions.create(
                model=config.llm.model_name,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=config.llm.claude_temperature,
                max_tokens=config.llm.claude_max_tokens
            )
            
            result = response.choices[0].message.content
            logger.info(f"Claude response received: {len(result)} characters")
            
            return result, config.llm.model_name
            
        except Exception as e:
            logger.error(f"Claude model ({config.llm.model_name}) failed: {e}")
            raise RuntimeError(f"Claude API error: {e}")
    
    def _call_fallback_model(self, prompt: str) -> tuple[str, str]:
        """è°ƒç”¨ fallback æ¨¡å‹ (R1)"""
        try:
            response = self.llm_client.chat.completions.create(
                model=config.llm.fallback_model,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=1.0,  # R1 æ¨èä½¿ç”¨ 1.0
                max_tokens=8000
            )
            
            # R1 è¿”å›æ¨ç†è¿‡ç¨‹å’Œæœ€ç»ˆå†…å®¹
            if hasattr(response.choices[0].message, 'reasoning_content'):
                reasoning = response.choices[0].message.reasoning_content
                logger.info(f"R1 reasoning: {reasoning[:200]}...")
            
            return response.choices[0].message.content, config.llm.fallback_model
            
        except Exception as e:
            logger.error(f"Fallback model ({config.llm.fallback_model}) also failed: {e}")
            raise RuntimeError(f"Both primary and fallback models failed: {e}")
    
    def _should_fallback(self, llm_output: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥fallbackåˆ°R1
        
        æ£€æŸ¥V3çš„è¾“å‡ºæ˜¯å¦æœ‰æ˜æ˜¾é—®é¢˜ï¼š
        1. æ®µè½1å­—æ•° < 120 (åªæœ‰å¹¿æ’­ï¼Œæ²¡æœ‰ååº”)
        2. æ®µè½1å­—æ•° > 400 (è¿‡åº¦èšåˆ)
        3. JSONè§£æå¤±è´¥
        """
        try:
            # å°è¯•æå–ç¬¬ä¸€ä¸ªæ®µè½çš„å­—æ•°
            if "```json" in llm_output:
                json_text = llm_output.split("```json")[1].split("```")[0].strip()
            elif "```" in llm_output:
                json_text = llm_output.split("```")[1].split("```")[0].strip()
            else:
                json_text = llm_output
            
            data = json.loads(json_text)
            segments = data.get("segments", [])
            
            if segments:
                first_seg_word_count = segments[0].get("metadata", {}).get("word_count", 0)
                
                # æ®µè½1å­—æ•°å¼‚å¸¸
                if first_seg_word_count < 120:
                    logger.warning(f"First segment too short: {first_seg_word_count} < 120")
                    return True
                if first_seg_word_count > 400:
                    logger.warning(f"First segment too long: {first_seg_word_count} > 400 (over-aggregation)")
                    return True
            
            return False
            
        except Exception as e:
            logger.warning(f"Validation check failed: {e}, will not fallback")
            return False
    
    def _call_llm(self, prompt: str) -> str:
        """è°ƒç”¨LLMè¿›è¡Œåˆ†æï¼ˆæ—§æ¥å£ï¼Œä¿ç•™å…¼å®¹æ€§ï¼‰"""
        result, _ = self._call_llm_with_fallback(prompt)
        return result
    
    def _parse_result(self, llm_output: str, chapter_number: int) -> ChapterFunctionalAnalysis:
        """è§£æLLMè¾“å‡ºçš„JSONç»“æœ"""
        # æå–JSONå†…å®¹ï¼ˆå¯èƒ½è¢«åŒ…è£¹åœ¨```json```ä¸­ï¼‰
        json_text = llm_output
        if "```json" in llm_output:
            json_text = llm_output.split("```json")[1].split("```")[0].strip()
        elif "```" in llm_output:
            json_text = llm_output.split("```")[1].split("```")[0].strip()
        
        # è§£æJSON
        try:
            data = json.loads(json_text)
        except json.JSONDecodeError as e:
            logger.error(f"JSON decode error: {e}")
            logger.error(f"LLM output: {llm_output[:500]}...")
            raise ValueError(f"Invalid JSON output from LLM: {e}")
        
        # æ„å»ºPydanticæ¨¡å‹
        try:
            return ChapterFunctionalAnalysis(**data)
        except Exception as e:
            logger.error(f"Pydantic validation error: {e}")
            raise ValueError(f"Failed to validate analysis result: {e}")
    
    def save_markdown(self,
                     analysis: ChapterFunctionalAnalysis,
                     output_path: Path) -> Path:
        """
        å°†åˆ†æç»“æœä¿å­˜ä¸ºMarkdownæ ¼å¼
        
        Args:
            analysis: ç« èŠ‚åˆ†æç»“æœ
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        logger.info(f"Saving Markdown to {output_path}")
        
        markdown_content = self._generate_markdown(analysis)
        
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        
        logger.info(f"Markdown saved successfully")
        return output_path
    
    def save_json(self,
                 analysis: ChapterFunctionalAnalysis,
                 output_path: Path) -> Path:
        """
        å°†åˆ†æç»“æœä¿å­˜ä¸ºJSONæ ¼å¼
        
        Args:
            analysis: ç« èŠ‚åˆ†æç»“æœ
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        logger.info(f"Saving JSON to {output_path}")
        
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(analysis.model_dump(mode='json'), f, ensure_ascii=False, indent=2)
        
        logger.info(f"JSON saved successfully")
        return output_path
    
    def _generate_markdown(self, analysis: ChapterFunctionalAnalysis) -> str:
        """ç”ŸæˆMarkdownæ ¼å¼çš„åˆ†ææŠ¥å‘Š"""
        md = []
        
        # æ ‡é¢˜å’Œå…ƒä¿¡æ¯
        md.append(f"# ç¬¬{analysis.chapter_number}ç« å®Œæ•´åˆ†æ®µåˆ†æ\n")
        md.append(f"> **ç« èŠ‚**: ç¬¬{analysis.chapter_number}ç«  - {analysis.chapter_title}")
        md.append(f"> **åˆ†ææ–¹æ³•**: [å°è¯´å™äº‹åˆ†æ®µåˆ†ææ–¹æ³•è®º](../../../../docs/NOVEL_SEGMENTATION_METHODOLOGY.md)")
        md.append(f"> **åˆ†ææ—¥æœŸ**: {analysis.analyzed_at.strftime('%Y-%m-%d')}")
        md.append(f"> **åˆ†æç‰ˆæœ¬**: {analysis.version}\n")
        md.append("---\n")
        
        # å„åŠŸèƒ½æ®µ
        for i, seg in enumerate(analysis.segments, 1):
            md.append(f"## {seg.title}\n")
            md.append("```")
            md.append(seg.content)
            md.append("```\n")
            
            # å™äº‹åŠŸèƒ½
            md.append("**[å™äº‹åŠŸèƒ½]**")
            for func in seg.tags.narrative_function:
                md.append(f"- {func}")
            md.append("")
            
            # å™äº‹ç»“æ„
            if seg.tags.structure:
                md.append("**[å™äº‹ç»“æ„]**")
                for struct in seg.tags.structure:
                    md.append(f"- {struct}")
                md.append("")
            
            # è§’è‰²ä¸å…³ç³»
            if seg.tags.character:
                md.append("**[è§’è‰²ä¸å…³ç³»]**")
                for char in seg.tags.character:
                    md.append(f"- {char}")
                md.append("")
            
            # æµ“ç¼©ä¼˜å…ˆçº§
            md.append("**[æµ“ç¼©ä¼˜å…ˆçº§]**")
            md.append(f"- {seg.tags.priority}")
            md.append("")
            
            # æµ“ç¼©å»ºè®®
            md.append("**[æµ“ç¼©å»ºè®®]**")
            md.append(f"- {seg.condensation_suggestion}")
            md.append("")
            
            # æ—¶ç©º
            if seg.tags.location or seg.tags.time:
                md.append("**[æ—¶ç©º]**")
                if seg.tags.location:
                    md.append(f"- åœ°ç‚¹ï¼š{seg.tags.location}")
                if seg.tags.time:
                    md.append(f"- æ—¶é—´ï¼š{seg.tags.time}")
                md.append("")
            
            # å…ƒæ•°æ®
            if seg.metadata.contains_first_appearance or seg.metadata.repetition_items or seg.metadata.foreshadowing:
                md.append("**[å…ƒæ•°æ®]**")
                if seg.metadata.contains_first_appearance:
                    md.append("- åŒ…å«é¦–æ¬¡å‡ºç°çš„è®¾å®š/é“å…·")
                if seg.metadata.repetition_items:
                    md.append(f"- é‡å¤å¼ºè°ƒï¼š{', '.join(seg.metadata.repetition_items)}")
                if seg.metadata.foreshadowing:
                    fh = seg.metadata.foreshadowing
                    md.append(f"- ä¼ç¬”ï¼š{fh.get('type', '')} - {fh.get('content', '')}")
                md.append("")
            
            md.append("---\n")
        
        # ç« èŠ‚æ•´ä½“åˆ†æ
        md.append(f"## ğŸ“Š ç¬¬{analysis.chapter_number}ç« æ•´ä½“åˆ†æ\n")
        
        # æ ¸å¿ƒåŠŸèƒ½ç»Ÿè®¡
        md.append("### æ ¸å¿ƒåŠŸèƒ½ç»Ÿè®¡\n")
        md.append("| åŠŸèƒ½ç±»å‹ | æ•°é‡ | å…³é”®æ®µè½ |")
        md.append("|---------|------|---------|")
        
        story_segments = [s.segment_id for s in analysis.segments if "æ•…äº‹æ¨è¿›" in s.tags.narrative_function]
        md.append(f"| **æ•…äº‹æ¨è¿›** | {len(story_segments)}æ¬¡ | {', '.join(story_segments[:3])}{'...' if len(story_segments) > 3 else ''} |")
        
        first_appearance_segments = [s.segment_id for s in analysis.segments if s.metadata.contains_first_appearance]
        md.append(f"| **æ ¸å¿ƒè®¾å®šï¼ˆé¦–æ¬¡ï¼‰** | {len(first_appearance_segments)}é¡¹ | {', '.join(first_appearance_segments[:3])}{'...' if len(first_appearance_segments) > 3 else ''} |")
        
        md.append("")
        
        # ä¼˜å…ˆçº§åˆ†å¸ƒ
        md.append("### ä¼˜å…ˆçº§åˆ†å¸ƒ\n")
        md.append(f"- **P0-éª¨æ¶**ï¼ˆ{analysis.chapter_summary.p0_count}å¤„ï¼‰ï¼š{', '.join(analysis.chapter_summary.key_events)}")
        md.append(f"- **P1-è¡€è‚‰**ï¼ˆ{analysis.chapter_summary.p1_count}å¤„ï¼‰ï¼šé‡è¦ç»†èŠ‚")
        md.append(f"- **P2-çš®è‚¤**ï¼ˆ{analysis.chapter_summary.p2_count}å¤„ï¼‰ï¼šæ°›å›´æ¸²æŸ“\n")
        
        # æ—¶ç©ºè½¨è¿¹
        locations = [s.tags.location for s in analysis.segments if s.tags.location]
        times = [s.tags.time for s in analysis.segments if s.tags.time]
        if locations or times:
            md.append("### æ—¶ç©ºè½¨è¿¹\n")
            md.append("```")
            if times:
                md.append(" â†’ ".join(times[:3]) + ("..." if len(times) > 3 else ""))
            if locations:
                md.append(" â†’ ".join(locations[:3]) + ("..." if len(locations) > 3 else ""))
            md.append("```\n")
        
        # ç»“æ„ç‰¹ç‚¹
        if analysis.structure_insight.opening_style:
            md.append("### ç»“æ„ç‰¹ç‚¹\n")
            if analysis.structure_insight.opening_style:
                md.append(f"1. **å¼€ç¯‡æ–¹å¼**ï¼š{analysis.structure_insight.opening_style}")
            if analysis.structure_insight.turning_point:
                md.append(f"2. **è½¬æŠ˜ç‚¹**ï¼š{analysis.structure_insight.turning_point}")
            if analysis.structure_insight.climax:
                md.append(f"3. **é«˜æ½®**ï¼š{analysis.structure_insight.climax}")
            if analysis.structure_insight.ending_hook:
                md.append(f"4. **ç« èŠ‚é’©å­**ï¼š{analysis.structure_insight.ending_hook}")
            md.append("")
        
        # æµ“ç¼©å»ºè®®
        if analysis.chapter_summary.condensed_version:
            md.append("### æµ“ç¼©å»ºè®®ï¼ˆ500å­—ç‰ˆæœ¬ï¼‰\n")
            md.append("```")
            md.append(analysis.chapter_summary.condensed_version)
            md.append("```\n")
        
        # æ–¹æ³•è®ºéªŒè¯
        if analysis.methodology_notes:
            md.append("---\n")
            md.append("## ğŸ¯ æ–¹æ³•è®ºéªŒè¯\n")
            for note in analysis.methodology_notes:
                md.append(f"âœ… {note}")
            md.append("")
        
        # åˆ†æå®Œæˆä¿¡æ¯
        md.append("---\n")
        md.append(f"**åˆ†æå®Œæˆæ—¶é—´**: {analysis.analyzed_at.strftime('%Y-%m-%d %H:%M:%S')}")
        md.append(f"**åˆ†æç‰ˆæœ¬**: {analysis.version}")
        md.append(f"**ä¸‹ä¸€æ­¥**: åˆ†æç¬¬{analysis.chapter_number + 1}ç« ï¼ŒéªŒè¯ä¼ç¬”å›æ”¶æƒ…å†µ\n")
        
        return "\n".join(md)
