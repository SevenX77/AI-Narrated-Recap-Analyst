"""
å¯¹æ¯”è¯„ä¼°Agent

å°†Generatedå†…å®¹ä¸Ground Truthå¯¹æ¯”ï¼Œç»™å‡ºè¯¦ç»†çš„å·®è·åˆ†æå’Œæ”¹è¿›å»ºè®®
"""

import json
from typing import Dict, Any, Optional
from datetime import datetime

from src.core.schemas_feedback import (
    RuleBook, ComparativeFeedback, DimensionScore,
    RuleViolation, SimilarityMetrics
)
from src.core.interfaces import BaseAgent
from src.utils.logger import logger
from src.utils.prompt_loader import load_prompts


class ComparativeEvaluatorAgent(BaseAgent):
    """
    å¯¹æ¯”è¯„ä¼°Agent
    
    è´Ÿè´£ï¼š
    1. å°†Generatedå†…å®¹ä¸GTå¯¹æ¯”
    2. æ‰¾å‡ºå…·ä½“å·®è·å¹¶ä¸¾ä¾‹
    3. è®¡ç®—ç›¸ä¼¼åº¦æŒ‡æ ‡
    4. ç»™å‡ºè¯¦ç»†æ”¹è¿›å»ºè®®
    """
    
    def __init__(self, client: Any, model_name: str = "deepseek-chat"):
        """
        åˆå§‹åŒ–å¯¹æ¯”è¯„ä¼°Agent
        
        Args:
            client: LLMå®¢æˆ·ç«¯
            model_name: æ¨¡å‹åç§°
        """
        super().__init__(context={})
        self.client = client
        self.model_name = model_name
        self.prompts = load_prompts("comparative_evaluation")
    
    def compare_with_ground_truth(
        self,
        generated_content: Dict[str, Any],
        ground_truth_content: Dict[str, Any],
        gt_heat_score: float,
        rulebook: RuleBook,
        gt_project_id: str
    ) -> ComparativeFeedback:
        """
        å°†Generatedå†…å®¹ä¸Ground Truthå¯¹æ¯”è¯„ä¼°
        
        Args:
            generated_content: ç”Ÿæˆçš„å†…å®¹æ•°æ®
            ground_truth_content: Ground Truthå†…å®¹æ•°æ®
            gt_heat_score: GTçš„å®é™…çƒ­åº¦å€¼
            rulebook: è§„åˆ™åº“
            gt_project_id: GTé¡¹ç›®ID
            
        Returns:
            ComparativeFeedback: è¯¦ç»†çš„å¯¹æ¯”è¯„ä¼°æŠ¥å‘Š
        """
        logger.info(f"ğŸ” å¼€å§‹å¯¹æ¯”è¯„ä¼°ï¼ˆå‚è€ƒGT: {gt_project_id}, çƒ­åº¦: {gt_heat_score}ï¼‰...")
        
        # å‡†å¤‡æ•°æ®
        gt_json = json.dumps(ground_truth_content, indent=2, ensure_ascii=False)
        generated_json = json.dumps(generated_content, indent=2, ensure_ascii=False)
        rulebook_json = rulebook.model_dump_json(indent=2)
        
        system_prompt = self.prompts["compare_with_ground_truth"]["system"]
        user_prompt = self.prompts["compare_with_ground_truth"]["user"].format(
            ground_truth_content=gt_json,
            gt_heat_score=gt_heat_score,
            generated_content=generated_json,
            rulebook=rulebook_json
        )
        
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.1,
                response_format={"type": "json_object"}
            )
            
            result_json = response.choices[0].message.content
            data = json.loads(result_json)
            
            # è§£ædimension_scores
            dimension_scores = []
            for dim_data in data.get("dimension_comparisons", []):
                violations = [
                    RuleViolation(**v) 
                    for comp in dim_data.get("detailed_comparison", [])
                    if comp.get("importance") in ["critical", "major"]
                    for v in [self._convert_comparison_to_violation(comp)]
                ]
                
                dimension_scores.append(DimensionScore(
                    dimension=dim_data.get("dimension"),
                    score=dim_data.get("generated_score", 0),
                    max_score=dim_data.get("gt_score", 100),
                    weight=1.0,  # å¯ä»¥ä»rulebookä¸­è·å–
                    violations=violations,
                    highlights=[],
                    gt_baseline=dim_data.get("gt_score")
                ))
            
            # è§£æsimilarity_metrics
            similarity_data = data.get("similarity_metrics", {})
            similarity_metrics = SimilarityMetrics(
                length_ratio=similarity_data.get("length_ratio", 1.0),
                pacing_similarity=similarity_data.get("pacing_similarity", 0.0),
                keyword_overlap=similarity_data.get("keyword_overlap", 0.0),
                info_density_ratio=similarity_data.get("info_density_ratio", 1.0),
                details=similarity_data
            )
            
            # æ„å»ºComparativeFeedback
            total_score = data.get("generated_total_score", 0)
            gt_total_score = data.get("gt_total_score", 100)
            predicted_heat = self._predict_heat_from_score(total_score)
            
            feedback = ComparativeFeedback(
                content_type=generated_content.get("type", "unknown"),
                total_score=total_score,
                max_score=100.0,
                predicted_heat_score=predicted_heat,
                gt_project_id=gt_project_id,
                gt_total_score=gt_total_score,
                gt_heat_score=gt_heat_score,
                score_gap=data.get("score_gap", 0),
                dimension_scores=dimension_scores,
                critical_issues=data.get("critical_issues", []),
                major_improvements=data.get("major_improvements", []),
                strengths=data.get("strengths", []),
                similarity_metrics=similarity_metrics,
                is_passed=total_score >= 80.0,
                recommendation=data.get("recommendation", "improve"),
                evaluated_at=datetime.now().isoformat(),
                rulebook_version=rulebook.version
            )
            
            logger.info(f"âœ… å¯¹æ¯”è¯„ä¼°å®Œæˆ:")
            logger.info(f"   - Generatedå¾—åˆ†: {total_score}/{gt_total_score}")
            logger.info(f"   - åˆ†æ•°å·®è·: {feedback.score_gap}")
            logger.info(f"   - é¢„æµ‹çƒ­åº¦: {predicted_heat:.1f}")
            logger.info(f"   - å»ºè®®: {feedback.recommendation}")
            
            return feedback
            
        except Exception as e:
            logger.error(f"âŒ å¯¹æ¯”è¯„ä¼°å¤±è´¥: {e}")
            raise
    
    def calculate_similarity(
        self,
        generated_content: Dict[str, Any],
        ground_truth_content: Dict[str, Any]
    ) -> SimilarityMetrics:
        """
        è®¡ç®—Generatedä¸GTçš„ç›¸ä¼¼åº¦æŒ‡æ ‡
        
        Args:
            generated_content: ç”Ÿæˆçš„å†…å®¹
            ground_truth_content: Ground Truthå†…å®¹
            
        Returns:
            SimilarityMetrics: ç›¸ä¼¼åº¦æŒ‡æ ‡
        """
        logger.info("ğŸ“Š è®¡ç®—ç›¸ä¼¼åº¦æŒ‡æ ‡...")
        
        gt_json = json.dumps(ground_truth_content, indent=2, ensure_ascii=False)
        generated_json = json.dumps(generated_content, indent=2, ensure_ascii=False)
        
        system_prompt = self.prompts["calculate_similarity"]["system"]
        user_prompt = self.prompts["calculate_similarity"]["user"].format(
            ground_truth=gt_json,
            generated=generated_json
        )
        
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.1,
                response_format={"type": "json_object"}
            )
            
            result_json = response.choices[0].message.content
            data = json.loads(result_json)
            
            similarity_metrics = SimilarityMetrics(
                length_ratio=data.get("length_ratio", 1.0),
                pacing_similarity=data.get("pacing_similarity", 0.0),
                keyword_overlap=data.get("keyword_overlap", 0.0),
                info_density_ratio=data.get("info_density_ratio", 1.0),
                details=data.get("details", {})
            )
            
            logger.info(f"âœ… ç›¸ä¼¼åº¦è®¡ç®—å®Œæˆ:")
            logger.info(f"   - é•¿åº¦æ¯”ä¾‹: {similarity_metrics.length_ratio:.2f}")
            logger.info(f"   - èŠ‚å¥ç›¸ä¼¼åº¦: {similarity_metrics.pacing_similarity:.2f}")
            logger.info(f"   - å…³é”®è¯é‡å : {similarity_metrics.keyword_overlap:.2f}")
            
            return similarity_metrics
            
        except Exception as e:
            logger.error(f"âŒ ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            raise
    
    def _convert_comparison_to_violation(self, comparison: Dict) -> RuleViolation:
        """
        å°†å¯¹æ¯”è¯¦æƒ…è½¬æ¢ä¸ºRuleViolationå¯¹è±¡
        
        Args:
            comparison: å¯¹æ¯”è¯¦æƒ…
            
        Returns:
            RuleViolation
        """
        severity_map = {
            "critical": "critical",
            "major": "major",
            "minor": "minor"
        }
        
        return RuleViolation(
            rule_id=f"COMP_{comparison.get('aspect', 'unknown').upper()}",
            rule_text=comparison.get('aspect', ''),
            dimension="comparison",
            severity=severity_map.get(comparison.get('importance', 'minor'), 'minor'),
            deduction=8 if comparison.get('importance') == 'critical' else 5,
            comparison={
                "ground_truth_example": comparison.get('gt_example', ''),
                "generated_example": comparison.get('generated_example', ''),
                "issue": comparison.get('issue', ''),
                "suggestion": comparison.get('suggestion', '')
            }
        )
    
    def _predict_heat_from_score(self, score: float) -> float:
        """
        æ ¹æ®è¯„åˆ†é¢„æµ‹çƒ­åº¦å€¼
        
        Args:
            score: è¯„åˆ† (0-100)
            
        Returns:
            é¢„æµ‹çš„çƒ­åº¦å€¼ (0-10)
        """
        if score >= 90:
            return 9.0 + (score - 90) / 10
        elif score >= 75:
            return 7.0 + (score - 75) / 7.5
        elif score >= 60:
            return 5.0 + (score - 60) / 7.5
        elif score >= 45:
            return 3.0 + (score - 45) / 7.5
        else:
            return score / 22.5
    
    async def process(self, **kwargs) -> Any:
        """
        BaseAgentæ¥å£å®ç°
        
        Args:
            **kwargs: åŒ…å« generated_content, ground_truth_content, gt_heat_score, rulebook, gt_project_id
            
        Returns:
            ComparativeFeedbackæˆ–SimilarityMetrics
        """
        generated_content = kwargs.get('generated_content')
        ground_truth_content = kwargs.get('ground_truth_content')
        gt_heat_score = kwargs.get('gt_heat_score')
        rulebook = kwargs.get('rulebook')
        gt_project_id = kwargs.get('gt_project_id')
        
        if all([generated_content, ground_truth_content, gt_heat_score, rulebook, gt_project_id]):
            return self.compare_with_ground_truth(
                generated_content,
                ground_truth_content,
                gt_heat_score,
                rulebook,
                gt_project_id
            )
        elif generated_content and ground_truth_content:
            return self.calculate_similarity(generated_content, ground_truth_content)
        
        raise ValueError("å‚æ•°ä¸è¶³ï¼Œæ— æ³•æ‰§è¡Œå¯¹æ¯”è¯„ä¼°")
