"""
çƒ­åº¦é©±åŠ¨è®­ç»ƒç³»ç»Ÿæµ‹è¯•è„šæœ¬

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨æ–°çš„çƒ­åº¦é©±åŠ¨è®­ç»ƒå·¥ä½œæµ
"""

import asyncio
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

from src.workflows.training_workflow_v2 import HeatDrivenTrainingWorkflow
from src.utils.logger import logger


async def test_rule_extraction():
    """
    æµ‹è¯•è§„åˆ™æå–
    
    å‰ææ¡ä»¶ï¼š
    1. project_index.jsonä¸­è‡³å°‘æœ‰2ä¸ªé¡¹ç›®æ ‡è®°ä¸ºis_ground_truth=true
    2. è¿™äº›é¡¹ç›®æœ‰heat_scoreå€¼
    3. è¿™äº›é¡¹ç›®çš„data/projects/PROJ_XXX/raw/ep01.srtæ–‡ä»¶å­˜åœ¨
    """
    logger.info("=" * 60)
    logger.info("æµ‹è¯•1: è§„åˆ™æå–")
    logger.info("=" * 60)
    
    workflow = HeatDrivenTrainingWorkflow()
    
    try:
        rulebook = await workflow.run(mode="extract")
        
        logger.info("\nâœ… è§„åˆ™æå–æˆåŠŸ!")
        logger.info(f"   ç‰ˆæœ¬: {rulebook.version}")
        logger.info(f"   æºé¡¹ç›®: {rulebook.extracted_from_projects}")
        logger.info(f"   Hookè§„åˆ™æ•°: {len(rulebook.hook_rules)}")
        logger.info(f"   Ep01è§„åˆ™æ•°: {len(rulebook.ep01_rules)}")
        logger.info(f"   Ep02+è§„åˆ™æ•°: {len(rulebook.ep02_plus_rules)}")
        
        return rulebook
        
    except Exception as e:
        logger.error(f"âŒ è§„åˆ™æå–å¤±è´¥: {e}")
        raise


async def test_rule_validation(rulebook=None):
    """
    æµ‹è¯•è§„åˆ™éªŒè¯
    
    Args:
        rulebook: è§„åˆ™åº“å¯¹è±¡ï¼ˆå¦‚æœä¸æä¾›ï¼Œåˆ™åŠ è½½æœ€æ–°ç‰ˆæœ¬ï¼‰
    """
    logger.info("\n" + "=" * 60)
    logger.info("æµ‹è¯•2: è§„åˆ™éªŒè¯")
    logger.info("=" * 60)
    
    workflow = HeatDrivenTrainingWorkflow()
    
    try:
        validation_result = await workflow.run(
            mode="validate",
            rulebook=rulebook
        )
        
        logger.info("\nâœ… è§„åˆ™éªŒè¯å®Œæˆ!")
        logger.info(f"   ç›¸å…³æ€§: {validation_result.correlation:.2f}")
        logger.info(f"   æ˜¯å¦é€šè¿‡: {validation_result.is_valid}")
        
        if validation_result.is_valid:
            logger.info("   âœ… è§„åˆ™éªŒè¯é€šè¿‡ï¼Œå¯ä»¥ç”¨äºè¯„ä¼°")
        else:
            logger.warning("   âš ï¸  è§„åˆ™éªŒè¯æœªé€šè¿‡ï¼Œå»ºè®®ä¼˜åŒ–")
            logger.warning("   ä¼˜åŒ–å»ºè®®:")
            for suggestion in validation_result.optimization_suggestions[:3]:
                logger.warning(f"     - {suggestion}")
        
        # æ˜¾ç¤ºå„é¡¹ç›®è¯„åˆ†
        logger.info("\n   å„é¡¹ç›®è¯„åˆ†:")
        for project_id, scores in validation_result.project_scores.items():
            predicted = scores.get('predicted_heat', 0)
            actual = scores.get('actual_heat', 0)
            gap = scores.get('gap', 0)
            logger.info(f"     {project_id}: é¢„æµ‹={predicted:.1f}, å®é™…={actual:.1f}, å·®è·={gap:+.1f}")
        
        return validation_result
        
    except Exception as e:
        logger.error(f"âŒ è§„åˆ™éªŒè¯å¤±è´¥: {e}")
        raise


async def test_content_evaluation(project_id="PROJ_002", rulebook=None):
    """
    æµ‹è¯•å†…å®¹è¯„ä¼°
    
    Args:
        project_id: å¾…è¯„ä¼°çš„é¡¹ç›®ID
        rulebook: è§„åˆ™åº“å¯¹è±¡ï¼ˆå¦‚æœä¸æä¾›ï¼Œåˆ™åŠ è½½æœ€æ–°ç‰ˆæœ¬ï¼‰
    """
    logger.info("\n" + "=" * 60)
    logger.info(f"æµ‹è¯•3: å†…å®¹è¯„ä¼° (é¡¹ç›®: {project_id})")
    logger.info("=" * 60)
    
    workflow = HeatDrivenTrainingWorkflow()
    
    try:
        feedback = await workflow.run(
            mode="evaluate",
            project_id=project_id,
            rulebook=rulebook
        )
        
        logger.info("\nâœ… å†…å®¹è¯„ä¼°å®Œæˆ!")
        logger.info(f"   æ€»åˆ†: {feedback.total_score}/{feedback.max_score}")
        logger.info(f"   é¢„æµ‹çƒ­åº¦: {feedback.predicted_heat_score:.1f}/10")
        logger.info(f"   GTå‚è€ƒ: {feedback.gt_project_id} (çƒ­åº¦={feedback.gt_heat_score})")
        logger.info(f"   åˆ†æ•°å·®è·: {feedback.score_gap:+.1f}")
        logger.info(f"   å»ºè®®: {feedback.recommendation}")
        
        # æ˜¾ç¤ºå„ç»´åº¦å¾—åˆ†
        logger.info("\n   å„ç»´åº¦å¾—åˆ†:")
        for dim_score in feedback.dimension_scores[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            logger.info(f"     {dim_score.dimension}: {dim_score.score}/{dim_score.max_score}")
        
        # æ˜¾ç¤ºå…³é”®é—®é¢˜
        if feedback.critical_issues:
            logger.warning("\n   âš ï¸  å…³é”®é—®é¢˜:")
            for issue in feedback.critical_issues[:3]:
                logger.warning(f"     - {issue}")
        
        # æ˜¾ç¤ºæ”¹è¿›å»ºè®®
        if feedback.major_improvements:
            logger.info("\n   ğŸ’¡ æ”¹è¿›å»ºè®®:")
            for improvement in feedback.major_improvements[:3]:
                logger.info(f"     - {improvement}")
        
        # æ˜¾ç¤ºäº®ç‚¹
        if feedback.strengths:
            logger.info("\n   âœ¨ äº®ç‚¹:")
            for strength in feedback.strengths[:3]:
                logger.info(f"     - {strength}")
        
        return feedback
        
    except Exception as e:
        logger.error(f"âŒ å†…å®¹è¯„ä¼°å¤±è´¥: {e}")
        raise


async def test_full_pipeline(eval_project_id="PROJ_002"):
    """
    æµ‹è¯•å®Œæ•´æµç¨‹
    
    Args:
        eval_project_id: å¾…è¯„ä¼°çš„é¡¹ç›®ID
    """
    logger.info("\n" + "=" * 60)
    logger.info("æµ‹è¯•4: å®Œæ•´æµç¨‹ï¼ˆæå–â†’éªŒè¯â†’è¯„ä¼°ï¼‰")
    logger.info("=" * 60)
    
    workflow = HeatDrivenTrainingWorkflow()
    
    try:
        results = await workflow.run(
            mode="full",
            eval_project_id=eval_project_id
        )
        
        logger.info("\nâœ… å®Œæ•´æµç¨‹æ‰§è¡ŒæˆåŠŸ!")
        logger.info(f"   è§„åˆ™åº“ç‰ˆæœ¬: {results['rulebook'].version}")
        logger.info(f"   éªŒè¯ç›¸å…³æ€§: {results['validation_result'].correlation:.2f}")
        if results['feedback']:
            logger.info(f"   è¯„ä¼°å¾—åˆ†: {results['feedback'].total_score}/100")
            logger.info(f"   é¢„æµ‹çƒ­åº¦: {results['feedback'].predicted_heat_score:.1f}/10")
        
        return results
        
    except Exception as e:
        logger.error(f"âŒ å®Œæ•´æµç¨‹å¤±è´¥: {e}")
        raise


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹æµ‹è¯•çƒ­åº¦é©±åŠ¨è®­ç»ƒç³»ç»Ÿ\n")
    
    # é€‰æ‹©æµ‹è¯•æ¨¡å¼
    test_mode = "full"  # å¯é€‰: "extract", "validate", "evaluate", "full"
    
    if test_mode == "extract":
        # ä»…æµ‹è¯•è§„åˆ™æå–
        await test_rule_extraction()
        
    elif test_mode == "validate":
        # å…ˆæå–è§„åˆ™ï¼Œå†éªŒè¯
        rulebook = await test_rule_extraction()
        await test_rule_validation(rulebook)
        
    elif test_mode == "evaluate":
        # å…ˆæå–è§„åˆ™ï¼Œå†è¯„ä¼°å†…å®¹
        rulebook = await test_rule_extraction()
        await test_content_evaluation(project_id="PROJ_002", rulebook=rulebook)
        
    elif test_mode == "full":
        # å®Œæ•´æµç¨‹
        await test_full_pipeline(eval_project_id="PROJ_002")
    
    logger.info("\n" + "=" * 60)
    logger.info("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
    logger.info("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())
