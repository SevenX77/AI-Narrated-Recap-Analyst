"""
æµ‹è¯•Novel-to-Scriptæ™ºèƒ½æ”¹ç¼–ç³»ç»Ÿçš„æ–°å·¥å…·

æµ‹è¯•å†…å®¹ï¼š
1. NovelSegmentationAnalyzer - å°è¯´åˆ†æ®µæ·±åº¦åˆ†æ
2. KeyInfoExtractor - å…³é”®ä¿¡æ¯æå–
3. ScriptSegmentAligner - Script-Novelç²¾ç¡®å¯¹é½

ä½¿ç”¨é¡¹ç›®ï¼šæœ«å“¥è¶…å‡¡å…¬è·¯ï¼ˆPROJ_002ï¼‰
"""

import asyncio
import json
import sys
from pathlib import Path

# æ·»åŠ srcåˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.tools.novel_segmentation_analyzer import NovelSegmentationAnalyzer
from src.tools.key_info_extractor import KeyInfoExtractor
from src.tools.script_segment_aligner import ScriptSegmentAligner


def print_section(title: str):
    """æ‰“å°ç« èŠ‚æ ‡é¢˜"""
    print("\n" + "=" * 80)
    print(f"  {title}")
    print("=" * 80 + "\n")


def test_novel_segmentation_analyzer():
    """æµ‹è¯•NovelSegmentationAnalyzer"""
    print_section("æµ‹è¯• 1: NovelSegmentationAnalyzer - å°è¯´åˆ†æ®µæ·±åº¦åˆ†æ")
    
    # è¯»å–ç« èŠ‚å†…å®¹ï¼ˆç¬¬ä¸€ç« ï¼‰
    project_dir = Path(__file__).parent.parent / "data/projects/with_novel/æœ«å“¥è¶…å‡¡å…¬è·¯"
    novel_file = project_dir / "novel/chpt_0001-0010.md"
    
    if not novel_file.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {novel_file}")
        return None
    
    with open(novel_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æå–ç¬¬ä¸€ç« å†…å®¹ï¼ˆç®€å•åˆ†å‰²ï¼‰
    chapters = content.split("=== ç¬¬")[1:]  # è·³è¿‡ç®€ä»‹
    if not chapters:
        print("âŒ æ— æ³•è§£æç« èŠ‚")
        return None
    
    first_chapter = "=== ç¬¬" + chapters[0].split("=== ç¬¬")[0]
    
    # æˆªå–å‰1000å­—ç¬¦ç”¨äºæµ‹è¯•ï¼ˆé¿å…è¶…é•¿æ–‡æœ¬ï¼‰
    first_chapter = first_chapter[:1000]
    
    print(f"ğŸ“– ç« èŠ‚å†…å®¹é•¿åº¦: {len(first_chapter)} å­—ç¬¦")
    print(f"ğŸ“– ç« èŠ‚å†…å®¹é¢„è§ˆ:\n{first_chapter[:200]}...\n")
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = NovelSegmentationAnalyzer()
    
    print("ğŸ”„ æ­£åœ¨è°ƒç”¨LLMè¿›è¡Œåˆ†æ®µåˆ†æ...")
    print("â³ è¿™å¯èƒ½éœ€è¦10-20ç§’ï¼Œè¯·ç¨å€™...\n")
    
    try:
        # æ‰§è¡Œåˆ†æ
        result = analyzer.execute(
            chapter_text=first_chapter,
            chapter_id="chpt_0001",
            chapter_title="ç¬¬ä¸€ç« ",
            character_list=["é™ˆé‡"],
            world_settings={"setting": "æœ«æ—¥ä¸–ç•Œ", "system": "å‡çº§ç³»ç»Ÿ"}
        )
        
        print("âœ… åˆ†ææˆåŠŸï¼\n")
        
        # æ‰“å°ç»“æœç»Ÿè®¡
        print(f"ğŸ“Š åˆ†æç»“æœç»Ÿè®¡:")
        print(f"  - æ€»æ®µè½æ•°: {result.chapter_summary.total_segments}")
        print(f"  - P0-éª¨æ¶: {result.chapter_summary.p0_count}")
        print(f"  - P1-è¡€è‚‰: {result.chapter_summary.p1_count}")
        print(f"  - P2-çš®è‚¤: {result.chapter_summary.p2_count}")
        print(f"  - å…³é”®äº‹ä»¶: {', '.join(result.chapter_summary.key_events)}")
        print(f"  - åŸ‹è®¾ä¼ç¬”: {', '.join(result.chapter_summary.foreshadowing_planted)}")
        
        # æ‰“å°å‰3ä¸ªæ®µè½çš„è¯¦ç»†ä¿¡æ¯
        print(f"\nğŸ“ å‰3ä¸ªæ®µè½è¯¦ç»†ä¿¡æ¯:")
        for i, seg in enumerate(result.segments[:3]):
            print(f"\n  ã€æ®µè½ {i+1}ã€‘ {seg.segment_id}")
            print(f"  åŸæ–‡: {seg.text[:50]}...")
            print(f"  æ ‡ç­¾:")
            print(f"    - å™äº‹åŠŸèƒ½: {', '.join(seg.tags.narrative_function)}")
            print(f"    - å™äº‹ç»“æ„: {', '.join(seg.tags.structure)}")
            print(f"    - ä¼˜å…ˆçº§: {seg.tags.priority}")
            print(f"  æµ“ç¼©å»ºè®®: {seg.metadata.condensation_suggestion[:50]}...")
        
        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        output_dir = project_dir / "novel/segmentation_analysis"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        output_file = output_dir / f"{result.chapter_id}_analysis_test.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result.model_dump(), f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        return result
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def test_key_info_extractor(chapter_analysis):
    """æµ‹è¯•KeyInfoExtractor"""
    print_section("æµ‹è¯• 2: KeyInfoExtractor - å…³é”®ä¿¡æ¯æå–")
    
    if not chapter_analysis:
        print("âš ï¸  è·³è¿‡æµ‹è¯•ï¼ˆéœ€è¦å…ˆå®Œæˆæµ‹è¯•1ï¼‰")
        return None
    
    # åˆ›å»ºæå–å™¨
    extractor = KeyInfoExtractor()
    
    print("ğŸ”„ æ­£åœ¨æå–å…³é”®ä¿¡æ¯...")
    
    try:
        # æ‰§è¡Œæå–
        key_info = extractor.execute(
            chapter_analyses=[chapter_analysis],
            scope="test_chapter_1"
        )
        
        print("âœ… æå–æˆåŠŸï¼\n")
        
        # æ‰“å°ç»“æœç»Ÿè®¡
        print(f"ğŸ“Š å…³é”®ä¿¡æ¯ç»Ÿè®¡:")
        print(f"  - P0éª¨æ¶: {len(key_info.p0_skeleton)} é¡¹")
        print(f"  - P1è¡€è‚‰: {len(key_info.p1_flesh)} é¡¹")
        print(f"  - P2çš®è‚¤: {len(key_info.p2_skin)} é¡¹")
        print(f"  - è§’è‰²æ•°é‡: {len(key_info.character_arcs)}")
        
        # æ‰“å°P0ä¿¡æ¯
        if key_info.p0_skeleton:
            print(f"\nğŸ“Œ P0éª¨æ¶ä¿¡æ¯ï¼ˆå‰3é¡¹ï¼‰:")
            for i, info in enumerate(key_info.p0_skeleton[:3]):
                print(f"  {i+1}. {info['segment_id']}")
                print(f"     å†…å®¹: {info['content'][:40]}...")
                print(f"     é‡è¦æ€§: {info['importance']}")
        
        # æ‰“å°ä¼ç¬”ä¿¡æ¯
        planted = key_info.foreshadowing_map.get("planted", [])
        if planted:
            print(f"\nğŸ£ åŸ‹è®¾çš„ä¼ç¬”:")
            for fh in planted:
                print(f"  - {fh['content']} (ç« èŠ‚: {fh['chapter_id']})")
        
        # æ‰“å°æµ“ç¼©æŒ‡å¯¼
        print(f"\nğŸ“‹ æµ“ç¼©æŒ‡å¯¼åŸåˆ™:")
        print(f"  å¿…é¡»ä¿ç•™: {', '.join(key_info.condensation_guidelines.get('must_retain', [])[:3])}")
        print(f"  å¯ä»¥ç®€åŒ–: {', '.join(key_info.condensation_guidelines.get('can_simplify', [])[:3])}")
        print(f"  å¯ä»¥çœç•¥: {', '.join(key_info.condensation_guidelines.get('can_omit', [])[:3])}")
        
        # ä¿å­˜ç»“æœ
        project_dir = Path(__file__).parent.parent / "data/projects/with_novel/æœ«å“¥è¶…å‡¡å…¬è·¯"
        output_dir = project_dir / "analysis"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        output_file = output_dir / "key_info_test.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(key_info.model_dump(), f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        return key_info
        
    except Exception as e:
        print(f"âŒ æå–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def test_script_segment_aligner(chapter_analysis):
    """æµ‹è¯•ScriptSegmentAligner"""
    print_section("æµ‹è¯• 3: ScriptSegmentAligner - Script-Novelç²¾ç¡®å¯¹é½")
    
    if not chapter_analysis:
        print("âš ï¸  è·³è¿‡æµ‹è¯•ï¼ˆéœ€è¦å…ˆå®Œæˆæµ‹è¯•1ï¼‰")
        return None
    
    # è¯»å–Script
    project_dir = Path(__file__).parent.parent / "data/projects/with_novel/æœ«å“¥è¶…å‡¡å…¬è·¯"
    script_file = project_dir / "script/ep01.md"
    
    if not script_file.exists():
        print(f"âŒ Scriptæ–‡ä»¶ä¸å­˜åœ¨: {script_file}")
        return None
    
    with open(script_file, 'r', encoding='utf-8') as f:
        script_content = f.read()
    
    print(f"ğŸ“œ Scripté•¿åº¦: {len(script_content)} å­—ç¬¦")
    print(f"ğŸ“œ Scripté¢„è§ˆ:\n{script_content[:200]}...\n")
    
    # åˆ›å»ºå¯¹é½å™¨
    aligner = ScriptSegmentAligner()
    
    print("ğŸ”„ æ­£åœ¨æ‰§è¡ŒScript-Novelå¯¹é½...")
    print("â³ è¿™å¯èƒ½éœ€è¦1-2åˆ†é’Ÿï¼ˆæ¯æ®µScriptéœ€è¦è°ƒç”¨LLMï¼‰ï¼Œè¯·ç¨å€™...\n")
    
    try:
        # æ‰§è¡Œå¯¹é½
        result = aligner.execute(
            script_text=script_content,
            novel_analyses=[chapter_analysis],
            episode_id="ep01_test"
        )
        
        print("âœ… å¯¹é½æˆåŠŸï¼\n")
        
        # æ‰“å°æ•´ä½“ç»Ÿè®¡
        stats = result.overall_stats
        print(f"ğŸ“Š å¯¹é½ç»“æœç»Ÿè®¡:")
        print(f"  - Scriptæ®µè½æ•°: {stats.total_script_segments}")
        print(f"  - å°è¯´æ®µè½æ•°: {stats.total_novel_segments}")
        print(f"  - æµ“ç¼©æ¯”ä¾‹: {stats.condensation_ratio:.2%}")
        print(f"  - P0ä¿ç•™ç‡: {stats.p0_retention_rate:.2%}")
        print(f"  - P1ä¿ç•™ç‡: {stats.p1_retention_rate:.2%}")
        print(f"  - P2ä¿ç•™ç‡: {stats.p2_retention_rate:.2%}")
        print(f"  - å¹³å‡å¯¹é½ç½®ä¿¡åº¦: {stats.avg_alignment_confidence:.2%}")
        
        # æ‰“å°å‰3ä¸ªå¯¹é½ç»“æœ
        print(f"\nğŸ“ å‰3ä¸ªå¯¹é½ç»“æœ:")
        for i, alignment in enumerate(result.alignments[:3]):
            print(f"\n  ã€å¯¹é½ {i+1}ã€‘")
            print(f"  Scriptæ—¶é—´: {alignment.script_segment.time_range}")
            print(f"  Scriptç±»å‹: {alignment.script_segment.segment_type}")
            print(f"  Scriptå†…å®¹: {alignment.script_segment.text[:50]}...")
            print(f"  å¯¹åº”å°è¯´æ®µè½: {', '.join(alignment.novel_source.segments[:3])}")
            print(f"  æµ“ç¼©æ¯”ä¾‹: {alignment.novel_source.condensation_ratio:.2%}")
            print(f"  ä¿ç•™æ ‡ç­¾: {', '.join(alignment.novel_source.retained_tags[:3])}")
            print(f"  æ”¹ç¼–æŠ€å·§: {', '.join(alignment.novel_source.transformation.get('techniques', [])[:2])}")
            print(f"  å¯¹é½ç½®ä¿¡åº¦: {alignment.analysis.alignment_confidence:.2%}")
        
        # ä¿å­˜ç»“æœ
        output_dir = project_dir / "script/alignment_to_novel"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        output_file = output_dir / f"{result.episode_id}_mapping_test.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result.model_dump(), f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        return result
        
    except Exception as e:
        print(f"âŒ å¯¹é½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    print("\n" + "ğŸš€" * 40)
    print("  Novel-to-Script æ™ºèƒ½æ”¹ç¼–ç³»ç»Ÿ - å·¥å…·æµ‹è¯•")
    print("ğŸš€" * 40)
    
    print("\nğŸ“Œ æµ‹è¯•è¯´æ˜:")
    print("  - ä½¿ç”¨é¡¹ç›®: æœ«å“¥è¶…å‡¡å…¬è·¯ï¼ˆPROJ_002ï¼‰")
    print("  - æµ‹è¯•æ•°æ®: ç¬¬ä¸€ç« ï¼ˆéƒ¨åˆ†å†…å®¹ï¼‰")
    print("  - LLMæ¨¡å‹: DeepSeek V3")
    print("  - é¢„è®¡è€—æ—¶: 2-3åˆ†é’Ÿ")
    
    input("\næŒ‰Enteré”®å¼€å§‹æµ‹è¯•...")
    
    # æµ‹è¯•1: å°è¯´åˆ†æ®µåˆ†æ
    chapter_analysis = test_novel_segmentation_analyzer()
    
    if not chapter_analysis:
        print("\nâŒ æµ‹è¯•1å¤±è´¥ï¼Œæ— æ³•ç»§ç»­åç»­æµ‹è¯•")
        return
    
    input("\næŒ‰Enteré”®ç»§ç»­æµ‹è¯•2...")
    
    # æµ‹è¯•2: å…³é”®ä¿¡æ¯æå–
    key_info = test_key_info_extractor(chapter_analysis)
    
    input("\næŒ‰Enteré”®ç»§ç»­æµ‹è¯•3...")
    
    # æµ‹è¯•3: Script-Novelå¯¹é½
    alignment_result = test_script_segment_aligner(chapter_analysis)
    
    # æ€»ç»“
    print_section("æµ‹è¯•æ€»ç»“")
    
    results = {
        "NovelSegmentationAnalyzer": "âœ…" if chapter_analysis else "âŒ",
        "KeyInfoExtractor": "âœ…" if key_info else "âŒ",
        "ScriptSegmentAligner": "âœ…" if alignment_result else "âŒ"
    }
    
    print("æµ‹è¯•ç»“æœ:")
    for tool, status in results.items():
        print(f"  {status} {tool}")
    
    if all(r == "âœ…" for r in results.values()):
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å·¥å…·è¿è¡Œæ­£å¸¸ã€‚")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚")
    
    print("\nğŸ“ è¾“å‡ºæ–‡ä»¶ä½ç½®:")
    print("  - åˆ†æ®µåˆ†æ: data/projects/with_novel/æœ«å“¥è¶…å‡¡å…¬è·¯/novel/segmentation_analysis/")
    print("  - å…³é”®ä¿¡æ¯: data/projects/with_novel/æœ«å“¥è¶…å‡¡å…¬è·¯/analysis/")
    print("  - å¯¹é½ç»“æœ: data/projects/with_novel/æœ«å“¥è¶…å‡¡å…¬è·¯/script/alignment_to_novel/")
    
    print("\n" + "=" * 80)


if __name__ == "__main__":
    main()
