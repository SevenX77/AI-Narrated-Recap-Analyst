"""
æµ‹è¯•è„šæœ¬ï¼šåªå¤„ç†ç¬¬1ç« 
"""

import sys
import re
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from src.tools.novel_chapter_processor import MetadataExtractor
from src.tools.novel_chapter_analyzer import NovelChapterAnalyzer
from src.core.artifact_manager import ArtifactManager


def main():
    """åªå¤„ç†ç¬¬1ç« """
    print("="*80)
    print("æµ‹è¯•ï¼šå¤„ç†ç¬¬1ç« ")
    print("="*80)
    
    # é…ç½®è·¯å¾„
    project_root = Path(__file__).parent.parent
    project_dir = project_root / "data/projects/with_novel/æœ«å“¥è¶…å‡¡å…¬è·¯"
    raw_novel = project_dir / "raw/novel.txt"
    novel_dir = project_dir / "novel"
    analysis_dir = novel_dir / "functional_analysis"
    
    # è¯»å–åŸå§‹å°è¯´
    print(f"\nğŸ“– è¯»å–å°è¯´: {raw_novel}")
    with open(raw_novel, 'r', encoding='utf-8') as f:
        novel_text = f.read()
    
    print(f"   æ–‡ä»¶å¤§å°: {len(novel_text)} å­—ç¬¦")
    
    # è¯†åˆ«ç« èŠ‚
    print("\nè¯†åˆ«ç« èŠ‚...")
    chapter_pattern = r'===\s*ç¬¬\s*(\d+)\s*ç« \s*(.*)===\s*\n'
    matches = list(re.finditer(chapter_pattern, novel_text))
    print(f"âœ… è¯†åˆ«åˆ° {len(matches)} ä¸ªç« èŠ‚")
    
    # åªå¤„ç†ç¬¬1ç« 
    match = matches[0]
    chapter_number = int(match.group(1))
    chapter_title = match.group(2).strip()
    
    # æå–ç« èŠ‚å†…å®¹
    start_pos = match.end()
    end_pos = matches[1].start()
    chapter_content = novel_text[start_pos:end_pos].strip()
    
    print(f"\n--- åˆ†æ ç¬¬{chapter_number}ç« : {chapter_title} ---")
    print(f"   å­—æ•°: {len(chapter_content)}")
    
    # åŠŸèƒ½æ®µåˆ†æ
    print("\nå¼€å§‹åˆ†æ...")
    analyzer = NovelChapterAnalyzer()
    analysis = analyzer.execute(
        chapter_content=chapter_content,
        chapter_number=chapter_number,
        chapter_title=chapter_title
    )
    
    # è½¬æ¢ä¸ºå­—å…¸
    analysis_dict = analysis.model_dump(mode='json')
    
    print(f"\nâœ… åˆ†æå®Œæˆ:")
    print(f"   åŠŸèƒ½æ®µæ•°: {len(analysis_dict['segments'])}")
    print(f"   P0æ®µè½: {analysis_dict['chapter_summary']['p0_count']}")
    print(f"   P1æ®µè½: {analysis_dict['chapter_summary']['p1_count']}")
    print(f"   P2æ®µè½: {analysis_dict['chapter_summary']['p2_count']}")
    
    # æ˜¾ç¤ºå‰2ä¸ªæ®µè½
    print("\nå‰2ä¸ªåŠŸèƒ½æ®µ:")
    for i, seg in enumerate(analysis_dict['segments'][:2]):
        print(f"\n{i+1}. {seg['title']}")
        print(f"   ID: {seg['segment_id']}")
        print(f"   ä¼˜å…ˆçº§: {seg['tags']['priority']}")
        print(f"   å­—æ•°: {len(seg['content'])}")
        print(f"   å†…å®¹é¢„è§ˆ: {seg['content'][:100]}...")
    
    # ä¿å­˜JSON
    artifact_type = f"chpt_{chapter_number:04d}_functional_analysis"
    versioned_path = ArtifactManager.save_artifact(
        content=analysis_dict,
        artifact_type=artifact_type,
        project_id="æœ«å“¥è¶…å‡¡å…¬è·¯",
        base_dir=str(analysis_dir),
        extension="json"
    )
    
    print(f"\nâœ… å·²ä¿å­˜: {Path(versioned_path).name}")
    
    # ä¿å­˜ Markdown - è¾“å‡ºåˆ° novel/ ç›®å½•
    md_content = _format_analysis_to_markdown(analysis_dict)
    md_file = novel_dir / f"ç¬¬{chapter_number}ç« å®Œæ•´åˆ†æ®µåˆ†æ.md"
    with open(md_file, 'w', encoding='utf-8') as f:
        f.write(md_content)
    print(f"   Markdown: {md_file.name}")
    
    print("\n" + "="*80)
    print("âœ… æµ‹è¯•å®Œæˆï¼")
    print("="*80)


def _format_analysis_to_markdown(analysis: dict) -> str:
    """å°†åˆ†æç»“æœæ ¼å¼åŒ–ä¸ºMarkdown"""
    lines = []
    
    lines.append(f"# ç¬¬{analysis['chapter_number']}ç«  - {analysis['chapter_title']}")
    lines.append("")
    lines.append(f"**åŠŸèƒ½æ®µæ•°**: {analysis['chapter_summary']['total_segments']}")
    lines.append(f"**P0æ®µè½**: {analysis['chapter_summary']['p0_count']}")
    lines.append(f"**P1æ®µè½**: {analysis['chapter_summary']['p1_count']}")
    lines.append(f"**P2æ®µè½**: {analysis['chapter_summary']['p2_count']}")
    lines.append("")
    lines.append("---")
    lines.append("")
    
    for seg in analysis['segments']:
        lines.append(f"## {seg['title']}")
        lines.append("")
        lines.append(f"**ID**: `{seg['segment_id']}`")
        lines.append("")
        
        # æ ‡ç­¾
        tags = seg['tags']
        if tags['narrative_function']:
            lines.append(f"**å™äº‹åŠŸèƒ½**: {', '.join(tags['narrative_function'])}")
        if tags['structure']:
            lines.append(f"**å™äº‹ç»“æ„**: {', '.join(tags['structure'])}")
        if tags['character']:
            lines.append(f"**è§’è‰²å…³ç³»**: {', '.join(tags['character'])}")
        lines.append(f"**ä¼˜å…ˆçº§**: {tags['priority']}")
        if tags.get('location'):
            lines.append(f"**åœ°ç‚¹**: {tags['location']}")
        if tags.get('time'):
            lines.append(f"**æ—¶é—´**: {tags['time']}")
        lines.append("")
        
        # å†…å®¹
        lines.append("### ğŸ“„ å†…å®¹")
        lines.append("")
        lines.append(seg['content'])
        lines.append("")
        
        # æµ“ç¼©å»ºè®®
        if seg.get('condensation_suggestion'):
            lines.append("### ğŸ’¡ æµ“ç¼©å»ºè®®")
            lines.append("")
            lines.append(seg['condensation_suggestion'])
            lines.append("")
        
        lines.append("---")
        lines.append("")
    
    return "\n".join(lines)


if __name__ == "__main__":
    main()
