"""
è‡ªåŠ¨æ‹†åˆ† novel_processing_workflow.py çš„è„šæœ¬

å°† 1828 è¡Œçš„å·¥ä½œæµæ‹†åˆ†æˆ 4 ä¸ªåŠŸèƒ½æ¨¡å—ï¼ˆä½¿ç”¨ Mixin æ¨¡å¼ï¼‰
"""
import re
from pathlib import Path
import os

os.chdir('/Users/sevenx/Documents/coding/AI-Narrated Recap Analyst')

# è¯»å–åŸæ–‡ä»¶
src_file = Path('src/workflows/novel_processing_workflow.py')
with open(src_file, 'r', encoding='utf-8') as f:
    content = f.read()

# æå– imports å’Œ docstring
imports_match = re.search(r'^(""".*?"""\n\n)(.*?)(class NovelProcessingWorkflow)', content, re.DOTALL)
file_docstring = imports_match.group(1)
imports_block = imports_match.group(2)

# æå–ç±»å®šä¹‰
class_match = re.search(r'(class NovelProcessingWorkflow.*?)(?=\n\n# |$)', content, re.DOTALL)
class_content = class_match.group(1)

# æå–æ‰€æœ‰æ–¹æ³•
method_pattern = r'(    (?:async )?def \w+.*?)(?=\n    (?:async )?def |\n\nclass |\Z)'
methods = {}
for match in re.finditer(method_pattern, class_content, re.DOTALL):
    method_text = match.group(1)
    method_name_match = re.search(r'def (\w+)\(', method_text)
    if method_name_match:
        method_name = method_name_match.group(1)
        methods[method_name] = method_text

print(f"ğŸ“Š æå–äº† {len(methods)} ä¸ªæ–¹æ³•")

# æ–¹æ³•åˆ†ç»„
groups = {
    'base_workflow.py': {
        'description': 'åŸºç¡€å·¥ä½œæµç±»å’Œè¾…åŠ©æ–¹æ³•',
        'methods': [
            '__init__',
            '_estimate_tokens',
            '_setup_processing_directory',
            '_save_intermediate_result',
            '_save_final_result',
            '_calculate_stats'
        ]
    },
    'core_steps.py': {
        'description': 'æ ¸å¿ƒå¤„ç†æ­¥éª¤ï¼ˆSteps 1-8 + runï¼‰',
        'methods': [
            'run',
            '_step1_import_novel',
            '_step2_extract_metadata',
            '_step3_detect_chapters',
            '_step4_segment_chapters',
            '_step5_annotate_chapters',
            '_step6_analyze_system',
            '_step7_track_system',
            '_step8_validate_quality'
        ]
    },
    'processing_helpers.py': {
        'description': 'æ‰¹å¤„ç†å’Œå•é¡¹å¤„ç†è¾…åŠ©æ–¹æ³•',
        'methods': [
            '_process_segmentation_batch',
            '_segment_single_chapter',
            '_process_annotation_batch',
            '_annotate_single_chapter',
            '_process_system_tracking_batch',
            '_track_single_chapter_system'
        ]
    },
    'report_generators.py': {
        'description': 'æŠ¥å‘Šå’Œå¯è§†åŒ–ç”Ÿæˆæ–¹æ³•',
        'methods': [
            '_output_step1_report',
            '_output_step2_report',
            '_output_step3_report',
            '_output_step4_report',
            '_output_step5_report',
            '_output_step67_report',
            '_output_step8_report',
            '_generate_metadata_markdown',
            '_generate_chapters_index_markdown',
            '_generate_chapter_markdown',
            '_generate_comprehensive_html',
            '_render_segmentation_html',
            '_render_annotation_html',
            '_render_system_html',
            '_render_quality_html'
        ]
    }
}

# åˆ›å»ºè¾“å‡ºç›®å½•
output_dir = Path('src/workflows/novel_processing')
output_dir.mkdir(exist_ok=True)

# æå–ç±»docstringå’Œå±æ€§
class_header = '''class NovelProcessingWorkflow(BaseWorkflow):
    """
    å°è¯´å¤„ç†å·¥ä½œæµ
    
    å®Œæ•´çš„å°è¯´å¤„ç†pipelineï¼Œæ”¯æŒå¹¶è¡Œå¤„ç†ã€é”™è¯¯æ¢å¤å’Œæ–­ç‚¹ç»­ä¼ ã€‚
    
    Attributes:
        name (str): å·¥ä½œæµåç§°
        config (NovelProcessingConfig): å·¥ä½œæµé…ç½®
        project_name (str): é¡¹ç›®åç§°
        processing_dir (str): ä¸­é—´ç»“æœä¿å­˜ç›®å½•
    
    Example:
        ```python
        workflow = NovelProcessingWorkflow()
        result = await workflow.run(
            novel_path="path/to/novel.txt",
            project_name="æœ«å“¥è¶…å‡¡å…¬è·¯_test",
            config=NovelProcessingConfig(
                chapter_range=(1, 10),
                enable_parallel=True
            )
        )
        ```
    """
    
    name: str = "novel_processing_workflow"
'''

# ç”Ÿæˆ Mixin ç±»
for filename, group_info in groups.items():
    if filename == 'base_workflow.py':
        # åŸºç¡€å·¥ä½œæµåŒ…å«ä¸»ç±»å®šä¹‰
        file_content = f'''{file_docstring}{imports_block}

# Configure logging
logger = logging.getLogger(__name__)


{class_header}
'''
        # æ·»åŠ æ–¹æ³•
        for method_name in group_info['methods']:
            if method_name in methods:
                file_content += '\n' + methods[method_name] + '\n'
        
    else:
        # Mixin ç±»ï¼ˆæ–¹æ³•é›†åˆï¼‰
        mixin_name = filename.replace('.py', '').replace('_', ' ').title().replace(' ', '')
        file_content = f'''"""
Novel Processing Workflow - {group_info['description']}

è¿™æ˜¯ NovelProcessingWorkflow çš„ Mixin ç±»ï¼ŒåŒ…å«{group_info['description']}ã€‚
"""


class {mixin_name}:
    """
    {group_info['description']} Mixin
    
    åŒ…å«ä»¥ä¸‹æ–¹æ³•ï¼š
'''
        for method_name in group_info['methods']:
            file_content += f'    - {method_name}\n'
        
        file_content += '    """\n'
        
        # æ·»åŠ æ–¹æ³•
        for method_name in group_info['methods']:
            if method_name in methods:
                file_content += '\n' + methods[method_name] + '\n'
    
    # å†™å…¥æ–‡ä»¶
    file_path = output_dir / filename
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(file_content)
    
    lines = file_content.count('\n')
    print(f"âœ… åˆ›å»º: {filename} ({len(group_info['methods'])} ä¸ªæ–¹æ³•, ~{lines} è¡Œ)")

# ç”Ÿæˆ __init__.pyï¼ˆæ•´åˆæ‰€æœ‰ Mixinï¼‰
init_content = f'''{file_docstring}{imports_block}

# å¯¼å…¥ Mixin ç±»
from .processing_helpers import ProcessingHelpers
from .report_generators import ReportGenerators
from .core_steps import CoreSteps

# Configure logging
logger = logging.getLogger(__name__)


{class_header}
'''

# æ·»åŠ  __init__ æ–¹æ³•
if '__init__' in methods:
    init_content += '\n' + methods['__init__'] + '\n'

# æ·»åŠ è¾…åŠ©æ–¹æ³•
for method_name in ['_estimate_tokens', '_setup_processing_directory', '_save_intermediate_result', '_save_final_result', '_calculate_stats']:
    if method_name in methods:
        init_content += '\n' + methods[method_name] + '\n'

# æ·»åŠ  Mixin ç»§æ‰¿è¯´æ˜æ³¨é‡Š
init_content += '''

# ç»§æ‰¿æ‰€æœ‰ Mixin æ–¹æ³•
# é€šè¿‡å¤šé‡ç»§æ‰¿å®ç°åŠŸèƒ½æ¨¡å—åŒ–ï¼š
# - CoreSteps: æ ¸å¿ƒå¤„ç†æ­¥éª¤
# - ProcessingHelpers: æ‰¹å¤„ç†è¾…åŠ©æ–¹æ³•
# - ReportGenerators: æŠ¥å‘Šç”Ÿæˆæ–¹æ³•

# åŠ¨æ€æ··å…¥æ–¹æ³•
for mixin in [CoreSteps, ProcessingHelpers, ReportGenerators]:
    for attr_name in dir(mixin):
        if not attr_name.startswith('_') or attr_name.startswith('_step') or attr_name.startswith('_process') or attr_name.startswith('_output') or attr_name.startswith('_generate') or attr_name.startswith('_render') or attr_name.startswith('_segment') or attr_name.startswith('_annotate') or attr_name.startswith('_track'):
            attr = getattr(mixin, attr_name)
            if callable(attr):
                setattr(NovelProcessingWorkflow, attr_name, attr)
'''

init_path = output_dir / '__init__.py'
with open(init_path, 'w', encoding='utf-8') as f:
    f.write(init_content)

print(f"âœ… åˆ›å»º: __init__.py (ä¸»å·¥ä½œæµç±»)")

print(f"\nğŸ‰ æ‹†åˆ†å®Œæˆï¼")
print(f"   åŸæ–‡ä»¶: {src_file} (1828 è¡Œ)")
print(f"   æ–°ç›®å½•: {output_dir}/ (5 ä¸ªæ–‡ä»¶)")
print(f"\n   ğŸ“¦ ä½¿ç”¨ Mixin æ¨¡å¼ä¿æŒå‘åå…¼å®¹")
