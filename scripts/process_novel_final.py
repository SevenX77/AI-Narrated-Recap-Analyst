"""
Novel å¤„ç†è„šæœ¬ - æœ€ç»ˆç®€åŒ–ç‰ˆæœ¬

éµå¾ªç”¨æˆ·è¦æ±‚ï¼š
1. ä¸ç”Ÿæˆå†—ä½™çš„å•ç« æ–‡ä»¶ï¼ˆchpt_0001.mdç­‰ï¼‰
2. ç›´æ¥ä» raw/novel.txt è¯»å–å¹¶åˆ†æ
3. åŠŸèƒ½æ®µåˆ†æå·²åŒ…å«åˆ†æ®µï¼Œæ— éœ€é¢å¤–æ­¥éª¤

æµç¨‹ï¼š
Step 1: æ•°æ®æ‘„å…¥ï¼ˆæ‰‹åŠ¨ï¼‰â†’ raw/novel.txt
Step 2: ç®€ä»‹æå– + LLMè¿‡æ»¤ â†’ chpt_0000_ç®€ä»‹.md
Step 3: åŠŸèƒ½æ®µåˆ†æï¼ˆR1æ¨¡å‹ï¼‰â†’ functional_analysis/
"""

import sys
import re
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from src.tools.novel_chapter_processor import MetadataExtractor
from src.tools.novel_chapter_analyzer import NovelChapterAnalyzer
from src.core.artifact_manager import ArtifactManager


def main():
    """ä¸»æµç¨‹"""
    print("="*80)
    print("Novel å¤„ç†è„šæœ¬ - æœ€ç»ˆç®€åŒ–ç‰ˆ")
    print("="*80)
    
    # é…ç½®è·¯å¾„
    project_root = Path(__file__).parent.parent
    project_dir = project_root / "data/projects/with_novel/æœ«å“¥è¶…å‡¡å…¬è·¯"
    raw_novel = project_dir / "raw/novel.txt"
    novel_dir = project_dir / "novel"
    analysis_dir = novel_dir / "functional_analysis"
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    novel_dir.mkdir(parents=True, exist_ok=True)
    analysis_dir.mkdir(parents=True, exist_ok=True)
    (analysis_dir / "history").mkdir(exist_ok=True)
    
    # è¯»å–åŸå§‹å°è¯´
    print(f"\nğŸ“– è¯»å–å°è¯´: {raw_novel}")
    with open(raw_novel, 'r', encoding='utf-8') as f:
        novel_text = f.read()
    
    print(f"   æ–‡ä»¶å¤§å°: {len(novel_text)} å­—ç¬¦")
    
    # ====================================
    # Step 1: ç®€ä»‹æå– + LLMè¿‡æ»¤
    # ====================================
    print("\n" + "="*80)
    print("Step 1: ç®€ä»‹æå–ï¼ˆä½¿ç”¨ MetadataExtractor + LLMï¼‰")
    print("="*80)
    
    extractor = MetadataExtractor(use_llm=True)
    metadata = extractor.execute(novel_text)
    
    # ä¿å­˜ç®€ä»‹
    intro_file = novel_dir / "chpt_0000_ç®€ä»‹.md"
    with open(intro_file, 'w', encoding='utf-8') as f:
        f.write(f"# {metadata['novel']['title']}\n\n")
        f.write("## ç®€ä»‹\n\n")
        f.write(metadata['novel']['introduction'])
    
    print(f"âœ… ç®€ä»‹å·²ä¿å­˜: {intro_file.name}")
    print(f"   ä½œè€…: {metadata['novel']['author']}")
    print(f"   æ ‡ç­¾: {', '.join(metadata['novel']['tags'])}")
    print(f"   ç®€ä»‹é•¿åº¦: {len(metadata['novel']['introduction'])} å­—ç¬¦")
    
    # ====================================
    # Step 2: è¯†åˆ«ç« èŠ‚ï¼ˆä¸ç”Ÿæˆå•ç« æ–‡ä»¶ï¼‰
    # ====================================
    print("\n" + "="*80)
    print("Step 2: è¯†åˆ«ç« èŠ‚è¾¹ç•Œ")
    print("="*80)
    
    chapter_pattern = r'===\s*ç¬¬\s*(\d+)\s*ç« \s*(.*)===\s*\n'
    matches = list(re.finditer(chapter_pattern, novel_text))
    
    print(f"âœ… è¯†åˆ«åˆ° {len(matches)} ä¸ªç« èŠ‚")
    
    # ====================================
    # Step 3: é€ç« åŠŸèƒ½æ®µåˆ†æï¼ˆR1æ¨¡å‹ï¼‰
    # ====================================
    print("\n" + "="*80)
    print("Step 3: åŠŸèƒ½æ®µåˆ†æï¼ˆä½¿ç”¨ NovelChapterAnalyzer + DeepSeek R1ï¼‰")
    print("="*80)
    
    analyzer = NovelChapterAnalyzer()
    
    for i, match in enumerate(matches[:10]):  # åªå¤„ç†å‰10ç« 
        chapter_number = int(match.group(1))
        chapter_title = match.group(2).strip()
        
        # æå–ç« èŠ‚å†…å®¹
        start_pos = match.end()
        end_pos = matches[i + 1].start() if i + 1 < len(matches) else len(novel_text)
        chapter_content = novel_text[start_pos:end_pos].strip()
        
        print(f"\n--- åˆ†æ ç¬¬{chapter_number}ç« : {chapter_title} ---")
        print(f"   å­—æ•°: {len(chapter_content)}")
        
        try:
            # åŠŸèƒ½æ®µåˆ†æ
            analysis = analyzer.execute(
                chapter_content=chapter_content,
                chapter_number=chapter_number,
                chapter_title=chapter_title
            )
            
            # è½¬æ¢ä¸ºå­—å…¸ï¼ˆå¤„ç†datetimeï¼‰
            analysis_dict = analysis.model_dump(mode='json')
            
            # ä¿å­˜JSONï¼ˆä½¿ç”¨ArtifactManagerï¼‰
            artifact_type = f"chpt_{chapter_number:04d}_functional_analysis"
            versioned_path = ArtifactManager.save_artifact(
                content=analysis_dict,
                artifact_type=artifact_type,
                project_id="æœ«å“¥è¶…å‡¡å…¬è·¯",
                base_dir=str(analysis_dir),
                extension="json"
            )
            
            print(f"âœ… ç¬¬{chapter_number}ç« åˆ†æå®Œæˆ:")
            print(f"   åŠŸèƒ½æ®µæ•°: {len(analysis_dict['segments'])}")
            print(f"   P0æ®µè½: {analysis_dict['chapter_summary']['p0_count']}")
            print(f"   P1æ®µè½: {analysis_dict['chapter_summary']['p1_count']}")
            print(f"   P2æ®µè½: {analysis_dict['chapter_summary']['p2_count']}")
            print(f"   å·²ä¿å­˜: {Path(versioned_path).name}")
            
            # åŒæ—¶ä¿å­˜ Markdown ç‰ˆæœ¬ï¼ˆäººç±»é˜…è¯»ï¼‰- è¾“å‡ºåˆ° novel/ ç›®å½•
            md_content = _format_analysis_to_markdown(analysis_dict)
            md_file = novel_dir / f"ç¬¬{chapter_number}ç« å®Œæ•´åˆ†æ®µåˆ†æ.md"
            with open(md_file, 'w', encoding='utf-8') as f:
                f.write(md_content)
            print(f"   Markdown: {md_file.name}")
            
        except Exception as e:
            print(f"âŒ ç¬¬{chapter_number}ç« åˆ†æå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    print("\n" + "="*80)
    print("âœ… æ‰€æœ‰å¤„ç†å®Œæˆï¼")
    print("="*80)
    print(f"\nğŸ“‚ è¾“å‡ºç›®å½•:")
    print(f"   ç®€ä»‹: {intro_file}")
    print(f"   JSONåˆ†æ: {analysis_dir}/chpt_XXXX_functional_analysis_latest.json")
    print(f"   Markdown: {novel_dir}/ç¬¬Xç« å®Œæ•´åˆ†æ®µåˆ†æ.md")
    print(f"   å†å²ç‰ˆæœ¬: {analysis_dir}/history/")
    print(f"\nâš ï¸  æ³¨æ„: ä¸å†ç”Ÿæˆå•ç« æ–‡ä»¶ï¼ˆchpt_0001.mdç­‰ï¼‰ï¼Œç›´æ¥è¾“å‡ºåŠŸèƒ½æ®µåˆ†æ")


def _format_analysis_to_markdown(analysis: dict) -> str:
    """å°†åˆ†æç»“æœæ ¼å¼åŒ–ä¸ºMarkdown"""
    lines = []
    
    lines.append(f"# ç¬¬{analysis['chapter_number']}ç«  - {analysis['chapter_title']}")
    lines.append("")
    lines.append(f"**åŠŸèƒ½æ®µæ•°**: {analysis['chapter_summary']['total_segments']}")
    lines.append(f"**P0æ®µè½**: {analysis['chapter_summary']['p0_count']}")
    lines.append(f"**P1æ®µè½**: {analysis['chapter_summary']['p1_count']}")
    lines.append(f"**P2æ®µè½**: {analysis['chapter_summary']['p2_count']}")
    lines.append("")
    lines.append("---")
    lines.append("")
    
    for seg in analysis['segments']:
        lines.append(f"## {seg['title']}")
        lines.append("")
        lines.append(f"**ID**: `{seg['segment_id']}`")
        lines.append("")
        
        # æ ‡ç­¾
        tags = seg['tags']
        if tags['narrative_function']:
            lines.append(f"**å™äº‹åŠŸèƒ½**: {', '.join(tags['narrative_function'])}")
        if tags['structure']:
            lines.append(f"**å™äº‹ç»“æ„**: {', '.join(tags['structure'])}")
        if tags['character']:
            lines.append(f"**è§’è‰²å…³ç³»**: {', '.join(tags['character'])}")
        lines.append(f"**ä¼˜å…ˆçº§**: {tags['priority']}")
        if tags.get('location'):
            lines.append(f"**åœ°ç‚¹**: {tags['location']}")
        if tags.get('time'):
            lines.append(f"**æ—¶é—´**: {tags['time']}")
        lines.append("")
        
        # å†…å®¹
        lines.append("### ğŸ“„ å†…å®¹")
        lines.append("")
        lines.append(seg['content'])
        lines.append("")
        
        # æµ“ç¼©å»ºè®®
        if seg.get('condensation_suggestion'):
            lines.append("### ğŸ’¡ æµ“ç¼©å»ºè®®")
            lines.append("")
            lines.append(seg['condensation_suggestion'])
            lines.append("")
        
        lines.append("---")
        lines.append("")
    
    return "\n".join(lines)


if __name__ == "__main__":
    main()
