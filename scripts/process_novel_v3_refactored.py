"""
Novel å¤„ç†è„šæœ¬ V3 - é‡æ„ç‰ˆæœ¬ï¼ˆä½¿ç”¨ç°æœ‰å·¥å…·ï¼‰

éµå¾ª .cursorrules å¼ºåˆ¶æ£€æŸ¥ï¼š
âœ… Step 1: æ‰¾åˆ° MetadataExtractor, NovelChapterProcessor, ArtifactManager
âœ… Step 2: æ‰¾åˆ°å·¥å…·æ–‡ä»¶è·¯å¾„
âœ… Step 3: æ­£ç¡®è°ƒç”¨ç°æœ‰å·¥å…·ï¼Œä¸é‡å¤å®ç°

æ–°æµç¨‹:
1. ä½¿ç”¨ MetadataExtractor æå–å¹¶è¿‡æ»¤ç®€ä»‹ï¼ˆLLMæ¸…ç†ï¼‰
2. ä½¿ç”¨ NovelChapterProcessor æ‹†åˆ†ç« èŠ‚
3. ä½¿ç”¨ NovelChapterAnalyzer è¿›è¡ŒåŠŸèƒ½åˆ†æï¼ˆR1æ¨¡å‹ï¼‰
4. ä½¿ç”¨ ArtifactManager ç®¡ç†ç‰ˆæœ¬
"""

import sys
import json
from pathlib import Path
from typing import Dict, List

# æ·»åŠ srcåˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.tools.novel_chapter_processor import MetadataExtractor, NovelChapterProcessor
from src.tools.novel_chapter_analyzer import NovelChapterAnalyzer
from src.core.artifact_manager import ArtifactManager
from src.core.config import LLMConfig


def main():
    """ä¸»æµç¨‹"""
    print("="*80)
    print("Novel å¤„ç†è„šæœ¬ V3 - é‡æ„ç‰ˆæœ¬")
    print("="*80)
    
    # é…ç½®è·¯å¾„
    project_root = Path(__file__).parent.parent
    project_dir = project_root / "data/projects/with_novel/æœ«å“¥è¶…å‡¡å…¬è·¯"
    raw_novel = project_dir / "raw/novel.txt"
    novel_dir = project_dir / "novel"
    analysis_dir = novel_dir / "functional_analysis"
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    novel_dir.mkdir(exist_ok=True)
    analysis_dir.mkdir(exist_ok=True)
    
    # è¯»å–åŸå§‹å°è¯´
    print(f"\nğŸ“– è¯»å–å°è¯´: {raw_novel}")
    with open(raw_novel, 'r', encoding='utf-8') as f:
        novel_text = f.read()
    
    # ====================================
    # Step 1: ä½¿ç”¨ MetadataExtractor æå–ç®€ä»‹ï¼ˆLLMè¿‡æ»¤ï¼‰
    # ====================================
    print("\n" + "="*80)
    print("Step 1: æå–å¹¶è¿‡æ»¤ç®€ä»‹ï¼ˆä½¿ç”¨ MetadataExtractor + LLMï¼‰")
    print("="*80)
    
    extractor = MetadataExtractor(use_llm=True)
    metadata = extractor.execute(novel_text)
    
    # ä¿å­˜ç®€ä»‹
    intro_file = novel_dir / "chpt_0000_ç®€ä»‹.md"
    with open(intro_file, 'w', encoding='utf-8') as f:
        f.write(f"# {metadata['novel']['title']}\n\n")
        f.write("## ç®€ä»‹\n\n")
        f.write(metadata['novel']['introduction'])
    
    print(f"âœ… ç®€ä»‹å·²ä¿å­˜: {intro_file.name}")
    print(f"   ä½œè€…: {metadata['novel']['author']}")
    print(f"   æ ‡ç­¾: {', '.join(metadata['novel']['tags'])}")
    print(f"   ç®€ä»‹é•¿åº¦: {len(metadata['novel']['introduction'])} å­—ç¬¦")
    
    # ====================================
    # Step 2: ä½¿ç”¨ NovelChapterProcessor æ‹†åˆ†ç« èŠ‚
    # ====================================
    print("\n" + "="*80)
    print("Step 2: æ‹†åˆ†ç« èŠ‚ï¼ˆä½¿ç”¨ NovelChapterProcessorï¼‰")
    print("="*80)
    
    processor = NovelChapterProcessor(chapters_per_file=1)  # æ¯ç« ä¸€ä¸ªæ–‡ä»¶
    result = processor.execute(
        novel_text=novel_text,
        output_dir=novel_dir,
        introduction_override=metadata['novel']['introduction']  # ä½¿ç”¨å·²è¿‡æ»¤çš„ç®€ä»‹
    )
    
    print(f"âœ… ç« èŠ‚æ‹†åˆ†å®Œæˆ:")
    print(f"   æ€»ç« èŠ‚æ•°: {result['total_chapters']}")
    print(f"   ç”Ÿæˆæ–‡ä»¶: {len(result['chapter_files'])} ä¸ª")
    
    # ====================================
    # Step 3: ä½¿ç”¨ NovelChapterAnalyzer è¿›è¡ŒåŠŸèƒ½åˆ†æï¼ˆR1æ¨¡å‹ï¼‰
    # ====================================
    print("\n" + "="*80)
    print("Step 3: åŠŸèƒ½åˆ†æï¼ˆä½¿ç”¨ NovelChapterAnalyzer + DeepSeek R1ï¼‰")
    print("="*80)
    
    # å¼ºåˆ¶ä½¿ç”¨ R1 æ¨¡å‹ï¼ˆé˜…è¯»ç†è§£ä»»åŠ¡ï¼‰
    llm_config = LLMConfig()
    analyzer = NovelChapterAnalyzer(
        model=llm_config.fallback_model,  # ä½¿ç”¨ R1 æ¨¡å‹
        enable_fallback=False  # R1å·²ç»æ˜¯æœ€å¼ºæ¨¡å‹ï¼Œä¸éœ€è¦fallback
    )
    
    # åˆ†ææ‰€æœ‰ç« èŠ‚
    for i in range(1, result['total_chapters'] + 1):
        chapter_file = novel_dir / f"chpt_{i:04d}.md"
        
        if not chapter_file.exists():
            print(f"âš ï¸  è·³è¿‡ ç¬¬{i}ç«  (æ–‡ä»¶ä¸å­˜åœ¨)")
            continue
        
        print(f"\n--- åˆ†æ ç¬¬{i}ç«  ---")
        
        with open(chapter_file, 'r', encoding='utf-8') as f:
            chapter_content = f.read()
        
        try:
            # æ‰§è¡ŒåŠŸèƒ½åˆ†æ
            analysis = analyzer.execute(chapter_content, chapter_id=i)
            
            # ====================================
            # Step 4: ä½¿ç”¨ ArtifactManager ä¿å­˜ç‰ˆæœ¬åŒ–ç»“æœ
            # ====================================
            artifact_type = f"chpt_{i:04d}_functional_analysis"
            versioned_path = ArtifactManager.save_artifact(
                content=analysis,
                artifact_type=artifact_type,
                project_id="æœ«å“¥è¶…å‡¡å…¬è·¯",
                base_dir=str(analysis_dir),
                extension="json"
            )
            
            print(f"âœ… ç¬¬{i}ç« åˆ†æå®Œæˆ:")
            print(f"   åŠŸèƒ½æ®µæ•°: {analysis['segment_count']}")
            print(f"   å·²ä¿å­˜: {Path(versioned_path).name}")
            print(f"   LatestæŒ‡é’ˆ: {artifact_type}_latest.json")
            
            # åŒæ—¶ä¿å­˜ Markdown ç‰ˆæœ¬ï¼ˆç”¨äºäººç±»é˜…è¯»ï¼‰
            md_content = _format_analysis_to_markdown(analysis)
            md_file = analysis_dir / f"ç¬¬{i}ç« å®Œæ•´åˆ†æ®µåˆ†æ.md"
            with open(md_file, 'w', encoding='utf-8') as f:
                f.write(md_content)
            print(f"   Markdown: {md_file.name}")
            
        except Exception as e:
            print(f"âŒ ç¬¬{i}ç« åˆ†æå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    print("\n" + "="*80)
    print("âœ… æ‰€æœ‰å¤„ç†å®Œæˆï¼")
    print("="*80)
    print(f"\nğŸ“‚ è¾“å‡ºç›®å½•:")
    print(f"   ç®€ä»‹: {intro_file}")
    print(f"   ç« èŠ‚: {novel_dir}/chpt_XXXX.md")
    print(f"   åˆ†æ: {analysis_dir}/chpt_XXXX_functional_analysis_latest.json")
    print(f"   å†å²ç‰ˆæœ¬: {analysis_dir}/history/")


def _format_analysis_to_markdown(analysis: Dict) -> str:
    """å°†åˆ†æç»“æœæ ¼å¼åŒ–ä¸ºMarkdownï¼ˆç”¨äºäººç±»é˜…è¯»ï¼‰"""
    lines = []
    
    lines.append(f"# ç¬¬{analysis['chapter_id']}ç«  - {analysis['chapter_title']}")
    lines.append("")
    lines.append(f"**åŠŸèƒ½æ®µæ•°**: {analysis['segment_count']}")
    lines.append(f"**æ€»å­—æ•°**: {analysis['total_chars']}")
    lines.append("")
    lines.append("---")
    lines.append("")
    
    for seg in analysis['segments']:
        seg_id = seg['segment_id']
        lines.append(f"## {seg_id}")
        lines.append("")
        lines.append(f"**åŠŸèƒ½**: {seg['metadata']['narrative_function']}")
        lines.append(f"**ä¼˜å…ˆçº§**: {seg['metadata']['condensation_priority']}")
        lines.append("")
        
        # æ ‡ç­¾
        if seg['metadata']['tags']:
            lines.append(f"**æ ‡ç­¾**: {' '.join(seg['metadata']['tags'])}")
            lines.append("")
        
        # å†…å®¹
        lines.append("### å†…å®¹")
        lines.append("")
        lines.append(seg['content'])
        lines.append("")
        
        # åˆ†æ
        lines.append("### åˆ†æ")
        lines.append("")
        lines.append(seg['metadata']['analysis'])
        lines.append("")
        
        # æµ“ç¼©å»ºè®®
        if seg['metadata']['condensation_suggestion']:
            lines.append("### æµ“ç¼©å»ºè®®")
            lines.append("")
            lines.append(seg['metadata']['condensation_suggestion'])
            lines.append("")
        
        lines.append("---")
        lines.append("")
    
    # ç« èŠ‚çº§æ´å¯Ÿ
    if analysis.get('chapter_insights'):
        lines.append("## ğŸ“Š ç« èŠ‚æ´å¯Ÿ")
        lines.append("")
        for key, value in analysis['chapter_insights'].items():
            lines.append(f"**{key}**: {value}")
        lines.append("")
    
    return "\n".join(lines)


if __name__ == "__main__":
    main()
