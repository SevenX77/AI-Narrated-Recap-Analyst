"""
LLMè°ƒç”¨ç®¡ç†å™¨é›†æˆæµ‹è¯•

æ¼”ç¤ºå¦‚ä½•åœ¨workflowä¸­ä½¿ç”¨LLMCallManager
"""

import asyncio
import sys
from pathlib import Path

project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.core.llm_rate_limiter import get_llm_manager, LLMRateLimitConfig


async def demo_basic_usage():
    """æ¼”ç¤ºåŸºæœ¬ä½¿ç”¨æ–¹æ³•"""
    print("=" * 80)
    print("ğŸ§ª æ¼”ç¤º1: åŸºæœ¬ä½¿ç”¨")
    print("=" * 80)
    
    # è·å–å…¨å±€ç®¡ç†å™¨
    manager = get_llm_manager()
    
    # æŸ¥çœ‹å½“å‰é…ç½®
    print("\nğŸ“‹ å½“å‰é…ç½®:")
    for key, config in manager.configs.items():
        if key != "conservative":
            print(f"  {key}: QPM={config.requests_per_minute}, å¹¶å‘={config.max_concurrent}")
    
    # æ¨¡æ‹ŸLLMè°ƒç”¨å‡½æ•°
    def mock_llm_call(prompt: str):
        """æ¨¡æ‹ŸLLMè°ƒç”¨"""
        import time
        import random
        
        time.sleep(0.1)  # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
        
        # 5%æ¦‚ç‡è§¦å‘é™æµ
        if random.random() < 0.05:
            raise Exception("Error code: 403 - access forbidden")
        
        return {"content": f"Response to: {prompt}", "tokens": 100}
    
    # ä½¿ç”¨ç®¡ç†å™¨è°ƒç”¨
    print("\nğŸ”„ å‘èµ·10æ¬¡LLMè°ƒç”¨ï¼ˆè‡ªåŠ¨é™æµ+é‡è¯•ï¼‰...")
    
    success_count = 0
    for i in range(10):
        try:
            result = await manager.call_with_rate_limit(
                func=mock_llm_call,
                provider="deepseek",
                model="deepseek-chat",
                estimated_tokens=100,
                prompt=f"Test prompt {i+1}"
            )
            success_count += 1
            print(f"  âœ… è¯·æ±‚{i+1}: {result['content'][:30]}...")
        except Exception as e:
            print(f"  âŒ è¯·æ±‚{i+1}å¤±è´¥: {e}")
    
    print(f"\nğŸ“Š æˆåŠŸç‡: {success_count}/10")
    
    # æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“ˆ ä½¿ç”¨ç»Ÿè®¡:")
    stats = manager.get_all_stats()
    for model, stat in stats.items():
        if stat['requests_last_minute'] > 0:
            print(f"  {model}:")
            print(f"    æœ€è¿‘1åˆ†é’Ÿè¯·æ±‚: {stat['requests_last_minute']}")
            print(f"    æœ€è¿‘1åˆ†é’Ÿtokens: {stat['tokens_last_minute']}")
            print(f"    å½“å‰å¹¶å‘: {stat['current_concurrent']}")


async def demo_concurrent_calls():
    """æ¼”ç¤ºå¹¶å‘è°ƒç”¨ï¼ˆè‡ªåŠ¨é™æµï¼‰"""
    print("\n" + "=" * 80)
    print("ğŸ§ª æ¼”ç¤º2: å¹¶å‘è°ƒç”¨ï¼ˆè‡ªåŠ¨é™æµæ§åˆ¶ï¼‰")
    print("=" * 80)
    
    manager = get_llm_manager()
    
    def mock_llm_call(task_id: int):
        import time
        import random
        
        time.sleep(0.2)
        
        if random.random() < 0.05:
            raise Exception("Error code: 403 - rate limit")
        
        return {"task_id": task_id, "status": "success"}
    
    # åˆ›å»º20ä¸ªå¹¶å‘ä»»åŠ¡
    print("\nğŸš€ å‘èµ·20ä¸ªå¹¶å‘ä»»åŠ¡...")
    print("ï¼ˆç®¡ç†å™¨ä¼šè‡ªåŠ¨é™æµï¼Œé¿å…è¶…è¿‡max_concurrentï¼‰")
    
    tasks = []
    for i in range(20):
        task = manager.call_with_rate_limit(
            func=mock_llm_call,
            provider="deepseek",
            model="deepseek-chat",
            estimated_tokens=100,
            task_id=i+1
        )
        tasks.append(task)
    
    # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    success = sum(1 for r in results if not isinstance(r, Exception))
    failed = len(results) - success
    
    print(f"\nğŸ“Š ç»“æœ:")
    print(f"  æˆåŠŸ: {success}/20")
    print(f"  å¤±è´¥: {failed}/20")


async def demo_rate_limit_detection():
    """æ¼”ç¤ºé™æµæ£€æµ‹"""
    print("\n" + "=" * 80)
    print("ğŸ§ª æ¼”ç¤º3: é™æµæ£€æµ‹ä¸æ™ºèƒ½é‡è¯•")
    print("=" * 80)
    
    manager = get_llm_manager()
    
    def mock_llm_with_rate_limit(attempt: int):
        """æ¨¡æ‹Ÿä¼šè§¦å‘é™æµçš„è°ƒç”¨"""
        import time
        
        time.sleep(0.1)
        
        # å‰2æ¬¡å¿…å®šè§¦å‘é™æµï¼Œç¬¬3æ¬¡æˆåŠŸ
        if attempt < 2:
            raise Exception("Error code: 403 - access forbidden")
        
        return {"status": "success", "attempt": attempt}
    
    print("\nğŸ”„ è°ƒç”¨ä¼šè§¦å‘é™æµçš„API...")
    print("ï¼ˆç®¡ç†å™¨ä¼šè‡ªåŠ¨æ£€æµ‹é™æµå¹¶å»¶é•¿é‡è¯•é—´éš”ï¼‰")
    
    attempt_counter = [0]  # ä½¿ç”¨åˆ—è¡¨æ¥åœ¨é—­åŒ…ä¸­ä¿®æ”¹
    
    def wrapped_call():
        attempt_counter[0] += 1
        return mock_llm_with_rate_limit(attempt_counter[0])
    
    try:
        result = await manager.call_with_rate_limit(
            func=wrapped_call,
            provider="deepseek",
            model="deepseek-chat",
            estimated_tokens=100
        )
        print(f"\nâœ… æœ€ç»ˆæˆåŠŸ: {result}")
        print(f"   æ€»å°è¯•æ¬¡æ•°: {attempt_counter[0]}")
    except Exception as e:
        print(f"\nâŒ æœ€ç»ˆå¤±è´¥: {e}")


async def demo_config_update():
    """æ¼”ç¤ºé…ç½®æ›´æ–°"""
    print("\n" + "=" * 80)
    print("ğŸ§ª æ¼”ç¤º4: åŠ¨æ€æ›´æ–°é…ç½®")
    print("=" * 80)
    
    manager = get_llm_manager()
    
    # æŸ¥çœ‹å½“å‰é…ç½®
    config = manager.get_config("deepseek", "deepseek-chat")
    print(f"\nğŸ“‹ å½“å‰DeepSeeké…ç½®:")
    print(f"  QPM: {config.requests_per_minute}")
    print(f"  å¹¶å‘: {config.max_concurrent}")
    print(f"  é‡è¯•æ¬¡æ•°: {config.max_retries}")
    
    # æ›´æ–°é…ç½®
    print("\nğŸ”§ æ›´æ–°é…ç½®...")
    manager.update_config(
        "deepseek_chat",
        requests_per_minute=100,  # æé«˜QPM
        max_concurrent=3,  # æé«˜å¹¶å‘
        is_tested=True,
        last_test_date="2026-02-10",
        test_notes="æµ‹è¯•éªŒè¯ï¼šå¯æ”¯æŒæ›´é«˜QPM"
    )
    
    # æŸ¥çœ‹æ›´æ–°åçš„é…ç½®
    config = manager.get_config("deepseek", "deepseek-chat")
    print(f"\nğŸ“‹ æ›´æ–°åçš„é…ç½®:")
    print(f"  QPM: {config.requests_per_minute}")
    print(f"  å¹¶å‘: {config.max_concurrent}")
    print(f"  å·²æµ‹è¯•: {config.is_tested}")
    print(f"  æµ‹è¯•æ—¥æœŸ: {config.last_test_date}")
    print(f"  æµ‹è¯•å¤‡æ³¨: {config.test_notes}")


async def main():
    """è¿è¡Œæ‰€æœ‰æ¼”ç¤º"""
    print("=" * 80)
    print("ğŸš€ LLMè°ƒç”¨ç®¡ç†å™¨é›†æˆæµ‹è¯•")
    print("=" * 80)
    
    # æ¼”ç¤º1: åŸºæœ¬ä½¿ç”¨
    await demo_basic_usage()
    
    # ç­‰å¾…ä¸€ä¸‹
    await asyncio.sleep(2)
    
    # æ¼”ç¤º2: å¹¶å‘è°ƒç”¨
    await demo_concurrent_calls()
    
    # ç­‰å¾…ä¸€ä¸‹
    await asyncio.sleep(2)
    
    # æ¼”ç¤º3: é™æµæ£€æµ‹
    await demo_rate_limit_detection()
    
    # æ¼”ç¤º4: é…ç½®æ›´æ–°
    await demo_config_update()
    
    print("\n" + "=" * 80)
    print("âœ… æ‰€æœ‰æ¼”ç¤ºå®Œæˆï¼")
    print("=" * 80)
    print("\nğŸ’¡ æç¤º:")
    print("  - é…ç½®å·²ä¿å­˜åˆ°: config/llm_configs.json")
    print("  - å¯è¿è¡Œæµ‹è¯•å·¥å…·: python3 scripts/test/test_llm_rate_limits.py")
    print("  - æŸ¥çœ‹æ–‡æ¡£: docs/core/LLM_RATE_LIMIT_SYSTEM.md")


if __name__ == "__main__":
    asyncio.run(main())
