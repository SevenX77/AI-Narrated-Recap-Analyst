"""
æµ‹è¯• DeepSeek å¤šæ¨¡å‹é…ç½®
éªŒè¯ v3.2 å’Œ v3.2 thinking ä¸¤ä¸ªæ¨¡å‹éƒ½èƒ½æ­£ç¡®ä½¿ç”¨
"""

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from src.core.llm_client_manager import get_llm_client, get_model_name
from src.core.config import config


def test_deepseek_models():
    """æµ‹è¯• DeepSeek å¤šæ¨¡å‹é…ç½®"""
    print("\n" + "="*80)
    print("  DeepSeek å¤šæ¨¡å‹é…ç½®æµ‹è¯•")
    print("="*80)
    
    print("\nâœ… æ£€æŸ¥ 1: é…ç½®éªŒè¯\n")
    
    print(f"  DeepSeek API Key: {'å·²é…ç½®' if config.llm.deepseek_api_key else 'æœªé…ç½®'}")
    print(f"  DeepSeek Base URL: {config.llm.deepseek_base_url}")
    print(f"  DeepSeek v3.2 Model: {config.llm.deepseek_v32_model}")
    print(f"  DeepSeek v3.2 Thinking Model: {config.llm.deepseek_v32_thinking_model}")
    print(f"  DeepSeek Default Model: {config.llm.deepseek_model_name}")
    
    print("\nâœ… æ£€æŸ¥ 2: æ¨¡å‹åç§°è·å–\n")
    
    # æµ‹è¯•è·å–ä¸åŒæ¨¡å‹åç§°
    default_model = get_model_name("deepseek")
    v32_model = get_model_name("deepseek", model_type="v32")
    thinking_model = get_model_name("deepseek", model_type="v32-thinking")
    
    print(f"  é»˜è®¤æ¨¡å‹: {default_model}")
    print(f"  v3.2 æ ‡å‡†æ¨¡å‹: {v32_model}")
    print(f"  v3.2 æ€ç»´é“¾æ¨¡å‹: {thinking_model}")
    
    # éªŒè¯
    assert default_model == "deepseek-chat", f"é»˜è®¤æ¨¡å‹é”™è¯¯: {default_model}"
    assert v32_model == "deepseek-chat", f"v3.2æ¨¡å‹é”™è¯¯: {v32_model}"
    assert thinking_model == "deepseek-reasoner", f"æ€ç»´é“¾æ¨¡å‹é”™è¯¯: {thinking_model}"
    
    print("\n  âœ… æ‰€æœ‰æ¨¡å‹åç§°æ­£ç¡®")
    
    print("\nâœ… æ£€æŸ¥ 3: å®¢æˆ·ç«¯åˆ›å»º\n")
    
    try:
        client = get_llm_client("deepseek")
        print(f"  âœ… DeepSeek å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        print(f"  âŒ å®¢æˆ·ç«¯åˆ›å»ºå¤±è´¥: {e}")
        return False
    
    print("\n" + "="*80)
    print("  ä½¿ç”¨ç¤ºä¾‹")
    print("="*80)
    
    print("\n```python")
    print("from src.core.llm_client_manager import get_llm_client, get_model_name")
    print("")
    print("# ä½¿ç”¨ v3.2 æ ‡å‡†æ¨¡å‹ï¼ˆå¿«é€Ÿã€ä½æˆæœ¬ï¼‰")
    print("client = get_llm_client('deepseek')")
    print("model = get_model_name('deepseek', model_type='v32')")
    print("# model = 'deepseek-chat'")
    print("")
    print("# ä½¿ç”¨ v3.2 æ€ç»´é“¾æ¨¡å‹ï¼ˆæ·±åº¦æ¨ç†ï¼‰")
    print("client = get_llm_client('deepseek')")
    print("model = get_model_name('deepseek', model_type='v32-thinking')")
    print("# model = 'deepseek-reasoner'")
    print("")
    print("# è°ƒç”¨ç¤ºä¾‹")
    print("response = client.chat.completions.create(")
    print("    model=model,")
    print("    messages=[{'role': 'user', 'content': '...'}]")
    print(")")
    print("```")
    
    print("\n" + "="*80)
    print("  ğŸ“‹ æµ‹è¯•æ€»ç»“")
    print("="*80)
    
    print("\nâœ… DeepSeek å¤šæ¨¡å‹é…ç½®æ­£ç¡®")
    print("âœ… æ”¯æŒ v3.2 æ ‡å‡†æ¨¡å‹ï¼ˆdeepseek-chatï¼‰")
    print("âœ… æ”¯æŒ v3.2 æ€ç»´é“¾æ¨¡å‹ï¼ˆdeepseek-reasonerï¼‰")
    print("âœ… å·¥å…·å¯é€šè¿‡ model_type å‚æ•°é€‰æ‹©æ¨¡å‹")
    
    print("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
    print("  â€¢ ç®€å•ä»»åŠ¡/å¿«é€Ÿå“åº” â†’ v3.2 æ ‡å‡†æ¨¡å‹")
    print("  â€¢ å¤æ‚æ¨ç†/é€»è¾‘åˆ†æ â†’ v3.2 æ€ç»´é“¾æ¨¡å‹")
    print("  â€¢ é»˜è®¤ä½¿ç”¨æ ‡å‡†æ¨¡å‹ï¼ˆæ€§ä»·æ¯”é«˜ï¼‰")
    
    print("\n")
    return True


if __name__ == "__main__":
    success = test_deepseek_models()
    sys.exit(0 if success else 1)
