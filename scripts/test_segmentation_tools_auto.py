"""
è‡ªåŠ¨æµ‹è¯•Novel-to-Scriptæ™ºèƒ½æ”¹ç¼–ç³»ç»Ÿçš„æ–°å·¥å…·ï¼ˆéäº¤äº’å¼ï¼‰

æµ‹è¯•å†…å®¹ï¼š
1. NovelSegmentationAnalyzer - å°è¯´åˆ†æ®µæ·±åº¦åˆ†æ
2. KeyInfoExtractor - å…³é”®ä¿¡æ¯æå–
3. ScriptSegmentAligner - Script-Novelç²¾ç¡®å¯¹é½

ä½¿ç”¨é¡¹ç›®ï¼šæœ«å“¥è¶…å‡¡å…¬è·¯ï¼ˆPROJ_002ï¼‰
"""

import json
import sys
from pathlib import Path

# æ·»åŠ srcåˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.tools.novel_segmentation_analyzer import NovelSegmentationAnalyzer
from src.tools.key_info_extractor import KeyInfoExtractor
from src.tools.script_segment_aligner import ScriptSegmentAligner


def print_section(title: str):
    """æ‰“å°ç« èŠ‚æ ‡é¢˜"""
    print("\n" + "=" * 80)
    print(f"  {title}")
    print("=" * 80 + "\n")


def test_novel_segmentation_analyzer():
    """æµ‹è¯•NovelSegmentationAnalyzer"""
    print_section("æµ‹è¯• 1: NovelSegmentationAnalyzer - å°è¯´åˆ†æ®µæ·±åº¦åˆ†æ")
    
    # è¯»å–ç« èŠ‚å†…å®¹ï¼ˆç¬¬ä¸€ç« ï¼‰
    project_dir = Path(__file__).parent.parent / "data/projects/with_novel/æœ«å“¥è¶…å‡¡å…¬è·¯"
    novel_file = project_dir / "novel/chpt_0001-0010.md"
    
    if not novel_file.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {novel_file}")
        return None
    
    with open(novel_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æå–ç¬¬ä¸€ç« å†…å®¹ï¼ˆæŒ‰## ç¬¬Xç« åˆ†å‰²ï¼‰
    import re
    chapter_pattern = r'## ç¬¬(\d+)ç« '
    matches = list(re.finditer(chapter_pattern, content))
    
    if len(matches) < 1:
        print("âŒ æ— æ³•è§£æç« èŠ‚")
        print(f"æ–‡ä»¶å†…å®¹é¢„è§ˆ: {content[:200]}")
        return None
    
    # æå–ç¬¬ä¸€ç« ï¼ˆä»ç¬¬ä¸€ç« æ ‡é¢˜åˆ°ç¬¬äºŒç« æ ‡é¢˜ä¹‹å‰ï¼Œæˆ–åˆ°æ–‡ä»¶æœ«å°¾ï¼‰
    start_pos = matches[0].start()
    end_pos = matches[1].start() if len(matches) > 1 else len(content)
    first_chapter = content[start_pos:end_pos]
    
    # æˆªå–å‰1200å­—ç¬¦ç”¨äºæµ‹è¯•ï¼ˆé¿å…è¶…é•¿æ–‡æœ¬å’Œé«˜æ˜‚æˆæœ¬ï¼‰
    first_chapter = first_chapter[:1200]
    
    print(f"ğŸ“– ç« èŠ‚å†…å®¹é•¿åº¦: {len(first_chapter)} å­—ç¬¦")
    print(f"ğŸ“– ç« èŠ‚å†…å®¹é¢„è§ˆ:\n{first_chapter[:200]}...\n")
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = NovelSegmentationAnalyzer()
    
    print("ğŸ”„ æ­£åœ¨è°ƒç”¨LLMè¿›è¡Œåˆ†æ®µåˆ†æ...")
    print("â³ è¿™å¯èƒ½éœ€è¦10-20ç§’ï¼Œè¯·ç¨å€™...\n")
    
    try:
        # æ‰§è¡Œåˆ†æ
        result = analyzer.execute(
            chapter_text=first_chapter,
            chapter_id="chpt_0001",
            chapter_title="ç¬¬ä¸€ç« ",
            character_list=["é™ˆé‡"],
            world_settings={"setting": "æœ«æ—¥ä¸–ç•Œ", "system": "å‡çº§ç³»ç»Ÿ"}
        )
        
        print("âœ… åˆ†ææˆåŠŸï¼\n")
        
        # æ‰“å°ç»“æœç»Ÿè®¡
        print(f"ğŸ“Š åˆ†æç»“æœç»Ÿè®¡:")
        print(f"  - æ€»æ®µè½æ•°: {result.chapter_summary.total_segments}")
        print(f"  - P0-éª¨æ¶: {result.chapter_summary.p0_count}")
        print(f"  - P1-è¡€è‚‰: {result.chapter_summary.p1_count}")
        print(f"  - P2-çš®è‚¤: {result.chapter_summary.p2_count}")
        print(f"  - å…³é”®äº‹ä»¶: {', '.join(result.chapter_summary.key_events)}")
        if result.chapter_summary.foreshadowing_planted:
            print(f"  - åŸ‹è®¾ä¼ç¬”: {', '.join(result.chapter_summary.foreshadowing_planted)}")
        
        # æ‰“å°å‰3ä¸ªæ®µè½çš„è¯¦ç»†ä¿¡æ¯
        print(f"\nğŸ“ å‰{min(3, len(result.segments))}ä¸ªæ®µè½è¯¦ç»†ä¿¡æ¯:")
        for i, seg in enumerate(result.segments[:3]):
            print(f"\n  ã€æ®µè½ {i+1}ã€‘ {seg.segment_id}")
            print(f"  åŸæ–‡: {seg.text[:50]}...")
            print(f"  æ ‡ç­¾:")
            print(f"    - å™äº‹åŠŸèƒ½: {', '.join(seg.tags.narrative_function) if seg.tags.narrative_function else 'æ— '}")
            print(f"    - å™äº‹ç»“æ„: {', '.join(seg.tags.structure) if seg.tags.structure else 'æ— '}")
            print(f"    - ä¼˜å…ˆçº§: {seg.tags.priority}")
            print(f"  æµ“ç¼©å»ºè®®: {seg.metadata.condensation_suggestion[:50]}...")
        
        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        output_dir = project_dir / "novel/segmentation_analysis"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        output_file = output_dir / f"{result.chapter_id}_analysis_test.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result.model_dump(mode='json'), f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        return result
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def test_key_info_extractor(chapter_analysis):
    """æµ‹è¯•KeyInfoExtractor"""
    print_section("æµ‹è¯• 2: KeyInfoExtractor - å…³é”®ä¿¡æ¯æå–")
    
    if not chapter_analysis:
        print("âš ï¸  è·³è¿‡æµ‹è¯•ï¼ˆéœ€è¦å…ˆå®Œæˆæµ‹è¯•1ï¼‰")
        return None
    
    # åˆ›å»ºæå–å™¨
    extractor = KeyInfoExtractor()
    
    print("ğŸ”„ æ­£åœ¨æå–å…³é”®ä¿¡æ¯...")
    
    try:
        # æ‰§è¡Œæå–
        key_info = extractor.execute(
            chapter_analyses=[chapter_analysis],
            scope="test_chapter_1"
        )
        
        print("âœ… æå–æˆåŠŸï¼\n")
        
        # æ‰“å°ç»“æœç»Ÿè®¡
        print(f"ğŸ“Š å…³é”®ä¿¡æ¯ç»Ÿè®¡:")
        print(f"  - P0éª¨æ¶: {len(key_info.p0_skeleton)} é¡¹")
        print(f"  - P1è¡€è‚‰: {len(key_info.p1_flesh)} é¡¹")
        print(f"  - P2çš®è‚¤: {len(key_info.p2_skin)} é¡¹")
        print(f"  - è§’è‰²æ•°é‡: {len(key_info.character_arcs)}")
        
        # æ‰“å°P0ä¿¡æ¯
        if key_info.p0_skeleton:
            print(f"\nğŸ“Œ P0éª¨æ¶ä¿¡æ¯ï¼ˆå‰{min(3, len(key_info.p0_skeleton))}é¡¹ï¼‰:")
            for i, info in enumerate(key_info.p0_skeleton[:3]):
                print(f"  {i+1}. {info['segment_id']}")
                print(f"     å†…å®¹: {info['content'][:40]}...")
                print(f"     é‡è¦æ€§: {info['importance']}")
        
        # æ‰“å°ä¼ç¬”ä¿¡æ¯
        planted = key_info.foreshadowing_map.get("planted", [])
        if planted:
            print(f"\nğŸ£ åŸ‹è®¾çš„ä¼ç¬”:")
            for fh in planted:
                print(f"  - {fh['content']} (ç« èŠ‚: {fh['chapter_id']})")
        
        # æ‰“å°æµ“ç¼©æŒ‡å¯¼
        print(f"\nğŸ“‹ æµ“ç¼©æŒ‡å¯¼åŸåˆ™:")
        must_retain = key_info.condensation_guidelines.get('must_retain', [])
        can_simplify = key_info.condensation_guidelines.get('can_simplify', [])
        can_omit = key_info.condensation_guidelines.get('can_omit', [])
        
        if must_retain:
            print(f"  å¿…é¡»ä¿ç•™: {', '.join(must_retain[:3])}")
        if can_simplify:
            print(f"  å¯ä»¥ç®€åŒ–: {', '.join(can_simplify[:3])}")
        if can_omit:
            print(f"  å¯ä»¥çœç•¥: {', '.join(can_omit[:3])}")
        
        # ä¿å­˜ç»“æœ
        project_dir = Path(__file__).parent.parent / "data/projects/with_novel/æœ«å“¥è¶…å‡¡å…¬è·¯"
        output_dir = project_dir / "analysis"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        output_file = output_dir / "key_info_test.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(key_info.model_dump(mode='json'), f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        return key_info
        
    except Exception as e:
        print(f"âŒ æå–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    print("\n" + "ğŸš€" * 40)
    print("  Novel-to-Script æ™ºèƒ½æ”¹ç¼–ç³»ç»Ÿ - è‡ªåŠ¨åŒ–æµ‹è¯•")
    print("ğŸš€" * 40)
    
    print("\nğŸ“Œ æµ‹è¯•è¯´æ˜:")
    print("  - ä½¿ç”¨é¡¹ç›®: æœ«å“¥è¶…å‡¡å…¬è·¯ï¼ˆPROJ_002ï¼‰")
    print("  - æµ‹è¯•æ•°æ®: ç¬¬ä¸€ç« ï¼ˆå‰800å­—ç¬¦ï¼‰")
    print("  - LLMæ¨¡å‹: DeepSeek V3")
    print("  - é¢„è®¡è€—æ—¶: 15-30ç§’")
    print("  - æ³¨æ„: ä»…æµ‹è¯•å‰2ä¸ªå·¥å…·ï¼ˆTool 1å’Œ2ï¼‰ï¼ŒTool 3éœ€è¦å®Œæ•´ç« èŠ‚åˆ†æ")
    
    # æµ‹è¯•1: å°è¯´åˆ†æ®µåˆ†æ
    chapter_analysis = test_novel_segmentation_analyzer()
    
    if not chapter_analysis:
        print("\nâŒ æµ‹è¯•1å¤±è´¥ï¼Œæ— æ³•ç»§ç»­åç»­æµ‹è¯•")
        return
    
    # æµ‹è¯•2: å…³é”®ä¿¡æ¯æå–
    key_info = test_key_info_extractor(chapter_analysis)
    
    # æ€»ç»“
    print_section("æµ‹è¯•æ€»ç»“")
    
    results = {
        "NovelSegmentationAnalyzer": "âœ…" if chapter_analysis else "âŒ",
        "KeyInfoExtractor": "âœ…" if key_info else "âŒ",
    }
    
    print("æµ‹è¯•ç»“æœ:")
    for tool, status in results.items():
        print(f"  {status} {tool}")
    
    print("\nğŸ“Œ è¯´æ˜:")
    print("  - ScriptSegmentAligner éœ€è¦å®Œæ•´ç« èŠ‚åˆ†æï¼Œåœ¨æ­¤å¿«é€Ÿæµ‹è¯•ä¸­è·³è¿‡")
    print("  - è‹¥è¦æµ‹è¯•å®Œæ•´æµç¨‹ï¼Œè¯·ä½¿ç”¨å®Œæ•´ç« èŠ‚å†…å®¹")
    
    if all(r == "âœ…" for r in results.values()):
        print("\nğŸ‰ æ ¸å¿ƒå·¥å…·æµ‹è¯•é€šè¿‡ï¼å·¥å…·è¿è¡Œæ­£å¸¸ã€‚")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚")
    
    print("\nğŸ“ è¾“å‡ºæ–‡ä»¶ä½ç½®:")
    print("  - åˆ†æ®µåˆ†æ: data/projects/with_novel/æœ«å“¥è¶…å‡¡å…¬è·¯/novel/segmentation_analysis/chpt_0001_analysis_test.json")
    print("  - å…³é”®ä¿¡æ¯: data/projects/with_novel/æœ«å“¥è¶…å‡¡å…¬è·¯/analysis/key_info_test.json")
    
    print("\n" + "=" * 80)


if __name__ == "__main__":
    main()
