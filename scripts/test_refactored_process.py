"""
æµ‹è¯•é‡æ„åçš„å¤„ç†è„šæœ¬ - åªå¤„ç†ç¬¬1ç« 
"""

import sys
import json
from pathlib import Path

# æ·»åŠ srcåˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.tools.novel_chapter_processor import MetadataExtractor, NovelChapterProcessor
from src.tools.novel_chapter_analyzer import NovelChapterAnalyzer
from src.core.artifact_manager import ArtifactManager
from src.core.config import LLMConfig


def main():
    """ä¸»æµç¨‹ - åªæµ‹è¯•ç¬¬1ç« """
    print("="*80)
    print("æµ‹è¯•é‡æ„åçš„å¤„ç†è„šæœ¬ï¼ˆç¬¬1ç« ï¼‰")
    print("="*80)
    
    # é…ç½®è·¯å¾„
    project_root = Path(__file__).parent.parent
    project_dir = project_root / "data/projects/with_novel/æœ«å“¥è¶…å‡¡å…¬è·¯"
    raw_novel = project_dir / "raw/novel.txt"
    novel_dir = project_dir / "novel"
    analysis_dir = novel_dir / "functional_analysis"
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    novel_dir.mkdir(exist_ok=True)
    analysis_dir.mkdir(exist_ok=True)
    
    # è¯»å–åŸå§‹å°è¯´
    print(f"\nğŸ“– è¯»å–å°è¯´: {raw_novel}")
    with open(raw_novel, 'r', encoding='utf-8') as f:
        novel_text = f.read()
    
    # Step 1: ä½¿ç”¨ MetadataExtractor æå–ç®€ä»‹
    print("\n" + "="*80)
    print("Step 1: æå–ç®€ä»‹ï¼ˆä½¿ç”¨ MetadataExtractorï¼‰")
    print("="*80)
    
    extractor = MetadataExtractor(use_llm=True)
    metadata = extractor.execute(novel_text)
    
    print(f"âœ… ç®€ä»‹æå–å®Œæˆ:")
    print(f"   ä½œè€…: {metadata['novel']['author']}")
    print(f"   æ ‡ç­¾: {', '.join(metadata['novel']['tags'])}")
    print(f"   ç®€ä»‹é•¿åº¦: {len(metadata['novel']['introduction'])} å­—ç¬¦")
    print(f"   ç®€ä»‹é¢„è§ˆ: {metadata['novel']['introduction'][:100]}...")
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«"åˆæœ‰ä¹¦å"
    if "åˆæœ‰ä¹¦å" in metadata['novel']['introduction']:
        print("   âŒ ç®€ä»‹ä»åŒ…å«'åˆæœ‰ä¹¦å'")
    else:
        print("   âœ… ç®€ä»‹å·²æ¸…ç†")
    
    # Step 2: è¯»å–å·²å­˜åœ¨çš„ç¬¬1ç« æ–‡ä»¶
    print("\n" + "="*80)
    print("Step 2: è¯»å–ç¬¬1ç« ")
    print("="*80)
    
    chapter_file = novel_dir / "chpt_0001.md"
    if not chapter_file.exists():
        print(f"âŒ ç¬¬1ç« æ–‡ä»¶ä¸å­˜åœ¨: {chapter_file}")
        return
    
    with open(chapter_file, 'r', encoding='utf-8') as f:
        chapter_content = f.read()
    
    print(f"âœ… ç¬¬1ç« å·²è¯»å–: {len(chapter_content)} å­—ç¬¦")
    
    # Step 3: ä½¿ç”¨ NovelChapterAnalyzerï¼ˆå†…ç½®V3->R1 fallbackï¼‰
    print("\n" + "="*80)
    print("Step 3: åŠŸèƒ½åˆ†æï¼ˆä½¿ç”¨ NovelChapterAnalyzer + å†…ç½®Fallbackæœºåˆ¶ï¼‰")
    print("="*80)
    
    llm_config = LLMConfig()
    print(f"   ä¸»æ¨¡å‹: {llm_config.primary_model}")
    print(f"   å¤‡ç”¨æ¨¡å‹: {llm_config.fallback_model}")
    print(f"   Fallbackå¯ç”¨: {llm_config.enable_fallback}")
    
    analyzer = NovelChapterAnalyzer()
    
    try:
        # æå–ç« èŠ‚å·å’Œæ ‡é¢˜
        import re
        title_match = re.search(r'# ç¬¬(\d+)ç«  - (.+)', chapter_content)
        chapter_number = int(title_match.group(1)) if title_match else 1
        chapter_title = title_match.group(2).strip() if title_match else "æœªçŸ¥æ ‡é¢˜"
        
        print(f"   ç« èŠ‚: ç¬¬{chapter_number}ç«  - {chapter_title}")
        
        analysis = analyzer.execute(
            chapter_content=chapter_content,
            chapter_number=chapter_number,
            chapter_title=chapter_title
        )
        
        # è½¬æ¢ä¸ºå­—å…¸ï¼ˆanalysis æ˜¯ Pydantic å¯¹è±¡ï¼‰
        # ä½¿ç”¨ mode='json' è‡ªåŠ¨å¤„ç† datetime ç­‰ç‰¹æ®Šç±»å‹
        analysis_dict = analysis.model_dump(mode='json')
        
        print(f"âœ… ç¬¬1ç« åˆ†æå®Œæˆ:")
        print(f"   åŠŸèƒ½æ®µæ•°: {len(analysis_dict['segments'])}")
        print(f"   ç¬¬1æ®µå­—æ•°: {len(analysis_dict['segments'][0]['content'])} å­—ç¬¦")
        print(f"   ç¬¬1æ®µåŠŸèƒ½: {analysis_dict['segments'][0]['tags']['narrative_function']}")
        print(f"   ç¬¬1æ®µä¼˜å…ˆçº§: {analysis_dict['segments'][0]['tags']['priority']}")
        
        # Step 4: ä½¿ç”¨ ArtifactManager ä¿å­˜
        print("\n" + "="*80)
        print("Step 4: ä¿å­˜ç»“æœï¼ˆä½¿ç”¨ ArtifactManagerï¼‰")
        print("="*80)
        
        artifact_type = "chpt_0001_functional_analysis"
        versioned_path = ArtifactManager.save_artifact(
            content=analysis_dict,
            artifact_type=artifact_type,
            project_id="æœ«å“¥è¶…å‡¡å…¬è·¯",
            base_dir=str(analysis_dir),
            extension="json"
        )
        
        print(f"âœ… å·²ä¿å­˜ç‰ˆæœ¬åŒ–æ–‡ä»¶: {Path(versioned_path).name}")
        
        # éªŒè¯ _latest.json å’Œ history/ ç»“æ„
        latest_file = analysis_dir / f"{artifact_type}_latest.json"
        history_dir = analysis_dir / "history"
        
        print("\nğŸ“‚ éªŒè¯ç‰ˆæœ¬ç®¡ç†:")
        print(f"   _latest.json å­˜åœ¨: {latest_file.exists()}")
        print(f"   history/ ç›®å½•å­˜åœ¨: {history_dir.exists()}")
        
        if history_dir.exists():
            history_files = list(history_dir.glob(f"{artifact_type}_v*.json"))
            print(f"   history/ ä¸­çš„ç‰ˆæœ¬æ•°: {len(history_files)}")
            for hf in history_files:
                print(f"      - {hf.name}")
        
        # æ£€æŸ¥ä¸»ç›®å½•ä¸­æ˜¯å¦è¿˜æœ‰æ—§ç‰ˆæœ¬æ–‡ä»¶
        root_versions = list(analysis_dir.glob(f"{artifact_type}_v*.json"))
        if root_versions:
            print(f"   âš ï¸  ä¸»ç›®å½•ä¸­ä»æœ‰ç‰ˆæœ¬æ–‡ä»¶: {len(root_versions)} ä¸ª")
            for rv in root_versions:
                print(f"      - {rv.name}")
        else:
            print(f"   âœ… ä¸»ç›®å½•åªæœ‰ _latest.json")
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    print("\n" + "="*80)
    print("âœ… æµ‹è¯•å®Œæˆï¼")
    print("="*80)


if __name__ == "__main__":
    main()
