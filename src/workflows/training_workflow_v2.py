"""
çƒ­åº¦é©±åŠ¨çš„è®­ç»ƒå·¥ä½œæµ (Training Workflow V2)

åŸºäºçœŸå®çƒ­åº¦æ•°æ®çš„è§„åˆ™å­¦ä¹ å’Œå†…å®¹è¯„ä¼°ç³»ç»Ÿ
"""

import os
import json
from typing import Dict, Any, List, Optional
from datetime import datetime

from src.core.interfaces import BaseWorkflow
from src.core.schemas_feedback import RuleBook, ValidationResult, ComparativeFeedback
from src.agents.rule_extractor import RuleExtractorAgent
from src.agents.rule_validator import RuleValidatorAgent
from src.agents.comparative_evaluator import ComparativeEvaluatorAgent
from src.agents.deepseek_analyst import get_llm_client
from src.core.artifact_manager import artifact_manager
from src.core.project_manager import project_manager
from src.utils.logger import logger, op_logger


class HeatDrivenTrainingWorkflow(BaseWorkflow):
    """
    çƒ­åº¦é©±åŠ¨çš„è®­ç»ƒå·¥ä½œæµ
    
    å·¥ä½œæµç¨‹ï¼š
    1. è§„åˆ™æå–ï¼šä»å¤šä¸ªGTé¡¹ç›®ä¸­æå–çˆ†æ¬¾è§„åˆ™
    2. è§„åˆ™éªŒè¯ï¼šéªŒè¯è§„åˆ™èƒ½å¦é¢„æµ‹GTé¡¹ç›®çš„çƒ­åº¦
    3. è§„åˆ™ä¼˜åŒ–ï¼šæ ¹æ®éªŒè¯ç»“æœè°ƒæ•´è§„åˆ™æƒé‡
    4. å†…å®¹è¯„ä¼°ï¼šç”¨ä¼˜åŒ–åçš„è§„åˆ™è¯„ä¼°æ–°ç”Ÿæˆçš„å†…å®¹
    """
    
    def __init__(self):
        """åˆå§‹åŒ–å·¥ä½œæµ"""
        super().__init__()
        self.client = get_llm_client()
        
        # åˆå§‹åŒ–Agents
        self.rule_extractor = RuleExtractorAgent(self.client)
        self.rule_validator = RuleValidatorAgent(self.client)
        self.comparative_evaluator = ComparativeEvaluatorAgent(self.client)
        
        # æ³¨å†ŒAgents
        self.register_agent(self.rule_extractor)
        self.register_agent(self.rule_validator)
        self.register_agent(self.comparative_evaluator)
        
        # è§„åˆ™åº“å­˜å‚¨è·¯å¾„
        self.rulebook_dir = os.path.join("data", "rule_books")
        os.makedirs(self.rulebook_dir, exist_ok=True)
    
    async def run(self, mode: str = "extract", **kwargs):
        """
        è¿è¡Œå·¥ä½œæµ
        
        Args:
            mode: è¿è¡Œæ¨¡å¼
                - "extract": æå–è§„åˆ™
                - "validate": éªŒè¯è§„åˆ™
                - "evaluate": è¯„ä¼°æ–°å†…å®¹
                - "full": å®Œæ•´æµç¨‹ï¼ˆæå–â†’éªŒè¯â†’è¯„ä¼°ï¼‰
            **kwargs: å…¶ä»–å‚æ•°
        """
        logger.info(f"ğŸš€ å¯åŠ¨çƒ­åº¦é©±åŠ¨è®­ç»ƒå·¥ä½œæµ (æ¨¡å¼: {mode})")
        
        if mode == "extract":
            return await self._run_rule_extraction(**kwargs)
        elif mode == "validate":
            return await self._run_rule_validation(**kwargs)
        elif mode == "evaluate":
            return await self._run_content_evaluation(**kwargs)
        elif mode == "full":
            return await self._run_full_pipeline(**kwargs)
        else:
            raise ValueError(f"æœªçŸ¥æ¨¡å¼: {mode}")
    
    async def _run_rule_extraction(
        self,
        gt_project_ids: Optional[List[str]] = None,
        **kwargs
    ) -> RuleBook:
        """
        è¿è¡Œè§„åˆ™æå–æµç¨‹
        
        Args:
            gt_project_ids: Ground Truthé¡¹ç›®IDåˆ—è¡¨
            
        Returns:
            RuleBook: æå–çš„è§„åˆ™åº“
        """
        logger.info("=" * 60)
        logger.info("é˜¶æ®µ1: è§„åˆ™æå–")
        logger.info("=" * 60)
        
        # 1. åŠ è½½é¡¹ç›®ç´¢å¼•ï¼Œè·å–çƒ­åº¦æ•°æ®
        project_index = self._load_project_index()
        
        # 2. ç­›é€‰Ground Truthé¡¹ç›®
        if not gt_project_ids:
            gt_project_ids = [
                pid for pid, info in project_index["projects"].items()
                if info.get("is_ground_truth", False) and info.get("heat_score") is not None
            ]
        
        if not gt_project_ids:
            raise ValueError("æœªæ‰¾åˆ°å¯ç”¨çš„Ground Truthé¡¹ç›®ï¼ˆéœ€è¦è®¾ç½®is_ground_truth=trueä¸”æœ‰heat_scoreï¼‰")
        
        logger.info(f"ğŸ“‹ æ‰¾åˆ° {len(gt_project_ids)} ä¸ªGround Truthé¡¹ç›®: {gt_project_ids}")
        
        # 3. åŠ è½½å„é¡¹ç›®æ•°æ®
        projects_data = {}
        heat_scores = {}
        explosive_flags = {}
        
        for project_id in gt_project_ids:
            project_info = project_index["projects"][project_id]
            heat_score = project_info.get("heat_score")
            
            if heat_score is None:
                logger.warning(f"âš ï¸  {project_id} ç¼ºå°‘çƒ­åº¦å€¼ï¼Œè·³è¿‡")
                continue
            
            # åŠ è½½é¡¹ç›®æ•°æ®
            project_data = self._load_project_data(project_id)
            if project_data:
                projects_data[project_id] = project_data
                heat_scores[project_id] = heat_score
                explosive_flags[project_id] = project_info.get("is_explosive", False)
                
                explosive_tag = " ğŸ”¥" if explosive_flags[project_id] else ""
                logger.info(f"âœ… {project_id}: çƒ­åº¦={heat_score}{explosive_tag}")
        
        if not projects_data:
            raise ValueError("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•é¡¹ç›®æ•°æ®")
        
        # 4. æå–è§„åˆ™
        logger.info("\nğŸ” å¼€å§‹æå–è§„åˆ™...")
        explosive_count = sum(1 for is_exp in explosive_flags.values() if is_exp)
        if explosive_count > 0:
            logger.info(f"   ğŸ“Œ åŒ…å« {explosive_count} ä¸ªå·²éªŒè¯çˆ†æ¬¾é¡¹ç›®ï¼ˆå°†è·å¾—æ›´é«˜æƒé‡ï¼‰")
        
        rulebook = self.rule_extractor.extract_rules_from_projects(
            projects_data,
            heat_scores,
            explosive_flags
        )
        
        # 5. ä¿å­˜è§„åˆ™åº“
        rulebook_path = self._save_rulebook(rulebook)
        logger.info(f"\nâœ… è§„åˆ™åº“å·²ä¿å­˜: {rulebook_path}")
        
        # 6. è®°å½•æ“ä½œ
        op_logger.log_operation(
            project_id="SYSTEM",
            action="Rule Extraction",
            output_files=[rulebook_path],
            details=f"ä» {len(projects_data)} ä¸ªé¡¹ç›®æå–è§„åˆ™"
        )
        
        return rulebook
    
    async def _run_rule_validation(
        self,
        rulebook: Optional[RuleBook] = None,
        rulebook_version: Optional[str] = None,
        **kwargs
    ) -> ValidationResult:
        """
        è¿è¡Œè§„åˆ™éªŒè¯æµç¨‹
        
        Args:
            rulebook: è§„åˆ™åº“å¯¹è±¡ï¼ˆå¦‚æœä¸æä¾›ï¼Œåˆ™åŠ è½½æœ€æ–°ç‰ˆæœ¬ï¼‰
            rulebook_version: è§„åˆ™åº“ç‰ˆæœ¬ï¼ˆå¦‚æœä¸æä¾›rulebookå¯¹è±¡ï¼‰
            
        Returns:
            ValidationResult: éªŒè¯ç»“æœ
        """
        logger.info("=" * 60)
        logger.info("é˜¶æ®µ2: è§„åˆ™éªŒè¯")
        logger.info("=" * 60)
        
        # 1. åŠ è½½è§„åˆ™åº“
        if not rulebook:
            rulebook = self._load_rulebook(rulebook_version)
        
        logger.info(f"ğŸ“– ä½¿ç”¨è§„åˆ™åº“: {rulebook.version}")
        
        # 2. åŠ è½½GTé¡¹ç›®æ•°æ®
        project_index = self._load_project_index()
        gt_project_ids = rulebook.extracted_from_projects
        
        projects_data = {}
        actual_heat_scores = {}
        
        for project_id in gt_project_ids:
            project_info = project_index["projects"].get(project_id)
            if not project_info:
                continue
            
            heat_score = project_info.get("heat_score")
            if heat_score is None:
                continue
            
            project_data = self._load_project_data(project_id)
            if project_data:
                projects_data[project_id] = project_data
                actual_heat_scores[project_id] = heat_score
        
        # 3. éªŒè¯è§„åˆ™
        logger.info("\nğŸ” å¼€å§‹éªŒè¯è§„åˆ™...")
        validation_result = self.rule_validator.validate_rulebook(
            rulebook,
            projects_data,
            actual_heat_scores
        )
        
        # 4. ä¿å­˜éªŒè¯ç»“æœ
        validation_path = os.path.join(
            self.rulebook_dir,
            f"validation_{rulebook.version}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        )
        with open(validation_path, 'w', encoding='utf-8') as f:
            json.dump(validation_result.model_dump(), f, indent=2, ensure_ascii=False)
        
        logger.info(f"\nâœ… éªŒè¯ç»“æœå·²ä¿å­˜: {validation_path}")
        logger.info(f"   - ç›¸å…³æ€§: {validation_result.correlation:.2f}")
        logger.info(f"   - æ˜¯å¦é€šè¿‡: {validation_result.is_valid}")
        
        # 5. å¦‚æœéªŒè¯é€šè¿‡ï¼Œæ›´æ–°è§„åˆ™åº“çŠ¶æ€
        if validation_result.is_valid:
            rulebook.heat_prediction_accuracy = validation_result.correlation
            self._save_rulebook(rulebook)
            logger.info("âœ… è§„åˆ™åº“å·²æ›´æ–°é¢„æµ‹å‡†ç¡®ç‡")
        else:
            logger.warning("âš ï¸  è§„åˆ™éªŒè¯æœªé€šè¿‡ï¼Œå»ºè®®æ ¹æ®ä¼˜åŒ–å»ºè®®è°ƒæ•´è§„åˆ™")
            for suggestion in validation_result.optimization_suggestions:
                logger.warning(f"   - {suggestion}")
        
        # 6. è®°å½•æ“ä½œ
        op_logger.log_operation(
            project_id="SYSTEM",
            action="Rule Validation",
            output_files=[validation_path],
            details=f"éªŒè¯è§„åˆ™åº“ {rulebook.version}, ç›¸å…³æ€§={validation_result.correlation:.2f}"
        )
        
        return validation_result
    
    async def _run_content_evaluation(
        self,
        project_id: str,
        generated_content: Optional[Dict[str, Any]] = None,
        gt_reference_project: Optional[str] = None,
        rulebook: Optional[RuleBook] = None,
        **kwargs
    ) -> ComparativeFeedback:
        """
        è¿è¡Œå†…å®¹è¯„ä¼°æµç¨‹
        
        Args:
            project_id: å¾…è¯„ä¼°é¡¹ç›®ID
            generated_content: ç”Ÿæˆçš„å†…å®¹æ•°æ®ï¼ˆå¦‚æœä¸æä¾›ï¼Œåˆ™ä»é¡¹ç›®ä¸­åŠ è½½ï¼‰
            gt_reference_project: å‚è€ƒçš„GTé¡¹ç›®ID
            rulebook: è§„åˆ™åº“ï¼ˆå¦‚æœä¸æä¾›ï¼Œåˆ™åŠ è½½æœ€æ–°ç‰ˆæœ¬ï¼‰
            
        Returns:
            ComparativeFeedback: è¯„ä¼°æŠ¥å‘Š
        """
        logger.info("=" * 60)
        logger.info(f"é˜¶æ®µ3: å†…å®¹è¯„ä¼° (é¡¹ç›®: {project_id})")
        logger.info("=" * 60)
        
        # 1. åŠ è½½è§„åˆ™åº“
        if not rulebook:
            rulebook = self._load_rulebook()
        
        logger.info(f"ğŸ“– ä½¿ç”¨è§„åˆ™åº“: {rulebook.version}")
        
        # 2. åŠ è½½Generatedå†…å®¹
        if not generated_content:
            generated_content = self._load_generated_content(project_id)
        
        # 3. ç¡®å®šå‚è€ƒGTé¡¹ç›®
        if not gt_reference_project:
            # ä½¿ç”¨çƒ­åº¦æœ€é«˜çš„GTé¡¹ç›®ä½œä¸ºå‚è€ƒ
            heat_scores = rulebook.project_heat_scores
            gt_reference_project = max(heat_scores.items(), key=lambda x: x[1])[0]
        
        logger.info(f"ğŸ“š å‚è€ƒGTé¡¹ç›®: {gt_reference_project}")
        
        # 4. åŠ è½½GTå†…å®¹
        gt_content = self._load_project_data(gt_reference_project)
        gt_heat_score = rulebook.project_heat_scores[gt_reference_project]
        
        # 5. å¯¹æ¯”è¯„ä¼°
        logger.info("\nğŸ” å¼€å§‹å¯¹æ¯”è¯„ä¼°...")
        feedback = self.comparative_evaluator.compare_with_ground_truth(
            generated_content,
            gt_content,
            gt_heat_score,
            rulebook,
            gt_reference_project
        )
        
        # 6. ä¿å­˜è¯„ä¼°æŠ¥å‘Š
        paths = project_manager.get_project_paths(project_id)
        report_dir = os.path.join(paths['root'], "training", "reports")
        os.makedirs(report_dir, exist_ok=True)
        
        report_path = artifact_manager.save_artifact(
            feedback.model_dump(),
            "comparative_feedback",
            project_id,
            report_dir
        )
        
        logger.info(f"\nâœ… è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        logger.info(f"   - æ€»åˆ†: {feedback.total_score}/{feedback.max_score}")
        logger.info(f"   - é¢„æµ‹çƒ­åº¦: {feedback.predicted_heat_score:.1f}")
        logger.info(f"   - å»ºè®®: {feedback.recommendation}")
        
        # 7. è®°å½•æ“ä½œ
        op_logger.log_operation(
            project_id=project_id,
            action="Content Evaluation",
            output_files=[report_path],
            details=f"å¾—åˆ†={feedback.total_score}, é¢„æµ‹çƒ­åº¦={feedback.predicted_heat_score:.1f}"
        )
        
        return feedback
    
    async def _run_full_pipeline(self, **kwargs) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´æµç¨‹ï¼šæå–â†’éªŒè¯â†’è¯„ä¼°
        
        Returns:
            åŒ…å«æ‰€æœ‰ç»“æœçš„å­—å…¸
        """
        logger.info("ğŸš€ è¿è¡Œå®Œæ•´çƒ­åº¦é©±åŠ¨è®­ç»ƒæµç¨‹")
        
        # 1. æå–è§„åˆ™
        rulebook = await self._run_rule_extraction(**kwargs)
        
        # 2. éªŒè¯è§„åˆ™
        validation_result = await self._run_rule_validation(rulebook=rulebook, **kwargs)
        
        # 3. å¦‚æœæœ‰å¾…è¯„ä¼°é¡¹ç›®ï¼Œè¿›è¡Œè¯„ä¼°
        project_id = kwargs.get('eval_project_id')
        feedback = None
        if project_id:
            feedback = await self._run_content_evaluation(
                project_id=project_id,
                rulebook=rulebook,
                **kwargs
            )
        
        logger.info("\n" + "=" * 60)
        logger.info("âœ… å®Œæ•´æµç¨‹æ‰§è¡Œå®Œæ¯•")
        logger.info("=" * 60)
        
        return {
            "rulebook": rulebook,
            "validation_result": validation_result,
            "feedback": feedback
        }
    
    # ==================== è¾…åŠ©æ–¹æ³• ====================
    
    def _load_project_index(self) -> Dict[str, Any]:
        """åŠ è½½é¡¹ç›®ç´¢å¼•"""
        index_path = os.path.join("data", "project_index.json")
        with open(index_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def _load_project_data(self, project_id: str) -> Optional[Dict[str, Any]]:
        """
        åŠ è½½é¡¹ç›®æ•°æ®
        
        Args:
            project_id: é¡¹ç›®ID
            
        Returns:
            é¡¹ç›®æ•°æ®å­—å…¸
        """
        try:
            paths = project_manager.get_project_paths(project_id)
            
            # åŠ è½½SRT
            srt_path = os.path.join(paths['raw'], "ep01.srt")
            if not os.path.exists(srt_path):
                logger.warning(f"âš ï¸  {project_id}: æœªæ‰¾åˆ°ep01.srt")
                return None
            
            with open(srt_path, 'r', encoding='utf-8') as f:
                srt_content = f.read()
            
            # åŠ è½½äº‹ä»¶æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
            events_path = os.path.join(paths['alignment'], "_backup", "ep01_script_events_latest.json")
            events = []
            if os.path.exists(events_path):
                with open(events_path, 'r', encoding='utf-8') as f:
                    events_data = json.load(f)
                    events = events_data.get('events', [])
            
            return {
                'srt_content': srt_content,
                'events': events,
                'hook_info': {}  # å¯ä»¥ä»hookæ£€æµ‹ç»“æœä¸­åŠ è½½
            }
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½é¡¹ç›® {project_id} æ•°æ®å¤±è´¥: {e}")
            return None
    
    def _load_generated_content(self, project_id: str) -> Dict[str, Any]:
        """åŠ è½½ç”Ÿæˆçš„å†…å®¹"""
        paths = project_manager.get_project_paths(project_id)
        script_path = os.path.join(paths['root'], "production", "scripts", "ep01_script_latest.json")
        
        if not os.path.exists(script_path):
            raise FileNotFoundError(f"æœªæ‰¾åˆ°ç”Ÿæˆçš„script: {script_path}")
        
        with open(script_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def _save_rulebook(self, rulebook: RuleBook) -> str:
        """ä¿å­˜è§„åˆ™åº“"""
        filename = f"rulebook_{rulebook.version}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        filepath = os.path.join(self.rulebook_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(rulebook.model_dump(), f, indent=2, ensure_ascii=False)
        
        # åŒæ—¶ä¿å­˜ä¸€ä»½latestç‰ˆæœ¬
        latest_path = os.path.join(self.rulebook_dir, f"rulebook_{rulebook.version}_latest.json")
        with open(latest_path, 'w', encoding='utf-8') as f:
            json.dump(rulebook.model_dump(), f, indent=2, ensure_ascii=False)
        
        return filepath
    
    def _load_rulebook(self, version: Optional[str] = None) -> RuleBook:
        """
        åŠ è½½è§„åˆ™åº“
        
        Args:
            version: ç‰ˆæœ¬å·ï¼ˆå¦‚æœä¸æä¾›ï¼ŒåŠ è½½æœ€æ–°ç‰ˆæœ¬ï¼‰
            
        Returns:
            RuleBook
        """
        if version:
            filepath = os.path.join(self.rulebook_dir, f"rulebook_{version}_latest.json")
        else:
            # æŸ¥æ‰¾æœ€æ–°çš„rulebookæ–‡ä»¶
            files = [f for f in os.listdir(self.rulebook_dir) if f.startswith("rulebook_") and f.endswith("_latest.json")]
            if not files:
                raise FileNotFoundError("æœªæ‰¾åˆ°ä»»ä½•è§„åˆ™åº“æ–‡ä»¶")
            files.sort(reverse=True)
            filepath = os.path.join(self.rulebook_dir, files[0])
        
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        return RuleBook(**data)
