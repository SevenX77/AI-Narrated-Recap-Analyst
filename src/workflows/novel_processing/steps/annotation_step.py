import logging
import asyncio
from typing import List, Dict
from src.core.schemas_novel import (
    ParagraphSegmentationResult,
    AnnotatedChapter,
    NovelProcessingConfig
)
from src.workflows.novel_processing.base import NovelProcessingWorkflowBase

logger = logging.getLogger(__name__)

async def step5_annotate_chapters(
    workflow: NovelProcessingWorkflowBase,
    segmentation_results: Dict[int, ParagraphSegmentationResult],
    workflow_config: NovelProcessingConfig,
    processing_dir: str
) -> Dict[int, AnnotatedChapter]:
    """Step 5: ç« èŠ‚å¹¶è¡Œæ ‡æ³¨"""
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ·ï¸ Step 5: ç« èŠ‚å¹¶è¡Œæ ‡æ³¨ (Three-Pass)")
    logger.info("=" * 60)
    
    annotation_results = {}
    chapters = list(segmentation_results.keys())
    
    if workflow_config.enable_parallel:
        # å¹¶è¡Œå¤„ç†
        logger.info(f"ğŸ”€ å¹¶è¡Œå¤„ç†æ¨¡å¼: å¹¶å‘æ•°={workflow_config.max_concurrent_chapters}")
        
        for i in range(0, len(chapters), workflow_config.max_concurrent_chapters):
            batch_chapters = chapters[i:i + workflow_config.max_concurrent_chapters]
            batch_results = await _process_annotation_batch(
                workflow,
                batch_chapters,
                segmentation_results,
                workflow_config,
                processing_dir
            )
            annotation_results.update(batch_results)
    else:
        # ä¸²è¡Œå¤„ç†
        logger.info("ğŸ“ ä¸²è¡Œå¤„ç†æ¨¡å¼")
        for chapter_num in chapters:
            try:
                result = await _annotate_single_chapter(
                    workflow,
                    chapter_num,
                    segmentation_results[chapter_num],
                    workflow_config
                )
                annotation_results[chapter_num] = result
                workflow._save_intermediate_result(
                    result,
                    f"step5_annotation/chapter_{chapter_num:03d}",
                    processing_dir
                )
            except Exception as e:
                logger.error(f"âŒ ç« èŠ‚{chapter_num}æ ‡æ³¨å¤±è´¥: {e}")
                if not workflow_config.continue_on_error:
                    raise
    
    logger.info(f"âœ… æ ‡æ³¨å®Œæˆ: {len(annotation_results)}/{len(chapters)} ç« èŠ‚")
    return annotation_results

async def _process_annotation_batch(
    workflow: NovelProcessingWorkflowBase,
    batch_chapters: List[int],
    segmentation_results: Dict[int, ParagraphSegmentationResult],
    workflow_config: NovelProcessingConfig,
    processing_dir: str
) -> Dict[int, AnnotatedChapter]:
    """å¹¶è¡Œå¤„ç†ä¸€æ‰¹ç« èŠ‚çš„æ ‡æ³¨"""
    tasks = [
        _annotate_single_chapter(
            workflow,
            chapter_num,
            segmentation_results[chapter_num],
            workflow_config
        )
        for chapter_num in batch_chapters
    ]
    
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    batch_results = {}
    for chapter_num, result in zip(batch_chapters, results):
        if isinstance(result, Exception):
            logger.error(f"âŒ ç« èŠ‚{chapter_num}æ ‡æ³¨å¤±è´¥: {result}")
            if not workflow_config.continue_on_error:
                raise result
        else:
            batch_results[chapter_num] = result
            workflow._save_intermediate_result(
                result,
                f"step5_annotation/chapter_{chapter_num:03d}",
                processing_dir
            )
    
    return batch_results

async def _annotate_single_chapter(
    workflow: NovelProcessingWorkflowBase,
    chapter_num: int,
    segmentation_result: ParagraphSegmentationResult,
    workflow_config: NovelProcessingConfig
) -> AnnotatedChapter:
    """æ ‡æ³¨å•ä¸ªç« èŠ‚ï¼ˆä½¿ç”¨LLMç®¡ç†å™¨ï¼‰"""
    logger.info(f"   å¤„ç†ç« èŠ‚ {chapter_num}")
    
    # ä¼°ç®—tokenæ•°ï¼ˆåŸºäºæ®µè½å†…å®¹ï¼‰
    total_chars = sum(len(p.content) for p in segmentation_result.paragraphs)
    estimated_tokens = workflow._estimate_tokens("x" * total_chars)
    
    # ä½¿ç”¨LLMç®¡ç†å™¨è°ƒç”¨ï¼ˆè‡ªåŠ¨é™æµ+é‡è¯•ï¼‰
    result = await workflow.llm_manager.call_with_rate_limit(
        func=workflow.novel_annotator.execute,
        provider=workflow_config.annotation_provider,
        model="claude-sonnet-4-5-20250929" if workflow_config.annotation_provider == "claude" else "deepseek-chat",
        estimated_tokens=estimated_tokens,
        segmentation_result=segmentation_result,
        enable_functional_tags=workflow_config.enable_functional_tags
    )
    
    # LLM calls: Pass1 + Pass2 + (optional Pass3)
    llm_calls = 3 if workflow_config.enable_functional_tags else 2
    workflow.llm_calls_count += llm_calls
    
    event_count = len(result.event_timeline.events)
    setting_count = len(result.setting_library.settings)
    logger.info(f"   âœ… ç« èŠ‚{chapter_num}: {event_count}ä¸ªäº‹ä»¶, {setting_count}ä¸ªè®¾å®š")
    
    return result
