import logging
import asyncio
from typing import List, Dict
from src.core.schemas_novel import (
    ChapterInfo,
    ParagraphSegmentationResult,
    NovelProcessingConfig
)
from src.workflows.novel_processing.base import NovelProcessingWorkflowBase

logger = logging.getLogger(__name__)

async def step4_segment_chapters(
    workflow: NovelProcessingWorkflowBase,
    novel_path: str,
    chapters: List[ChapterInfo],
    workflow_config: NovelProcessingConfig,
    processing_dir: str
) -> Dict[int, ParagraphSegmentationResult]:
    """Step 4: ç« èŠ‚å¹¶è¡Œåˆ†æ®µ"""
    logger.info("\n" + "=" * 60)
    logger.info("âœ‚ï¸ Step 4: ç« èŠ‚å¹¶è¡Œåˆ†æ®µ (Two-Pass)")
    logger.info("=" * 60)
    
    # è¯»å–å°è¯´å†…å®¹
    with open(novel_path, 'r', encoding='utf-8') as f:
        novel_content = f.read()
    
    segmentation_results = {}
    
    if workflow_config.enable_parallel:
        # å¹¶è¡Œå¤„ç†
        logger.info(f"ğŸ”€ å¹¶è¡Œå¤„ç†æ¨¡å¼: å¹¶å‘æ•°={workflow_config.max_concurrent_chapters}")
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(chapters), workflow_config.max_concurrent_chapters):
            batch = chapters[i:i + workflow_config.max_concurrent_chapters]
            batch_results = await _process_segmentation_batch(
                workflow, batch, novel_content, workflow_config, processing_dir
            )
            segmentation_results.update(batch_results)
    else:
        # ä¸²è¡Œå¤„ç†
        logger.info("ğŸ“ ä¸²è¡Œå¤„ç†æ¨¡å¼")
        for chapter in chapters:
            try:
                result = await _segment_single_chapter(
                    workflow, chapter, novel_content, workflow_config
                )
                segmentation_results[chapter.number] = result
                workflow._save_intermediate_result(
                    result,
                    f"step4_segmentation/chapter_{chapter.number:03d}",
                    processing_dir
                )
            except Exception as e:
                logger.error(f"âŒ ç« èŠ‚{chapter.number}åˆ†æ®µå¤±è´¥: {e}")
                if not workflow_config.continue_on_error:
                    raise
    
    logger.info(f"âœ… åˆ†æ®µå®Œæˆ: {len(segmentation_results)}/{len(chapters)} ç« èŠ‚")
    return segmentation_results

async def _process_segmentation_batch(
    workflow: NovelProcessingWorkflowBase,
    batch: List[ChapterInfo],
    novel_content: str,
    workflow_config: NovelProcessingConfig,
    processing_dir: str
) -> Dict[int, ParagraphSegmentationResult]:
    """å¹¶è¡Œå¤„ç†ä¸€æ‰¹ç« èŠ‚çš„åˆ†æ®µ"""
    tasks = [
        _segment_single_chapter(workflow, chapter, novel_content, workflow_config)
        for chapter in batch
    ]
    
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    batch_results = {}
    for chapter, result in zip(batch, results):
        if isinstance(result, Exception):
            logger.error(f"âŒ ç« èŠ‚{chapter.number}åˆ†æ®µå¤±è´¥: {result}")
            if not workflow_config.continue_on_error:
                raise result
        else:
            batch_results[chapter.number] = result
            workflow._save_intermediate_result(
                result,
                f"step4_segmentation/chapter_{chapter.number:03d}",
                processing_dir
            )
    
    return batch_results

async def _segment_single_chapter(
    workflow: NovelProcessingWorkflowBase,
    chapter: ChapterInfo,
    novel_content: str,
    workflow_config: NovelProcessingConfig
) -> ParagraphSegmentationResult:
    """åˆ†æ®µå•ä¸ªç« èŠ‚ï¼ˆä½¿ç”¨LLMç®¡ç†å™¨ï¼‰"""
    logger.info(f"   å¤„ç†ç« èŠ‚ {chapter.number}: {chapter.title}")
    
    # æå–ç« èŠ‚å†…å®¹
    lines = novel_content.split('\n')
    end_line = chapter.end_line if chapter.end_line is not None else len(lines)
    chapter_content = '\n'.join(lines[chapter.start_line:end_line])
    
    # ä½¿ç”¨LLMç®¡ç†å™¨è°ƒç”¨ï¼ˆè‡ªåŠ¨é™æµ+é‡è¯•ï¼‰
    seg_output = await workflow.llm_manager.call_with_rate_limit(
        func=workflow.novel_segmenter.execute,
        provider=workflow_config.segmentation_provider,
        model="claude-sonnet-4-5-20250929" if workflow_config.segmentation_provider == "claude" else "deepseek-chat",
        estimated_tokens=workflow._estimate_tokens(chapter_content),
        chapter_content=chapter_content,
        chapter_number=chapter.number,
        chapter_title=chapter.title
    )
    
    workflow.llm_calls_count += 2  # Two-Pass
    
    # æå– json_resultï¼ˆParagraphSegmentationResultï¼‰
    result = seg_output.json_result
    logger.info(f"   âœ… ç« èŠ‚{chapter.number}: {len(result.paragraphs)}ä¸ªæ®µè½")
    
    return result
