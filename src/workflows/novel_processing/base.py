import logging
import time
import json
from pathlib import Path
from typing import Optional, Dict, Any, List

from src.core.interfaces import BaseWorkflow
from src.core.schemas_novel import (
    NovelProcessingConfig,
    NovelProcessingResult,
    ChapterProcessingError
)
from src.core.llm_rate_limiter import get_llm_manager
from src.workflows import report_generator

# Tools
from src.tools.novel_importer import NovelImporter
from src.tools.novel_metadata_extractor import NovelMetadataExtractor
from src.tools.novel_chapter_detector import NovelChapterDetector
from src.tools.novel_segmenter import NovelSegmenter
from src.tools.novel_annotator import NovelAnnotator
from src.tools.novel_system_analyzer import NovelSystemAnalyzer
from src.tools.novel_system_detector import NovelSystemDetector
from src.tools.novel_system_tracker import NovelSystemTracker
from src.tools.novel_validator import NovelValidator

logger = logging.getLogger(__name__)

class NovelProcessingWorkflowBase(BaseWorkflow):
    """
    å°è¯´å¤„ç†å·¥ä½œæµåŸºç±»
    åŒ…å«åˆå§‹åŒ–ã€çŠ¶æ€ç®¡ç†å’Œè¾…åŠ©æ–¹æ³•
    """
    
    name: str = "novel_processing_workflow"
    
    def __init__(self):
        super().__init__()
        
        # åˆå§‹åŒ–å·¥å…·
        self.novel_importer = NovelImporter()
        self.metadata_extractor = NovelMetadataExtractor()
        self.chapter_detector = NovelChapterDetector()
        self.novel_segmenter = NovelSegmenter()
        self.novel_annotator = NovelAnnotator()
        self.system_analyzer = NovelSystemAnalyzer()
        self.system_detector = NovelSystemDetector()
        self.system_tracker = NovelSystemTracker()
        self.novel_validator = NovelValidator()
        
        # åˆå§‹åŒ–LLMè°ƒç”¨ç®¡ç†å™¨
        self.llm_manager = get_llm_manager()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.llm_calls_count = 0
        self.total_cost = 0.0
        self.start_time = 0.0
        
        logger.info(f"âœ… {self.name} åˆå§‹åŒ–å®Œæˆ")

    def _estimate_tokens(self, text: str) -> int:
        """
        ä¼°ç®—æ–‡æœ¬çš„tokenæ•°é‡
        ä¸­æ–‡: 1å­— â‰ˆ 1.5 tokens
        è‹±æ–‡: 1è¯ â‰ˆ 1.3 tokens
        è¾“å‡º: å‡è®¾ä¸ºè¾“å…¥çš„20%
        """
        chinese_chars = len([c for c in text if '\u4e00' <= c <= '\u9fff'])
        other_chars = len(text) - chinese_chars
        
        input_tokens = int(chinese_chars * 1.5 + other_chars * 0.3)
        output_tokens = int(input_tokens * 0.2)
        
        return input_tokens + output_tokens

    def _setup_processing_directory(self, project_name: str) -> str:
        """åˆ›å»ºå¤„ç†ç›®å½•"""
        base_dir = Path("data") / "projects" / project_name / "processing"
        base_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        (base_dir / "structured").mkdir(exist_ok=True)
        (base_dir / "structured" / "step4_segmentation").mkdir(exist_ok=True)
        (base_dir / "structured" / "step5_annotation").mkdir(exist_ok=True)
        (base_dir / "structured" / "step7_system_tracking").mkdir(exist_ok=True)
        (base_dir / "reports").mkdir(exist_ok=True)
        
        # åˆ›å»ºnovelæ–‡ä»¶å¤¹ï¼ˆå­˜æ”¾å¯è¯»Markdownï¼‰
        novel_dir = Path("data") / "projects" / project_name / "novel"
        novel_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºvisualizationæ–‡ä»¶å¤¹ï¼ˆå­˜æ”¾HTMLæŸ¥çœ‹å™¨ï¼‰
        viz_dir = Path("data") / "projects" / project_name / "visualization"
        viz_dir.mkdir(parents=True, exist_ok=True)
        
        return str(base_dir)

    def _save_intermediate_result(
        self,
        data: Any,
        filename: str,
        processing_dir: str
    ):
        """ä¿å­˜ä¸­é—´ç»“æœåˆ°structuredå­ç›®å½•"""
        # ç»Ÿä¸€ä¿å­˜åˆ°structuredå­ç›®å½•
        if not filename.startswith("structured/"):
            if filename.startswith("step"):
                filepath = Path(processing_dir) / "structured" / f"{filename}.json"
            else:
                filepath = Path(processing_dir) / f"{filename}.json"
        else:
            filepath = Path(processing_dir) / f"{filename}.json"
        
        filepath.parent.mkdir(parents=True, exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            if hasattr(data, 'model_dump'):
                json.dump(data.model_dump(), f, indent=2, ensure_ascii=False, default=str)
            else:
                json.dump(data, f, indent=2, ensure_ascii=False, default=str)

    def _save_final_result(self, result: NovelProcessingResult, processing_dir: str):
        """
        ä¿å­˜æœ€ç»ˆç»“æœï¼ˆä½¿ç”¨æ–‡ä»¶å¼•ç”¨ä»£æ›¿å†…å®¹åµŒå…¥ï¼‰
        """
        filepath = Path(processing_dir) / "final_result.json"
        
        # æ„å»ºè½»é‡çº§ç»“æœï¼ˆä½¿ç”¨æ–‡ä»¶å¼•ç”¨ï¼‰
        lightweight_result = {
            "project_name": result.project_name,
            "workflow_version": result.workflow_version,
            "created_at": result.created_at.isoformat() if result.created_at else None,
            "completed_steps": result.completed_steps,
            
            # å…ƒæ•°æ®ï¼ˆä½“ç§¯å°ï¼Œå†…åµŒï¼‰
            "import_result": result.import_result.model_dump() if result.import_result else None,
            "metadata": result.metadata.model_dump() if result.metadata else None,
            "chapters": [ch.model_dump() for ch in result.chapters],
            
            # å¤§ä½“ç§¯æ•°æ®ä½¿ç”¨å¼•ç”¨
            "segmentation_results": {
                chapter_num: f"structured/step4_segmentation/chapter_{chapter_num:03d}.json"
                for chapter_num in result.segmentation_results.keys()
            },
            "annotation_results": {
                chapter_num: f"structured/step5_annotation/chapter_{chapter_num:03d}.json"
                for chapter_num in result.annotation_results.keys()
            },
            "system_catalog": "structured/step6_system_catalog.json" if result.system_catalog else None,
            "system_updates": {
                chapter_num: f"structured/step7_system_tracking/chapter_{chapter_num:03d}.json"
                for chapter_num in result.system_updates.keys()
            },
            "system_tracking": {
                chapter_num: f"structured/step7_system_tracking/chapter_{chapter_num:03d}.json"
                for chapter_num in result.system_tracking.keys()
            },
            "validation_report": "structured/step8_validation_report.json" if result.validation_report else None,
            
            # ç»Ÿè®¡ä¿¡æ¯ï¼ˆå†…åµŒï¼‰
            "processing_stats": result.processing_stats,
            "processing_time": result.processing_time,
            "llm_calls_count": result.llm_calls_count,
            "total_cost": result.total_cost,
            "errors": [err.model_dump() for err in result.errors],
            
            # è¾…åŠ©ä¿¡æ¯
            "intermediate_results_dir": result.intermediate_results_dir,
            "novel_markdown_dir": f"data/projects/{result.project_name}/novel",
            "visualization_dir": f"data/projects/{result.project_name}/visualization"
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(lightweight_result, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"ğŸ’¾ æœ€ç»ˆç»“æœå·²ä¿å­˜ï¼ˆå¼•ç”¨æ–¹å¼ï¼‰: {filepath}")

    def _calculate_stats(self, result: NovelProcessingResult) -> Dict[str, Any]:
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "total_chapters": len(result.chapters),
            "successful_chapters": len(result.segmentation_results),
            "failed_chapters": len(result.chapters) - len(result.segmentation_results),
            "total_paragraphs": sum(
                len(seg.paragraphs) for seg in result.segmentation_results.values()
            ),
            "total_events": sum(
                len(ann.event_timeline.events) for ann in result.annotation_results.values()
            ),
            "total_settings": sum(
                len(ann.setting_library.settings) for ann in result.annotation_results.values()
            ),
            "avg_paragraphs_per_chapter": (
                sum(len(seg.paragraphs) for seg in result.segmentation_results.values()) /
                len(result.segmentation_results)
            ) if result.segmentation_results else 0
        }
