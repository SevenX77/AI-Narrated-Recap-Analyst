"""
Promptä¼˜åŒ–å™¨ (Prompt Optimizer)

ä½¿ç”¨LLMåˆ†ææ ‡æ³¨é”™è¯¯ï¼Œè‡ªåŠ¨ä¼˜åŒ–Promptï¼š
1. èšåˆé«˜Heaté”™è¯¯æ¡ˆä¾‹
2. LLMåˆ†æé”™è¯¯æ¨¡å¼
3. ç”Ÿæˆä¼˜åŒ–åçš„Prompt
4. ç‰ˆæœ¬ç®¡ç†
"""

import json
import logging
import yaml
from typing import List, Dict, Optional
from pathlib import Path
from datetime import datetime

from src.core.schemas import AlignmentAnnotation, PromptVersion
from src.utils.prompt_loader import load_prompts

logger = logging.getLogger(__name__)


class PromptOptimizer:
    """
    Promptä¼˜åŒ–å™¨
    
    å·¥ä½œæµç¨‹ï¼š
        1. ç­›é€‰é«˜Heaté”™è¯¯ï¼ˆHeat>60ï¼‰
        2. åˆ†æé”™è¯¯æ¨¡å¼
        3. è®©LLMä¼˜åŒ–Prompt
        4. ä¿å­˜æ–°ç‰ˆæœ¬
    """
    
    def __init__(
        self,
        llm_client,
        model_name: str = "deepseek-chat",
        prompt_dir: str = "src/prompts",
        version_dir: str = "data/alignment_optimization/prompts"
    ):
        """
        åˆå§‹åŒ–Promptä¼˜åŒ–å™¨
        
        Args:
            llm_client: LLMå®¢æˆ·ç«¯
            model_name: æ¨¡å‹åç§°
            prompt_dir: å½“å‰Promptç›®å½•
            version_dir: Promptç‰ˆæœ¬å†å²ç›®å½•
        """
        self.llm_client = llm_client
        self.model_name = model_name
        self.prompt_dir = Path(prompt_dir)
        self.version_dir = Path(version_dir)
        self.version_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info("âœ… PromptOptimizer åˆå§‹åŒ–å®Œæˆ")
    
    async def optimize_prompt(
        self,
        layer: str,
        annotations: List[AlignmentAnnotation],
        current_prompt_key: str,
        heat_threshold: float = 60.0
    ) -> PromptVersion:
        """
        ä¼˜åŒ–Prompt
        
        Args:
            layer: å±‚çº§åç§°
            annotations: æ ‡æ³¨æ•°æ®
            current_prompt_key: å½“å‰Promptçš„keyï¼ˆå¦‚"extract_world_building"ï¼‰
            heat_threshold: Heaté˜ˆå€¼
        
        Returns:
            æ–°Promptç‰ˆæœ¬
        """
        logger.info(f"ğŸ”§ ä¼˜åŒ–Prompt: {layer} (prompt_key={current_prompt_key})")
        
        # 1. ç­›é€‰é«˜Heaté”™è¯¯
        high_heat_errors = [a for a in annotations if a.heat_score >= heat_threshold]
        
        if not high_heat_errors:
            logger.warning(f"   æ— é«˜Heaté”™è¯¯ï¼ˆ>={heat_threshold}ï¼‰ï¼Œæ— éœ€ä¼˜åŒ–")
            return None
        
        logger.info(f"   é«˜Heaté”™è¯¯: {len(high_heat_errors)}ä¸ª")
        
        # 2. åˆ†æé”™è¯¯æ¨¡å¼
        error_patterns = self._analyze_error_patterns(high_heat_errors)
        
        # 3. åŠ è½½å½“å‰Prompt
        current_prompt = self._load_current_prompt(current_prompt_key)
        
        # 4. LLMä¼˜åŒ–
        optimized_prompt = await self._llm_optimize(
            layer=layer,
            current_prompt=current_prompt,
            error_patterns=error_patterns,
            high_heat_errors=high_heat_errors
        )
        
        # 5. ç”Ÿæˆç‰ˆæœ¬ä¿¡æ¯
        # è·å–å½“å‰ç‰ˆæœ¬å·
        existing_versions = self._get_existing_versions(layer)
        if existing_versions:
            last_version = existing_versions[-1]
            version_num = float(last_version.split('v')[1]) + 0.1
        else:
            version_num = 1.1
        
        new_version = f"v{version_num:.1f}"
        
        # 6. åˆ›å»ºPromptVersion
        prompt_version = PromptVersion(
            version=new_version,
            layer=layer,
            parent_version=existing_versions[-1] if existing_versions else "v1.0",
            prompt_content=optimized_prompt,
            change_summary=error_patterns["summary"],
            optimized_for=list(error_patterns["error_types"].keys()),
            heat_addressed=[a.heat_score for a in high_heat_errors]
        )
        
        # 7. ä¿å­˜ç‰ˆæœ¬
        self._save_prompt_version(prompt_version)
        
        logger.info(f"âœ… Promptä¼˜åŒ–å®Œæˆ: {new_version}")
        logger.info(f"   é’ˆå¯¹é”™è¯¯: {', '.join(prompt_version.optimized_for)}")
        logger.info(f"   è§£å†³Heat: {sum(prompt_version.heat_addressed):.1f}")
        
        return prompt_version
    
    def _analyze_error_patterns(
        self,
        annotations: List[AlignmentAnnotation]
    ) -> Dict:
        """åˆ†æé”™è¯¯æ¨¡å¼"""
        error_types = {}
        for ann in annotations:
            if ann.error_type:
                if ann.error_type not in error_types:
                    error_types[ann.error_type] = []
                error_types[ann.error_type].append({
                    "script": ann.script_content,
                    "novel": ann.novel_content,
                    "feedback": ann.human_feedback,
                    "heat": ann.heat_score
                })
        
        # ç”Ÿæˆæ‘˜è¦
        summary_parts = []
        for error_type, cases in error_types.items():
            summary_parts.append(f"{error_type}({len(cases)}ä¸ª)")
        summary = "ä¿®å¤" + "ã€".join(summary_parts)
        
        return {
            "error_types": error_types,
            "summary": summary,
            "total_errors": len(annotations),
            "total_heat": sum(a.heat_score for a in annotations)
        }
    
    def _load_current_prompt(self, prompt_key: str) -> str:
        """åŠ è½½å½“å‰Prompt"""
        prompts = load_prompts("layered_extraction")
        prompt_data = prompts.get(prompt_key, {})
        
        if not prompt_data:
            raise ValueError(f"æœªæ‰¾åˆ°Prompt: {prompt_key}")
        
        # ç»„åˆsystemå’Œuseréƒ¨åˆ†
        system = prompt_data.get("system", "")
        user = prompt_data.get("user", "")
        
        return f"ã€System Promptã€‘\n{system}\n\nã€User Promptã€‘\n{user}"
    
    async def _llm_optimize(
        self,
        layer: str,
        current_prompt: str,
        error_patterns: Dict,
        high_heat_errors: List[AlignmentAnnotation]
    ) -> str:
        """ä½¿ç”¨LLMä¼˜åŒ–Prompt"""
        # æ„å»ºé”™è¯¯æ¡ˆä¾‹æè¿°
        cases_desc = []
        for error_type, cases in error_patterns["error_types"].items():
            cases_desc.append(f"\nã€{error_type}ã€‘({len(cases)}ä¸ªæ¡ˆä¾‹)")
            for i, case in enumerate(cases[:3], 1):  # åªå±•ç¤ºå‰3ä¸ªæ¡ˆä¾‹
                cases_desc.append(f"\næ¡ˆä¾‹{i}:")
                cases_desc.append(f"  Script: {case['script']}")
                cases_desc.append(f"  Novel:  {case['novel']}")
                if case['feedback']:
                    cases_desc.append(f"  é—®é¢˜: {case['feedback']}")
                cases_desc.append(f"  Heat: {case['heat']:.1f}")
        
        optimization_prompt = f"""ä½ æ˜¯ä¸€ä¸ªPromptå·¥ç¨‹ä¸“å®¶ã€‚å½“å‰Promptåœ¨æå–{layer}å±‚ä¿¡æ¯æ—¶å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š

ã€é«˜Heaté”™è¯¯æ¡ˆä¾‹ã€‘ï¼ˆHeat>60ï¼Œæ€»è®¡{len(high_heat_errors)}ä¸ªï¼‰
{''.join(cases_desc)}

ã€å½“å‰Promptã€‘
{current_prompt}

ã€ä¼˜åŒ–è¦æ±‚ã€‘
1. åˆ†æé”™è¯¯åŸå› ï¼ˆä¸ºä»€ä¹ˆä¼šå‡ºç°è¿™äº›é—®é¢˜ï¼Ÿï¼‰
2. é’ˆå¯¹æ€§ä¼˜åŒ–Promptï¼š
   - å¢å¼ºã€æå–åŸåˆ™ã€‘éƒ¨åˆ†
   - å¢åŠ æ­£åä¾‹è¯´æ˜
   - å¼ºè°ƒå®¹æ˜“é—æ¼çš„å…³é”®ç‚¹
   - æ·»åŠ æ ‡å¿—è¯è¯†åˆ«
3. ä¿æŒåŸæœ‰æ­£ç¡®çš„éƒ¨åˆ†
4. è¿”å›å®Œæ•´çš„ä¼˜åŒ–åPromptï¼ˆåŒ…å«Systemå’ŒUseréƒ¨åˆ†ï¼‰

ã€è¾“å‡ºæ ¼å¼ã€‘
ç›´æ¥è¿”å›ä¼˜åŒ–åçš„å®Œæ•´Promptå†…å®¹ï¼Œä¿æŒYAMLæ ¼å¼ï¼ŒåŒ…å«ï¼š
- systeméƒ¨åˆ†ï¼ˆåŒ…å«ä¼˜åŒ–åçš„ã€æå–åŸåˆ™ã€‘ã€ã€ç¤ºä¾‹ã€‘ç­‰ï¼‰
- useréƒ¨åˆ†

ä¸è¦æœ‰ä»»ä½•é¢å¤–çš„è§£é‡Šæˆ–markdownä»£ç å—æ ‡è®°ã€‚"""
        
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªPromptä¼˜åŒ–ä¸“å®¶ï¼Œæ“…é•¿åˆ†æé”™è¯¯å¹¶æ”¹è¿›Promptè´¨é‡ã€‚"},
            {"role": "user", "content": optimization_prompt}
        ]
        
        try:
            response = self.llm_client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                temperature=0.3
            )
            
            optimized_prompt = response.choices[0].message.content
            
            logger.info("   LLMä¼˜åŒ–å®Œæˆ")
            logger.debug(f"   æ–°Prompté•¿åº¦: {len(optimized_prompt)}å­—ç¬¦")
            
            return optimized_prompt
            
        except Exception as e:
            logger.error(f"âŒ LLMä¼˜åŒ–å¤±è´¥: {e}")
            raise
    
    def _get_existing_versions(self, layer: str) -> List[str]:
        """è·å–å·²æœ‰ç‰ˆæœ¬åˆ—è¡¨"""
        layer_dir = self.version_dir / layer
        if not layer_dir.exists():
            return []
        
        versions = []
        for file in layer_dir.glob("v*.yaml"):
            versions.append(file.stem)
        
        return sorted(versions)
    
    def _save_prompt_version(self, prompt_version: PromptVersion):
        """ä¿å­˜Promptç‰ˆæœ¬"""
        layer_dir = self.version_dir / prompt_version.layer
        layer_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜Promptå†…å®¹
        prompt_file = layer_dir / f"{prompt_version.version}.yaml"
        with open(prompt_file, 'w', encoding='utf-8') as f:
            f.write(prompt_version.prompt_content)
        
        # ä¿å­˜å…ƒæ•°æ®
        meta_file = layer_dir / f"{prompt_version.version}_meta.json"
        with open(meta_file, 'w', encoding='utf-8') as f:
            json.dump(prompt_version.dict(exclude={"prompt_content"}), f, ensure_ascii=False, indent=2, default=str)
        
        # æ›´æ–°metrics.json
        self._update_metrics(prompt_version)
        
        logger.info(f"   å·²ä¿å­˜: {prompt_file}")
    
    def _update_metrics(self, prompt_version: PromptVersion):
        """æ›´æ–°metricsæ–‡ä»¶"""
        layer_dir = self.version_dir / prompt_version.layer
        metrics_file = layer_dir / "metrics.json"
        
        # åŠ è½½ç°æœ‰metrics
        if metrics_file.exists():
            with open(metrics_file, 'r', encoding='utf-8') as f:
                metrics = json.load(f)
        else:
            metrics = {"versions": []}
        
        # æ·»åŠ æ–°ç‰ˆæœ¬metrics
        metrics["versions"].append({
            "version": prompt_version.version,
            "created_at": str(prompt_version.created_at),
            "change_summary": prompt_version.change_summary,
            "optimized_for": prompt_version.optimized_for,
            "heat_addressed": round(sum(prompt_version.heat_addressed), 2),
            "metrics": prompt_version.metrics or {}
        })
        
        # ä¿å­˜
        with open(metrics_file, 'w', encoding='utf-8') as f:
            json.dump(metrics, f, ensure_ascii=False, indent=2)
