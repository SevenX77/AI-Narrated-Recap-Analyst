"""
LLMè°ƒç”¨é™æµç®¡ç†å™¨

æä¾›ç»Ÿä¸€çš„LLMè°ƒç”¨é™æµã€é‡è¯•ã€å¹¶å‘æ§åˆ¶æœºåˆ¶ã€‚
æ”¯æŒä¸åŒæ¨¡å‹çš„ä¸åŒé™æµè§„åˆ™ã€‚
"""

import asyncio
import time
import logging
from typing import Dict, Optional, Any, Callable
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from collections import deque
import json
from pathlib import Path

logger = logging.getLogger(__name__)


@dataclass
class LLMRateLimitConfig:
    """
    LLMæä¾›å•†çš„é™æµé…ç½®
    
    Attributes:
        provider: æä¾›å•†åç§°ï¼ˆå¦‚ 'anthropic', 'deepseek', 'openai'ï¼‰
        model: æ¨¡å‹åç§°ï¼ˆå¦‚ 'claude-3-5-sonnet-20241022'ï¼‰
        
        # é™æµè§„åˆ™
        requests_per_minute: æ¯åˆ†é’Ÿæœ€å¤§è¯·æ±‚æ•°ï¼ˆQPMï¼‰
        requests_per_day: æ¯å¤©æœ€å¤§è¯·æ±‚æ•°ï¼ˆQPDï¼‰
        tokens_per_minute: æ¯åˆ†é’Ÿæœ€å¤§tokenæ•°ï¼ˆTPMï¼‰
        tokens_per_day: æ¯å¤©æœ€å¤§tokenæ•°ï¼ˆTPDï¼‰
        
        # å¹¶å‘æ§åˆ¶
        max_concurrent: æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
        
        # é‡è¯•ç­–ç•¥
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        base_retry_delay: åŸºç¡€é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
        max_retry_delay: æœ€å¤§é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
        
        # é”™è¯¯ç è¯†åˆ«
        rate_limit_errors: è¯†åˆ«ä¸ºé™æµçš„é”™è¯¯ç åˆ—è¡¨
    """
    provider: str
    model: str
    
    # é™æµè§„åˆ™ï¼ˆNoneè¡¨ç¤ºä¸é™åˆ¶ï¼‰
    requests_per_minute: Optional[int] = None
    requests_per_day: Optional[int] = None
    tokens_per_minute: Optional[int] = None
    tokens_per_day: Optional[int] = None
    
    # å¹¶å‘æ§åˆ¶
    max_concurrent: int = 1
    
    # é‡è¯•ç­–ç•¥
    max_retries: int = 3
    base_retry_delay: float = 2.0
    max_retry_delay: float = 60.0
    
    # é”™è¯¯ç è¯†åˆ«
    rate_limit_errors: list = field(default_factory=lambda: ["403", "429", "rate limit", "too many requests"])
    
    # æµ‹è¯•çŠ¶æ€
    is_tested: bool = False
    last_test_date: Optional[str] = None
    test_notes: str = ""


# é¢„å®šä¹‰çš„LLMé™æµé…ç½®
DEFAULT_LLM_CONFIGS = {
    "anthropic_claude": LLMRateLimitConfig(
        provider="anthropic",
        model="claude-3-5-sonnet-20241022",
        requests_per_minute=50,  # éœ€è¦æµ‹è¯•éªŒè¯
        requests_per_day=None,
        tokens_per_minute=40000,  # éœ€è¦æµ‹è¯•éªŒè¯
        max_concurrent=3,
        max_retries=3,
        base_retry_delay=2.0,
        is_tested=False,
        test_notes="é»˜è®¤é…ç½®ï¼Œå¾…æµ‹è¯•éªŒè¯"
    ),
    
    "deepseek_chat": LLMRateLimitConfig(
        provider="deepseek",
        model="deepseek-chat",
        requests_per_minute=60,  # éœ€è¦æµ‹è¯•éªŒè¯
        requests_per_day=None,
        tokens_per_minute=None,
        max_concurrent=2,
        max_retries=3,
        base_retry_delay=3.0,
        is_tested=False,
        test_notes="é»˜è®¤é…ç½®ï¼Œå¾…æµ‹è¯•éªŒè¯"
    ),
    
    "openai_gpt4": LLMRateLimitConfig(
        provider="openai",
        model="gpt-4",
        requests_per_minute=500,  # ä»˜è´¹è´¦æˆ·
        requests_per_day=10000,
        tokens_per_minute=10000,
        max_concurrent=5,
        max_retries=3,
        base_retry_delay=1.0,
        is_tested=False,
        test_notes="ä»˜è´¹è´¦æˆ·é…ç½®"
    ),
    
    # ä¿å®ˆé…ç½®ï¼ˆç”¨äºæœªæµ‹è¯•çš„æ¨¡å‹ï¼‰
    "conservative": LLMRateLimitConfig(
        provider="unknown",
        model="unknown",
        requests_per_minute=10,
        requests_per_day=None,
        tokens_per_minute=None,
        max_concurrent=1,
        max_retries=5,
        base_retry_delay=5.0,
        is_tested=False,
        test_notes="ä¿å®ˆé…ç½®ï¼Œç”¨äºæœªçŸ¥æ¨¡å‹"
    )
}


class RateLimiter:
    """
    é™æµå™¨ï¼ˆæ»‘åŠ¨çª—å£ç®—æ³•ï¼‰
    
    è·Ÿè¸ªæ—¶é—´çª—å£å†…çš„è¯·æ±‚æ•°å’Œtokenæ•°ï¼Œç¡®ä¿ä¸è¶…è¿‡é™åˆ¶ã€‚
    """
    
    def __init__(self, config: LLMRateLimitConfig):
        self.config = config
        
        # è¯·æ±‚æ—¶é—´æˆ³é˜Ÿåˆ—ï¼ˆåˆ†é’Ÿçº§ï¼‰
        self.minute_requests: deque = deque()
        
        # è¯·æ±‚æ—¶é—´æˆ³é˜Ÿåˆ—ï¼ˆå¤©çº§ï¼‰
        self.day_requests: deque = deque()
        
        # Tokenä½¿ç”¨è®°å½•
        self.minute_tokens: deque = deque()
        self.day_tokens: deque = deque()
        
        # å¹¶å‘æ§åˆ¶
        self.current_concurrent = 0
        self.concurrent_lock = asyncio.Lock()
    
    async def acquire(self, estimated_tokens: int = 1000) -> bool:
        """
        è¯·æ±‚è·å–æ‰§è¡Œæƒé™
        
        Args:
            estimated_tokens: é¢„ä¼°çš„tokenä½¿ç”¨é‡
        
        Returns:
            æ˜¯å¦è·å¾—æ‰§è¡Œæƒé™
        """
        now = time.time()
        
        # æ¸…ç†è¿‡æœŸè®°å½•
        self._cleanup_old_records(now)
        
        # æ£€æŸ¥å¹¶å‘é™åˆ¶
        if self.current_concurrent >= self.config.max_concurrent:
            logger.debug(f"å¹¶å‘æ•°è¾¾åˆ°ä¸Šé™: {self.current_concurrent}/{self.config.max_concurrent}")
            return False
        
        # æ£€æŸ¥QPMé™åˆ¶
        if self.config.requests_per_minute:
            if len(self.minute_requests) >= self.config.requests_per_minute:
                logger.debug(f"QPMé™åˆ¶: {len(self.minute_requests)}/{self.config.requests_per_minute}")
                return False
        
        # æ£€æŸ¥QPDé™åˆ¶
        if self.config.requests_per_day:
            if len(self.day_requests) >= self.config.requests_per_day:
                logger.warning(f"QPDé™åˆ¶: {len(self.day_requests)}/{self.config.requests_per_day}")
                return False
        
        # æ£€æŸ¥TPMé™åˆ¶
        if self.config.tokens_per_minute:
            current_minute_tokens = sum(tokens for _, tokens in self.minute_tokens)
            if current_minute_tokens + estimated_tokens > self.config.tokens_per_minute:
                logger.debug(f"TPMé™åˆ¶: {current_minute_tokens + estimated_tokens}/{self.config.tokens_per_minute}")
                return False
        
        # æ£€æŸ¥TPDé™åˆ¶
        if self.config.tokens_per_day:
            current_day_tokens = sum(tokens for _, tokens in self.day_tokens)
            if current_day_tokens + estimated_tokens > self.config.tokens_per_day:
                logger.warning(f"TPDé™åˆ¶: {current_day_tokens + estimated_tokens}/{self.config.tokens_per_day}")
                return False
        
        # è®°å½•è¯·æ±‚
        self.minute_requests.append(now)
        self.day_requests.append(now)
        self.minute_tokens.append((now, estimated_tokens))
        self.day_tokens.append((now, estimated_tokens))
        
        # å¢åŠ å¹¶å‘è®¡æ•°
        async with self.concurrent_lock:
            self.current_concurrent += 1
        
        return True
    
    async def release(self):
        """é‡Šæ”¾æ‰§è¡Œæƒé™"""
        async with self.concurrent_lock:
            self.current_concurrent = max(0, self.current_concurrent - 1)
    
    def _cleanup_old_records(self, now: float):
        """æ¸…ç†è¿‡æœŸçš„è®°å½•"""
        # æ¸…ç†åˆ†é’Ÿçº§è®°å½•ï¼ˆä¿ç•™60ç§’å†…ï¼‰
        minute_ago = now - 60
        while self.minute_requests and self.minute_requests[0] < minute_ago:
            self.minute_requests.popleft()
        while self.minute_tokens and self.minute_tokens[0][0] < minute_ago:
            self.minute_tokens.popleft()
        
        # æ¸…ç†å¤©çº§è®°å½•ï¼ˆä¿ç•™24å°æ—¶å†…ï¼‰
        day_ago = now - 86400
        while self.day_requests and self.day_requests[0] < day_ago:
            self.day_requests.popleft()
        while self.day_tokens and self.day_tokens[0][0] < day_ago:
            self.day_tokens.popleft()
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–å½“å‰ç»Ÿè®¡ä¿¡æ¯"""
        now = time.time()
        self._cleanup_old_records(now)
        
        return {
            "current_concurrent": self.current_concurrent,
            "requests_last_minute": len(self.minute_requests),
            "requests_last_day": len(self.day_requests),
            "tokens_last_minute": sum(tokens for _, tokens in self.minute_tokens),
            "tokens_last_day": sum(tokens for _, tokens in self.day_tokens),
        }


class LLMCallManager:
    """
    LLMè°ƒç”¨ç®¡ç†å™¨
    
    ç»Ÿä¸€ç®¡ç†æ‰€æœ‰LLMè°ƒç”¨ï¼Œæä¾›é™æµã€é‡è¯•ã€å¹¶å‘æ§åˆ¶ã€‚
    """
    
    def __init__(self, config_file: Optional[str] = None):
        """
        åˆå§‹åŒ–ç®¡ç†å™¨
        
        Args:
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        """
        self.config_file = config_file or "config/llm_configs.json"
        self.configs: Dict[str, LLMRateLimitConfig] = {}
        self.limiters: Dict[str, RateLimiter] = {}
        
        # åŠ è½½é…ç½®
        self._load_configs()
        
        # åˆå§‹åŒ–é™æµå™¨
        for key, config in self.configs.items():
            self.limiters[key] = RateLimiter(config)
        
        logger.info(f"âœ… LLMCallManageråˆå§‹åŒ–å®Œæˆï¼ŒåŠ è½½{len(self.configs)}ä¸ªé…ç½®")
    
    def _load_configs(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        config_path = Path(self.config_file)
        
        if config_path.exists():
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                for key, config_dict in data.items():
                    self.configs[key] = LLMRateLimitConfig(**config_dict)
                
                logger.info(f"ğŸ“‚ ä»{config_path}åŠ è½½é…ç½®")
            except Exception as e:
                logger.warning(f"âš ï¸ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                self.configs = DEFAULT_LLM_CONFIGS.copy()
        else:
            logger.info("ğŸ“‚ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            self.configs = DEFAULT_LLM_CONFIGS.copy()
            self._save_configs()
    
    def _save_configs(self):
        """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
        config_path = Path(self.config_file)
        config_path.parent.mkdir(parents=True, exist_ok=True)
        
        data = {}
        for key, config in self.configs.items():
            data[key] = {
                "provider": config.provider,
                "model": config.model,
                "requests_per_minute": config.requests_per_minute,
                "requests_per_day": config.requests_per_day,
                "tokens_per_minute": config.tokens_per_minute,
                "tokens_per_day": config.tokens_per_day,
                "max_concurrent": config.max_concurrent,
                "max_retries": config.max_retries,
                "base_retry_delay": config.base_retry_delay,
                "max_retry_delay": config.max_retry_delay,
                "rate_limit_errors": config.rate_limit_errors,
                "is_tested": config.is_tested,
                "last_test_date": config.last_test_date,
                "test_notes": config.test_notes
            }
        
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ’¾ é…ç½®å·²ä¿å­˜åˆ°{config_path}")
    
    def get_config(self, provider: str, model: str) -> LLMRateLimitConfig:
        """
        è·å–æŒ‡å®šæ¨¡å‹çš„é…ç½®
        
        Args:
            provider: æä¾›å•†åç§°
            model: æ¨¡å‹åç§°
        
        Returns:
            é™æµé…ç½®
        """
        # å°è¯•ç²¾ç¡®åŒ¹é…
        key = f"{provider}_{model}".replace("-", "_").replace(".", "_")
        if key in self.configs:
            return self.configs[key]
        
        # å°è¯•æä¾›å•†åŒ¹é…
        for config_key, config in self.configs.items():
            if config.provider == provider:
                logger.info(f"ä½¿ç”¨{provider}çš„é€šç”¨é…ç½®")
                return config
        
        # ä½¿ç”¨ä¿å®ˆé…ç½®
        logger.warning(f"âš ï¸ æœªæ‰¾åˆ°{provider}/{model}çš„é…ç½®ï¼Œä½¿ç”¨ä¿å®ˆé…ç½®")
        return self.configs.get("conservative", DEFAULT_LLM_CONFIGS["conservative"])
    
    async def call_with_rate_limit(
        self,
        func: Callable,
        provider: str,
        model: str,
        estimated_tokens: int = 1000,
        *args,
        **kwargs
    ) -> Any:
        """
        å¸¦é™æµæ§åˆ¶çš„LLMè°ƒç”¨
        
        Args:
            func: è¦è°ƒç”¨çš„å‡½æ•°
            provider: æä¾›å•†åç§°
            model: æ¨¡å‹åç§°
            estimated_tokens: é¢„ä¼°tokenä½¿ç”¨é‡
            *args, **kwargs: ä¼ é€’ç»™funcçš„å‚æ•°
        
        Returns:
            å‡½æ•°æ‰§è¡Œç»“æœ
        """
        config = self.get_config(provider, model)
        limiter = self.limiters.get(
            f"{provider}_{model}".replace("-", "_").replace(".", "_"),
            RateLimiter(config)
        )
        
        # ç­‰å¾…è·å–æ‰§è¡Œæƒé™
        while not await limiter.acquire(estimated_tokens):
            wait_time = min(5.0, config.base_retry_delay)
            logger.debug(f"â³ ç­‰å¾…é™æµé‡Šæ”¾...{wait_time}ç§’")
            await asyncio.sleep(wait_time)
        
        try:
            # æ‰§è¡Œå‡½æ•°ï¼ˆå¸¦é‡è¯•ï¼‰
            result = await self._execute_with_retry(
                func, config, *args, **kwargs
            )
            return result
        
        finally:
            # é‡Šæ”¾æ‰§è¡Œæƒé™
            await limiter.release()
    
    async def _execute_with_retry(
        self,
        func: Callable,
        config: LLMRateLimitConfig,
        *args,
        **kwargs
    ) -> Any:
        """
        å¸¦é‡è¯•çš„æ‰§è¡Œ
        
        Args:
            func: è¦æ‰§è¡Œçš„å‡½æ•°
            config: é™æµé…ç½®
            *args, **kwargs: ä¼ é€’ç»™funcçš„å‚æ•°
        
        Returns:
            å‡½æ•°æ‰§è¡Œç»“æœ
        """
        last_exception = None
        
        for attempt in range(config.max_retries + 1):
            try:
                # æ‰§è¡Œå‡½æ•°
                result = func(*args, **kwargs)
                return result
            
            except Exception as e:
                last_exception = e
                error_msg = str(e)
                
                # æ£€æµ‹æ˜¯å¦æ˜¯é™æµé”™è¯¯
                is_rate_limit = any(
                    err_code in error_msg
                    for err_code in config.rate_limit_errors
                )
                
                if attempt < config.max_retries:
                    # è®¡ç®—å»¶è¿Ÿï¼ˆæŒ‡æ•°é€€é¿ï¼‰
                    delay = min(
                        config.base_retry_delay * (2 ** attempt),
                        config.max_retry_delay
                    )
                    
                    # å¦‚æœæ˜¯é™æµé”™è¯¯ï¼Œå¢åŠ é¢å¤–å»¶è¿Ÿ
                    if is_rate_limit:
                        delay *= 2
                        logger.warning(f"ğŸš« æ£€æµ‹åˆ°APIé™æµ")
                    
                    logger.warning(
                        f"âš ï¸ æ‰§è¡Œå¤±è´¥ï¼ˆç¬¬{attempt + 1}/{config.max_retries + 1}æ¬¡å°è¯•ï¼‰: {error_msg[:100]}"
                    )
                    logger.info(f"â³ ç­‰å¾…{delay:.1f}ç§’åé‡è¯•...")
                    
                    await asyncio.sleep(delay)
                else:
                    logger.error(f"âŒ é‡è¯•{config.max_retries}æ¬¡åä»ç„¶å¤±è´¥")
        
        raise last_exception
    
    def get_all_stats(self) -> Dict[str, Dict[str, Any]]:
        """è·å–æ‰€æœ‰é™æµå™¨çš„ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        for key, limiter in self.limiters.items():
            stats[key] = limiter.get_stats()
        return stats
    
    def update_config(self, key: str, **kwargs):
        """
        æ›´æ–°é…ç½®
        
        Args:
            key: é…ç½®é”®
            **kwargs: è¦æ›´æ–°çš„é…ç½®é¡¹
        """
        if key in self.configs:
            config = self.configs[key]
            for k, v in kwargs.items():
                if hasattr(config, k):
                    setattr(config, k, v)
            
            self._save_configs()
            logger.info(f"âœ… é…ç½®{key}å·²æ›´æ–°")
        else:
            logger.warning(f"âš ï¸ é…ç½®{key}ä¸å­˜åœ¨")


# å…¨å±€å•ä¾‹
_global_manager: Optional[LLMCallManager] = None


def get_llm_manager() -> LLMCallManager:
    """è·å–å…¨å±€LLMè°ƒç”¨ç®¡ç†å™¨"""
    global _global_manager
    if _global_manager is None:
        _global_manager = LLMCallManager()
    return _global_manager
