"""
NovelValidator - å°è¯´å¤„ç†è´¨é‡éªŒè¯å·¥å…·

éªŒè¯å°è¯´å¤„ç†çš„å„ä¸ªç¯èŠ‚è´¨é‡ï¼Œç”Ÿæˆè´¨é‡æŠ¥å‘Šã€‚
"""

import logging
from typing import List, Dict, Any
from datetime import datetime

from src.core.interfaces import BaseTool
from src.core.schemas_novel import (
    NovelImportResult,
    ChapterInfo,
    ParagraphSegmentationResult,
    AnnotatedChapter,
    NovelValidationReport,
    ValidationIssue
)

logger = logging.getLogger(__name__)


class NovelValidator(BaseTool):
    """
    å°è¯´å¤„ç†è´¨é‡éªŒè¯å·¥å…·
    
    èŒè´£ (Responsibility):
        éªŒè¯å°è¯´å¤„ç†çš„å„ä¸ªç¯èŠ‚è´¨é‡ï¼Œç¡®ä¿æ•°æ®å‡†ç¡®æ€§å’Œåˆç†æ€§ã€‚
    
    æ£€æŸ¥é¡¹:
        1. ç¼–ç æ­£ç¡®æ€§: æ£€æµ‹ä¹±ç å­—ç¬¦
        2. ç« èŠ‚å®Œæ•´æ€§: éªŒè¯ç« èŠ‚è¿ç»­æ€§
        3. åˆ†æ®µåˆç†æ€§: ABCç±»åˆ†å¸ƒã€è¿‡åº¦åˆ†æ®µ
        4. æ ‡æ³¨åˆç†æ€§: äº‹ä»¶æ•°é‡ã€è®¾å®šæ•°é‡
    
    æ¥å£ (Interface):
        è¾“å…¥:
            - import_result: NovelImportResult
            - chapter_infos: List[ChapterInfo]
            - segmentation_results: List[ParagraphSegmentationResult]
            - annotation_results: List[AnnotatedChapter]
        
        è¾“å‡º:
            - NovelValidationReport: éªŒè¯æŠ¥å‘Š
    """
    
    name = "novel_validator"
    description = "éªŒè¯å°è¯´å¤„ç†è´¨é‡"
    
    def __init__(self):
        """åˆå§‹åŒ–éªŒè¯å™¨"""
        super().__init__()
        self.quality_weights = {
            "encoding": 0.2,      # ç¼–ç æ­£ç¡®æ€§æƒé‡
            "chapter": 0.25,      # ç« èŠ‚å®Œæ•´æ€§æƒé‡
            "segmentation": 0.3,  # åˆ†æ®µåˆç†æ€§æƒé‡
            "annotation": 0.25    # æ ‡æ³¨åˆç†æ€§æƒé‡
        }
    
    def execute(
        self,
        import_result: NovelImportResult,
        chapter_infos: List[ChapterInfo],
        segmentation_results: List[ParagraphSegmentationResult] = None,
        annotation_results: List[AnnotatedChapter] = None,
        **kwargs
    ) -> NovelValidationReport:
        """
        æ‰§è¡Œå°è¯´è´¨é‡éªŒè¯
        
        Args:
            import_result: å°è¯´å¯¼å…¥ç»“æœ
            chapter_infos: ç« èŠ‚ä¿¡æ¯åˆ—è¡¨
            segmentation_results: åˆ†æ®µç»“æœåˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            annotation_results: æ ‡æ³¨ç»“æœåˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            NovelValidationReport: éªŒè¯æŠ¥å‘Š
        """
        logger.info(f"ğŸ” å¼€å§‹éªŒè¯å°è¯´å¤„ç†è´¨é‡: {import_result.project_name}")
        
        issues: List[ValidationIssue] = []
        warnings: List[str] = []
        recommendations: List[str] = []
        
        # 1. ç¼–ç æ­£ç¡®æ€§æ£€æŸ¥
        encoding_check = self._check_encoding(import_result, issues, warnings)
        
        # 2. ç« èŠ‚å®Œæ•´æ€§æ£€æŸ¥
        chapter_check = self._check_chapters(chapter_infos, issues, warnings)
        
        # 3. åˆ†æ®µåˆç†æ€§æ£€æŸ¥
        segmentation_check = {}
        if segmentation_results:
            segmentation_check = self._check_segmentation(
                segmentation_results, issues, warnings, recommendations
            )
        
        # 4. æ ‡æ³¨åˆç†æ€§æ£€æŸ¥
        annotation_check = {}
        if annotation_results:
            annotation_check = self._check_annotation(
                annotation_results, issues, warnings, recommendations
            )
        
        # è®¡ç®—æ€»ä½“è´¨é‡è¯„åˆ†
        quality_score = self._calculate_quality_score(
            encoding_check,
            chapter_check,
            segmentation_check,
            annotation_check
        )
        
        # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        statistics = self._generate_statistics(
            import_result,
            chapter_infos,
            segmentation_results,
            annotation_results
        )
        
        # ç”ŸæˆæŠ¥å‘Š
        report = NovelValidationReport(
            project_name=import_result.project_name,
            validation_time=datetime.now(),
            quality_score=quality_score,
            is_valid=quality_score >= 70.0,
            encoding_check=encoding_check,
            chapter_check=chapter_check,
            segmentation_check=segmentation_check,
            annotation_check=annotation_check,
            issues=issues,
            warnings=warnings,
            recommendations=recommendations,
            statistics=statistics
        )
        
        logger.info(f"âœ… éªŒè¯å®Œæˆ: è´¨é‡è¯„åˆ† {quality_score:.1f}/100")
        logger.info(f"   é—®é¢˜æ•°: {len(issues)}, è­¦å‘Šæ•°: {len(warnings)}")
        
        return report
    
    def _check_encoding(
        self,
        import_result: NovelImportResult,
        issues: List[ValidationIssue],
        warnings: List[str]
    ) -> Dict[str, Any]:
        """æ£€æŸ¥ç¼–ç æ­£ç¡®æ€§"""
        logger.info("   æ£€æŸ¥ç¼–ç æ­£ç¡®æ€§...")
        
        # è¯»å–æ–‡ä»¶å†…å®¹æ£€æŸ¥ä¹±ç 
        invalid_chars = ['ï¿½', '\ufffd']
        invalid_count = 0
        
        try:
            with open(import_result.saved_path, 'r', encoding='utf-8') as f:
                content = f.read()
                for char in invalid_chars:
                    invalid_count += content.count(char)
            
            passed = invalid_count == 0
            
            if not passed:
                issues.append(ValidationIssue(
                    severity="error",
                    category="encoding",
                    description=f"æ£€æµ‹åˆ° {invalid_count} ä¸ªä¹±ç å­—ç¬¦",
                    location="novel_content",
                    recommendation="é‡æ–°å¯¼å…¥æ–‡ä»¶ï¼Œæ£€æŸ¥åŸå§‹æ–‡ä»¶ç¼–ç "
                ))
            
            return {
                "passed": passed,
                "invalid_chars_count": invalid_count,
                "encoding": import_result.encoding
            }
            
        except Exception as e:
            logger.error(f"ç¼–ç æ£€æŸ¥å¤±è´¥: {e}")
            issues.append(ValidationIssue(
                severity="error",
                category="encoding",
                description=f"æ— æ³•è¯»å–æ–‡ä»¶: {str(e)}",
                location="novel_file"
            ))
            return {"passed": False, "error": str(e)}
    
    def _check_chapters(
        self,
        chapter_infos: List[ChapterInfo],
        issues: List[ValidationIssue],
        warnings: List[str]
    ) -> Dict[str, Any]:
        """æ£€æŸ¥ç« èŠ‚å®Œæ•´æ€§"""
        logger.info("   æ£€æŸ¥ç« èŠ‚å®Œæ•´æ€§...")
        
        if not chapter_infos:
            issues.append(ValidationIssue(
                severity="error",
                category="chapter",
                description="æœªæ£€æµ‹åˆ°ä»»ä½•ç« èŠ‚",
                recommendation="æ£€æŸ¥ç« èŠ‚æ ‡é¢˜æ ¼å¼"
            ))
            return {"passed": False, "total_chapters": 0}
        
        # æ£€æŸ¥ç« èŠ‚è¿ç»­æ€§
        chapter_numbers = [ch.number for ch in chapter_infos]
        expected = list(range(1, len(chapter_infos) + 1))
        missing = set(expected) - set(chapter_numbers)
        duplicates = [num for num in chapter_numbers if chapter_numbers.count(num) > 1]
        
        passed = len(missing) == 0 and len(duplicates) == 0
        
        if missing:
            issues.append(ValidationIssue(
                severity="error",
                category="chapter",
                description=f"ç¼ºå¤±ç« èŠ‚: {sorted(missing)}",
                recommendation="æ£€æŸ¥ç« èŠ‚æ ‡é¢˜æ˜¯å¦å®Œæ•´"
            ))
        
        if duplicates:
            issues.append(ValidationIssue(
                severity="error",
                category="chapter",
                description=f"é‡å¤ç« èŠ‚: {sorted(set(duplicates))}",
                recommendation="æ£€æŸ¥ç« èŠ‚æ£€æµ‹é€»è¾‘"
            ))
        
        return {
            "passed": passed,
            "total_chapters": len(chapter_infos),
            "missing_chapters": sorted(missing),
            "duplicate_chapters": sorted(set(duplicates))
        }
    
    def _check_segmentation(
        self,
        segmentation_results: List[ParagraphSegmentationResult],
        issues: List[ValidationIssue],
        warnings: List[str],
        recommendations: List[str]
    ) -> Dict[str, Any]:
        """æ£€æŸ¥åˆ†æ®µåˆç†æ€§"""
        logger.info("   æ£€æŸ¥åˆ†æ®µåˆç†æ€§...")
        
        if not segmentation_results:
            return {"passed": True, "message": "æ— åˆ†æ®µç»“æœ"}
        
        # ç»Ÿè®¡ABCç±»åˆ†å¸ƒ
        total_paragraphs = 0
        type_counts = {"A": 0, "B": 0, "C": 0}
        paragraph_counts = []
        
        for seg in segmentation_results:
            total_paragraphs += len(seg.paragraphs)
            paragraph_counts.append(len(seg.paragraphs))
            
            for p in seg.paragraphs:
                type_counts[p.type] = type_counts.get(p.type, 0) + 1
        
        # è®¡ç®—åˆ†å¸ƒæ¯”ä¾‹
        type_ratios = {t: c / total_paragraphs for t, c in type_counts.items()}
        
        # æ£€æŸ¥ABCç±»åˆ†å¸ƒæ˜¯å¦åˆç†
        # æ­£å¸¸èŒƒå›´: A:10-30%, B:60-80%, C:0-10%
        distribution_ok = True
        
        if type_ratios.get("A", 0) < 0.10 or type_ratios.get("A", 0) > 0.30:
            warnings.append(f"Aç±»æ¯”ä¾‹å¼‚å¸¸: {type_ratios.get('A', 0):.1%} (æ­£å¸¸èŒƒå›´: 10%-30%)")
            distribution_ok = False
        
        if type_ratios.get("B", 0) < 0.60 or type_ratios.get("B", 0) > 0.80:
            warnings.append(f"Bç±»æ¯”ä¾‹å¼‚å¸¸: {type_ratios.get('B', 0):.1%} (æ­£å¸¸èŒƒå›´: 60%-80%)")
            distribution_ok = False
        
        if type_ratios.get("C", 0) > 0.10:
            warnings.append(f"Cç±»æ¯”ä¾‹è¿‡é«˜: {type_ratios.get('C', 0):.1%} (æ­£å¸¸èŒƒå›´: 0%-10%)")
            distribution_ok = False
        
        # æ£€æŸ¥è¿‡åº¦åˆ†æ®µ
        avg_paragraphs = sum(paragraph_counts) / len(paragraph_counts)
        max_paragraphs = max(paragraph_counts)
        
        if max_paragraphs > 50:
            chapter_idx = paragraph_counts.index(max_paragraphs) + 1
            issues.append(ValidationIssue(
                severity="warning",
                category="segmentation",
                description=f"ç¬¬{chapter_idx}ç« åˆ†æ®µæ•°é‡è¿‡å¤šï¼ˆ{max_paragraphs}æ®µï¼‰",
                location=f"chapter_{chapter_idx}",
                recommendation="æ£€æŸ¥åˆ†æ®µé€»è¾‘ï¼Œè€ƒè™‘åˆå¹¶ç›¸ä¼¼æ®µè½"
            ))
        
        # ç”Ÿæˆå»ºè®®
        if distribution_ok and avg_paragraphs >= 8 and avg_paragraphs <= 15:
            recommendations.append("åˆ†æ®µè´¨é‡ä¼˜ç§€ï¼Œå»ºè®®ä¿æŒå½“å‰ç­–ç•¥")
        elif not distribution_ok:
            recommendations.append("ABCç±»åˆ†å¸ƒä¸å‡ï¼Œå»ºè®®reviewåˆ†æ®µPrompt")
        
        return {
            "passed": distribution_ok,
            "total_paragraphs": total_paragraphs,
            "avg_paragraphs_per_chapter": avg_paragraphs,
            "max_paragraphs": max_paragraphs,
            "abc_distribution": type_ratios
        }
    
    def _check_annotation(
        self,
        annotation_results: List[AnnotatedChapter],
        issues: List[ValidationIssue],
        warnings: List[str],
        recommendations: List[str]
    ) -> Dict[str, Any]:
        """æ£€æŸ¥æ ‡æ³¨åˆç†æ€§"""
        logger.info("   æ£€æŸ¥æ ‡æ³¨åˆç†æ€§...")
        
        if not annotation_results:
            return {"passed": True, "message": "æ— æ ‡æ³¨ç»“æœ"}
        
        # ç»Ÿè®¡äº‹ä»¶å’Œè®¾å®šæ•°é‡
        total_events = sum(ann.event_timeline.total_events for ann in annotation_results)
        total_settings = sum(ann.setting_library.total_settings for ann in annotation_results)
        
        avg_events = total_events / len(annotation_results)
        avg_settings = total_settings / len(annotation_results)
        
        # æ£€æŸ¥æ˜¯å¦åˆç†ï¼ˆæ¯ç« åº”è¯¥æœ‰3-15ä¸ªäº‹ä»¶ï¼‰
        annotation_ok = True
        
        if avg_events < 3:
            warnings.append(f"å¹³å‡äº‹ä»¶æ•°é‡è¿‡å°‘: {avg_events:.1f}/ç«  (å»ºè®®>3)")
            annotation_ok = False
        elif avg_events > 15:
            warnings.append(f"å¹³å‡äº‹ä»¶æ•°é‡è¿‡å¤š: {avg_events:.1f}/ç«  (å»ºè®®<15)")
            annotation_ok = False
        
        if avg_settings < 1:
            warnings.append(f"å¹³å‡è®¾å®šæ•°é‡è¿‡å°‘: {avg_settings:.1f}/ç«  (å»ºè®®>1)")
        
        if annotation_ok:
            recommendations.append("æ ‡æ³¨è´¨é‡è‰¯å¥½ï¼Œäº‹ä»¶å’Œè®¾å®šæ•°é‡åˆç†")
        
        return {
            "passed": annotation_ok,
            "total_events": total_events,
            "total_settings": total_settings,
            "avg_events_per_chapter": avg_events,
            "avg_settings_per_chapter": avg_settings
        }
    
    def _calculate_quality_score(
        self,
        encoding_check: Dict,
        chapter_check: Dict,
        segmentation_check: Dict,
        annotation_check: Dict
    ) -> float:
        """è®¡ç®—æ€»ä½“è´¨é‡è¯„åˆ†"""
        scores = {}
        
        # ç¼–ç è¯„åˆ†
        scores["encoding"] = 100.0 if encoding_check.get("passed") else 0.0
        
        # ç« èŠ‚è¯„åˆ†
        scores["chapter"] = 100.0 if chapter_check.get("passed") else 50.0
        
        # åˆ†æ®µè¯„åˆ†
        if segmentation_check:
            scores["segmentation"] = 100.0 if segmentation_check.get("passed") else 70.0
        else:
            scores["segmentation"] = 100.0  # æ— åˆ†æ®µç»“æœï¼Œä¸æ‰£åˆ†
        
        # æ ‡æ³¨è¯„åˆ†
        if annotation_check:
            scores["annotation"] = 100.0 if annotation_check.get("passed") else 70.0
        else:
            scores["annotation"] = 100.0  # æ— æ ‡æ³¨ç»“æœï¼Œä¸æ‰£åˆ†
        
        # åŠ æƒå¹³å‡
        total_score = sum(
            scores[key] * weight 
            for key, weight in self.quality_weights.items()
        )
        
        return round(total_score, 1)
    
    def _generate_statistics(
        self,
        import_result: NovelImportResult,
        chapter_infos: List[ChapterInfo],
        segmentation_results: List[ParagraphSegmentationResult],
        annotation_results: List[AnnotatedChapter]
    ) -> Dict[str, Any]:
        """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "file_size": import_result.file_size,
            "char_count": import_result.char_count,
            "chapter_count": len(chapter_infos),
        }
        
        if segmentation_results:
            stats["total_paragraphs"] = sum(len(s.paragraphs) for s in segmentation_results)
            stats["avg_paragraphs_per_chapter"] = stats["total_paragraphs"] / len(segmentation_results)
        
        if annotation_results:
            stats["total_events"] = sum(a.event_timeline.total_events for a in annotation_results)
            stats["total_settings"] = sum(a.setting_library.total_settings for a in annotation_results)
        
        return stats
