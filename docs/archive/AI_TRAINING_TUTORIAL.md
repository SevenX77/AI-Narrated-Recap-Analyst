# AIæ¨¡å‹è®­ç»ƒä¸å¾®è°ƒå®Œå…¨æŒ‡å—

**æ—¥æœŸ**: 2026-02-05  
**ç›®æ ‡è¯»è€…**: å¯¹AIè®­ç»ƒä¸ç†Ÿæ‚‰çš„å¼€å‘è€…  
**èŒƒå›´**: ä»é›¶åˆ°å®æˆ˜çš„å®Œæ•´æµç¨‹

---

## ğŸ“ ç¬¬ä¸€éƒ¨åˆ†ï¼šæ ¸å¿ƒæ¦‚å¿µç§‘æ™®

### 1. ä»€ä¹ˆæ˜¯"æ¨¡å‹"ï¼Ÿ

æƒ³è±¡ä¸€ä¸ª**è¶…çº§èªæ˜çš„å­¦ç”Ÿ**ï¼š

| æ¯”å–» | AIæ¨¡å‹ä¸­çš„å¯¹åº”æ¦‚å¿µ |
|------|------------------|
| å­¦ç”Ÿçš„å¤§è„‘ | **æ¨¡å‹å‚æ•°**ï¼ˆæ•°åäº¿ä¸ªæ•°å­—ï¼‰ |
| æ•™ç§‘ä¹¦ | **è®­ç»ƒæ•°æ®**ï¼ˆå¤§é‡æ–‡æœ¬ç¤ºä¾‹ï¼‰ |
| å­¦ä¹ è¿‡ç¨‹ | **è®­ç»ƒ**ï¼ˆè°ƒæ•´å‚æ•°ï¼‰ |
| è¯¾åè¾…å¯¼ | **å¾®è°ƒ**ï¼ˆé’ˆå¯¹æ€§è®­ç»ƒï¼‰ |
| è€ƒè¯• | **æ¨ç†/ç”Ÿæˆ**ï¼ˆå®é™…ä½¿ç”¨ï¼‰ |

---

### 2. é¢„è®­ç»ƒ vs å¾®è°ƒï¼šæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

#### ğŸ« é¢„è®­ç»ƒï¼ˆPre-trainingï¼‰

**ç±»æ¯”**ï¼šè®©å­¦ç”Ÿè¯»éæ•´ä¸ªå›¾ä¹¦é¦†

```
è¾“å…¥ï¼šæµ·é‡é€šç”¨æ–‡æœ¬ï¼ˆç»´åŸºç™¾ç§‘ã€ä¹¦ç±ã€ç½‘é¡µ...ï¼‰
ç›®æ ‡ï¼šå­¦ä¼šè¯­è¨€çš„åŸºæœ¬è§„å¾‹
è€—æ—¶ï¼šå‡ ä¸ªæœˆï¼Œæˆæœ¬å‡ ç™¾ä¸‡ç¾å…ƒ
ç»“æœï¼šGPT-4ã€DeepSeekç­‰"åŸºç¡€æ¨¡å‹"
```

**æˆ‘ä»¬é€šå¸¸ä¸åšé¢„è®­ç»ƒ**ï¼Œå› ä¸ºï¼š
- âŒ éœ€è¦æµ·é‡æ•°æ®ï¼ˆå‡ TBæ–‡æœ¬ï¼‰
- âŒ éœ€è¦å¤§é‡GPUï¼ˆå‡ åƒå¼ å¡ï¼‰
- âŒ éœ€è¦å‡ ä¸ªæœˆæ—¶é—´
- âœ… å¯ä»¥ç›´æ¥ç”¨ç°æˆçš„ï¼ˆGPT-4ã€DeepSeekï¼‰

---

#### ğŸ¯ å¾®è°ƒï¼ˆFine-tuningï¼‰

**ç±»æ¯”**ï¼šè®©å­¦ç”Ÿä¸“é—¨å­¦ä¹ æŸä¸ªç§‘ç›®

```
è¾“å…¥ï¼šç‰¹å®šä»»åŠ¡çš„æ ‡æ³¨æ•°æ®ï¼ˆå‡ ç™¾åˆ°å‡ ä¸‡æ¡ï¼‰
ç›®æ ‡ï¼šå­¦ä¼šå®Œæˆç‰¹å®šä»»åŠ¡
è€—æ—¶ï¼šå‡ å°æ—¶åˆ°å‡ å¤©
ç»“æœï¼šä½ çš„ä¸“ç”¨æ¨¡å‹
```

**è¿™æ˜¯æˆ‘ä»¬è¦åšçš„**ï¼š
- âœ… æ•°æ®é‡å°ï¼ˆå‡ ç™¾åˆ°å‡ ä¸‡æ¡ï¼‰
- âœ… æˆæœ¬ä½ï¼ˆå‡ ååˆ°å‡ ç™¾ç¾å…ƒï¼‰
- âœ… æ—¶é—´çŸ­ï¼ˆå‡ å°æ—¶åˆ°å‡ å¤©ï¼‰
- âœ… æ•ˆæœå¥½ï¼ˆé’ˆå¯¹æ€§å¼ºï¼‰

---

## ğŸ”„ ç¬¬äºŒéƒ¨åˆ†ï¼šå¾®è°ƒçš„å®Œæ•´æµç¨‹

### é˜¶æ®µ 0ï¼šå‡†å¤‡å·¥ä½œ

#### 0.1 é€‰æ‹©åŸºç¡€æ¨¡å‹

| æ¨¡å‹ | ä¼˜ç‚¹ | ç¼ºç‚¹ | æ¨èåœºæ™¯ |
|------|------|------|---------|
| **GPT-4** | æ•ˆæœæœ€å¥½ | è´µï¼ˆ$30/1M tokensï¼‰ | é¢„ç®—å……è¶³ã€è¦æ±‚æœ€é«˜è´¨é‡ |
| **DeepSeek V3** | æ€§ä»·æ¯”é«˜ | ä¸­æ–‡æ›´å¼º | **æ¨è**ï¼ˆä¸­æ–‡é¡¹ç›®ï¼‰ |
| **Qwen** | å¼€æº | éœ€è‡ªå·±éƒ¨ç½² | æœ‰æœåŠ¡å™¨èµ„æº |
| **Llama 3** | å¼€æº | è‹±æ–‡ä¸ºä¸» | è‹±æ–‡é¡¹ç›® |

**æˆ‘ä»¬çš„é€‰æ‹©**ï¼šDeepSeek V3
- ğŸ’° æˆæœ¬ä½ï¼š$0.27/1M tokensï¼ˆè¾“å…¥ï¼‰
- ğŸ‡¨ğŸ‡³ ä¸­æ–‡å¼ºï¼šä¸­æ–‡è®­ç»ƒæ•°æ®å¤š
- âš¡ é€Ÿåº¦å¿«ï¼šæ¨ç†é€Ÿåº¦å¿«
- ğŸ”§ æ˜“ç”¨ï¼šAPIå…¼å®¹OpenAI

---

#### 0.2 å‡†å¤‡è®­ç»ƒæ•°æ®

**æ•°æ®æ ¼å¼**ï¼šJSONLï¼ˆæ¯è¡Œä¸€ä¸ªJSONï¼‰

```jsonl
{"messages": [
  {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„å°è¯´åˆ†æå¸ˆ..."},
  {"role": "user", "content": "è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬..."},
  {"role": "assistant", "content": "åˆ†æç»“æœï¼š..."}
]}
{"messages": [
  {"role": "system", "content": "..."},
  {"role": "user", "content": "..."},
  {"role": "assistant", "content": "..."}
]}
...
```

**æ•°æ®é‡å»ºè®®**ï¼š

| ä»»åŠ¡ç±»å‹ | æœ€å°‘ | æ¨è | ç†æƒ³ |
|---------|------|------|------|
| ç®€å•åˆ†ç±» | 50 | 200 | 500+ |
| ä¿¡æ¯æå– | 100 | 500 | 1000+ |
| ç”Ÿæˆä»»åŠ¡ | 200 | 1000 | 5000+ |
| å¤æ‚æ¨ç† | 500 | 2000 | 10000+ |

---

### é˜¶æ®µ 1ï¼šç”Ÿæˆè®­ç»ƒæ•°æ®

#### æ–¹æ³•Aï¼šæ‰‹å·¥æ ‡æ³¨ï¼ˆæœ€å‡†ç¡®ï¼Œæœ€æ…¢ï¼‰

```python
# ç¤ºä¾‹ï¼šæ ‡æ³¨å°è¯´æƒ…èŠ‚
annotation = {
    "text": "é™ˆé‡éª‘ç€äºŒå…«å¤§æ åœ¨è½¦é˜Ÿä¸­...",
    "label": {
        "type": "æ•…äº‹æ¨è¿›",
        "summary": "ä¸»è§’ä»æ±ŸåŸå‡ºå‘",
        "props": ["äºŒå…«å¤§æ "],
        "location": {"from": "æ±ŸåŸ", "to": "è·¯ä¸Š"}
    }
}
```

**æ—¶é—´æˆæœ¬**ï¼š1-5åˆ†é’Ÿ/æ¡

---

#### æ–¹æ³•Bï¼šLLMè¾…åŠ©æ ‡æ³¨ï¼ˆå¿«é€Ÿï¼Œéœ€è¦äººå·¥å®¡æ ¸ï¼‰

```python
# ä½¿ç”¨ç°æœ‰æ¨¡å‹ç”Ÿæˆåˆæ­¥æ ‡æ³¨
def generate_annotation(text: str) -> dict:
    prompt = f"è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æƒ…èŠ‚è¦ç´ ï¼š{text}"
    response = llm.generate(prompt)
    return response

# äººå·¥å®¡æ ¸å’Œä¿®æ­£
annotation = generate_annotation(text)
# äººå·¥æ£€æŸ¥ â†’ ä¿®æ­£é”™è¯¯ â†’ ä¿å­˜
```

**æ—¶é—´æˆæœ¬**ï¼š30ç§’-1åˆ†é’Ÿ/æ¡ï¼ˆå«å®¡æ ¸ï¼‰

---

#### æ–¹æ³•Cï¼šFew-shot + æ‰¹é‡ç”Ÿæˆï¼ˆæœ€å¿«ï¼Œè´¨é‡éœ€æ§åˆ¶ï¼‰

```python
# ä½¿ç”¨å°‘é‡é«˜è´¨é‡ç¤ºä¾‹ï¼Œè®©LLMç”Ÿæˆæ›´å¤š
few_shot_examples = load_examples("gold_standard.jsonl")  # 5-10ä¸ªç²¾å“ç¤ºä¾‹

for chapter in chapters:
    annotation = llm.generate(
        prompt=build_few_shot_prompt(few_shot_examples, chapter)
    )
    # è‡ªåŠ¨è´¨é‡æ£€æŸ¥
    if quality_check(annotation) > 0.8:
        save_to_training_set(annotation)
    else:
        add_to_manual_review_queue(annotation)
```

**æ—¶é—´æˆæœ¬**ï¼šå‡ ç§’/æ¡ï¼ˆè‡ªåŠ¨ï¼‰+ äººå·¥æŠ½æŸ¥10-20%

---

### é˜¶æ®µ 2ï¼šæ•°æ®å‡†å¤‡ä¸éªŒè¯

#### 2.1 æ•°æ®æ¸…æ´—

```python
def clean_training_data(data: List[dict]) -> List[dict]:
    """æ¸…æ´—è®­ç»ƒæ•°æ®"""
    cleaned = []
    
    for item in data:
        # 1. å»é‡
        if is_duplicate(item, cleaned):
            continue
        
        # 2. æ ¼å¼éªŒè¯
        if not validate_format(item):
            logger.warning(f"Invalid format: {item}")
            continue
        
        # 3. è´¨é‡æ£€æŸ¥
        if get_quality_score(item) < 0.7:
            logger.warning(f"Low quality: {item}")
            continue
        
        cleaned.append(item)
    
    return cleaned
```

---

#### 2.2 æ•°æ®åˆ†å‰²

```python
# åˆ†å‰²æ•°æ®é›†
train_data, val_data, test_data = split_dataset(
    cleaned_data,
    train_ratio=0.8,    # 80% è®­ç»ƒ
    val_ratio=0.1,      # 10% éªŒè¯
    test_ratio=0.1      # 10% æµ‹è¯•
)

# ä¿å­˜
save_jsonl(train_data, "train.jsonl")
save_jsonl(val_data, "val.jsonl")
save_jsonl(test_data, "test.jsonl")
```

**ä¸ºä»€ä¹ˆè¦åˆ†å‰²ï¼Ÿ**

| æ•°æ®é›† | ç”¨é€” | è¯´æ˜ |
|--------|------|------|
| **è®­ç»ƒé›†** | è®­ç»ƒæ¨¡å‹ | æ¨¡å‹å­¦ä¹ çš„æ•°æ® |
| **éªŒè¯é›†** | è°ƒæ•´å‚æ•° | é˜²æ­¢è¿‡æ‹Ÿåˆ |
| **æµ‹è¯•é›†** | æœ€ç»ˆè¯„ä¼° | æ¨¡æ‹ŸçœŸå®åœºæ™¯ |

---

### é˜¶æ®µ 3ï¼šæ¨¡å‹å¾®è°ƒ

#### 3.1 é€‰æ‹©å¾®è°ƒæ–¹å¼

| æ–¹å¼ | è¯´æ˜ | æˆæœ¬ | æ•ˆæœ | æ¨èåœºæ™¯ |
|------|------|------|------|---------|
| **APIå¾®è°ƒ** | è°ƒç”¨æœåŠ¡å•†API | ä½ | å¥½ | **æ¨è**ï¼ˆå¿«é€Ÿå¼€å§‹ï¼‰ |
| **LoRAå¾®è°ƒ** | åªè®­ç»ƒéƒ¨åˆ†å‚æ•° | ä¸­ | å¥½ | æœ‰GPUï¼ˆå•å¡ï¼‰ |
| **å…¨é‡å¾®è°ƒ** | è®­ç»ƒæ‰€æœ‰å‚æ•° | é«˜ | æœ€å¥½ | æœ‰å¤§é‡GPU |

**æˆ‘ä»¬æ¨èï¼šAPIå¾®è°ƒ**
- âœ… æ— éœ€GPU
- âœ… é…ç½®ç®€å•
- âœ… æˆæœ¬å¯æ§

---

#### 3.2 DeepSeek API å¾®è°ƒç¤ºä¾‹

```python
from openai import OpenAI

client = OpenAI(
    api_key="your_deepseek_api_key",
    base_url="https://api.deepseek.com"
)

# 1. ä¸Šä¼ è®­ç»ƒæ•°æ®
with open("train.jsonl", "rb") as f:
    train_file = client.files.create(
        file=f,
        purpose="fine-tune"
    )

# 2. åˆ›å»ºå¾®è°ƒä»»åŠ¡
fine_tune_job = client.fine_tuning.jobs.create(
    training_file=train_file.id,
    model="deepseek-chat",
    hyperparameters={
        "n_epochs": 3,              # è®­ç»ƒè½®æ•°
        "batch_size": 8,            # æ‰¹æ¬¡å¤§å°
        "learning_rate": 5e-5       # å­¦ä¹ ç‡
    }
)

print(f"å¾®è°ƒä»»åŠ¡ID: {fine_tune_job.id}")

# 3. ç›‘æ§è®­ç»ƒè¿›åº¦
while True:
    job_status = client.fine_tuning.jobs.retrieve(fine_tune_job.id)
    print(f"çŠ¶æ€: {job_status.status}")
    
    if job_status.status == "succeeded":
        fine_tuned_model = job_status.fine_tuned_model
        print(f"å¾®è°ƒå®Œæˆï¼æ¨¡å‹ID: {fine_tuned_model}")
        break
    elif job_status.status == "failed":
        print(f"å¾®è°ƒå¤±è´¥: {job_status.error}")
        break
    
    time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
```

---

#### 3.3 å…³é”®è¶…å‚æ•°è§£é‡Š

| å‚æ•° | è¯´æ˜ | æ¨èå€¼ | è°ƒæ•´å»ºè®® |
|------|------|--------|---------|
| **n_epochs** | è®­ç»ƒè½®æ•° | 3-5 | æ•°æ®å¤šâ†’å°‘è½®ï¼Œæ•°æ®å°‘â†’å¤šè½® |
| **batch_size** | æ‰¹æ¬¡å¤§å° | 4-16 | GPUå†…å­˜å¤§â†’å¤§æ‰¹æ¬¡ |
| **learning_rate** | å­¦ä¹ ç‡ | 1e-5 ~ 5e-5 | è¿‡æ‹Ÿåˆâ†’é™ä½ï¼Œæ¬ æ‹Ÿåˆâ†’æé«˜ |

**å¸¸è§é—®é¢˜**ï¼š

```python
# é—®é¢˜1ï¼šè¿‡æ‹Ÿåˆï¼ˆè®­ç»ƒé›†å¥½ï¼ŒéªŒè¯é›†å·®ï¼‰
# è§£å†³ï¼šå‡å°‘ n_epochs æˆ–å¢åŠ  dropout

# é—®é¢˜2ï¼šæ¬ æ‹Ÿåˆï¼ˆè®­ç»ƒé›†å’ŒéªŒè¯é›†éƒ½å·®ï¼‰
# è§£å†³ï¼šå¢åŠ  n_epochs æˆ–æé«˜ learning_rate

# é—®é¢˜3ï¼šè®­ç»ƒä¸ç¨³å®šï¼ˆlossæ³¢åŠ¨å¤§ï¼‰
# è§£å†³ï¼šé™ä½ learning_rate æˆ–å‡å° batch_size
```

---

### é˜¶æ®µ 4ï¼šæ¨¡å‹è¯„ä¼°

#### 4.1 è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡

```python
def evaluate_model(model_id: str, test_data: List[dict]) -> dict:
    """è¯„ä¼°å¾®è°ƒåçš„æ¨¡å‹"""
    
    metrics = {
        "accuracy": 0.0,      # å‡†ç¡®ç‡
        "precision": 0.0,     # ç²¾ç¡®ç‡
        "recall": 0.0,        # å¬å›ç‡
        "f1_score": 0.0       # F1åˆ†æ•°
    }
    
    correct = 0
    total = len(test_data)
    
    for item in test_data:
        # ä½¿ç”¨å¾®è°ƒæ¨¡å‹ç”Ÿæˆ
        prediction = client.chat.completions.create(
            model=model_id,
            messages=item["messages"][:-1]  # å»æ‰æ­£ç¡®ç­”æ¡ˆ
        ).choices[0].message.content
        
        # ä¸æ­£ç¡®ç­”æ¡ˆå¯¹æ¯”
        ground_truth = item["messages"][-1]["content"]
        
        if compare(prediction, ground_truth):
            correct += 1
    
    metrics["accuracy"] = correct / total
    return metrics
```

---

#### 4.2 äººå·¥è¯„ä¼°ï¼ˆæ›´é‡è¦ï¼ï¼‰

```python
# æŠ½æ ·æµ‹è¯•
test_samples = random.sample(test_data, 20)

for i, sample in enumerate(test_samples):
    print(f"\n=== æµ‹è¯• {i+1} ===")
    print(f"è¾“å…¥: {sample['input']}")
    print(f"æœŸæœ›è¾“å‡º: {sample['expected']}")
    
    # å¾®è°ƒæ¨¡å‹è¾“å‡º
    prediction = model.generate(sample['input'])
    print(f"å®é™…è¾“å‡º: {prediction}")
    
    # äººå·¥æ‰“åˆ†
    score = input("è´¨é‡è¯„åˆ† (1-5): ")
    feedback = input("é—®é¢˜æè¿°: ")
    
    log_evaluation(sample, prediction, score, feedback)
```

**äººå·¥è¯„ä¼°ç»´åº¦**ï¼š

| ç»´åº¦ | è¯´æ˜ | è¯„åˆ†æ ‡å‡† |
|------|------|---------|
| **å‡†ç¡®æ€§** | ä¿¡æ¯æ˜¯å¦æ­£ç¡® | 1-5åˆ† |
| **å®Œæ•´æ€§** | æ˜¯å¦é—æ¼é‡è¦ä¿¡æ¯ | 1-5åˆ† |
| **æµç•…æ€§** | è¡¨è¾¾æ˜¯å¦è‡ªç„¶ | 1-5åˆ† |
| **ç›¸å…³æ€§** | æ˜¯å¦åç¦»ä¸»é¢˜ | 1-5åˆ† |

---

### é˜¶æ®µ 5ï¼šéƒ¨ç½²ä¸ä½¿ç”¨

#### 5.1 ä½¿ç”¨å¾®è°ƒæ¨¡å‹

```python
# ä½¿ç”¨æ–¹å¼ä¸åŸæ¨¡å‹å®Œå…¨ç›¸åŒ
response = client.chat.completions.create(
    model="ft-your-fine-tuned-model-id",  # ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹ID
    messages=[
        {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„å°è¯´åˆ†æå¸ˆ"},
        {"role": "user", "content": "è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬..."}
    ]
)

print(response.choices[0].message.content)
```

---

#### 5.2 A/Bæµ‹è¯•

```python
def ab_test(input_text: str):
    """å¯¹æ¯”åŸæ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹"""
    
    # åŸæ¨¡å‹
    base_response = client.chat.completions.create(
        model="deepseek-chat",
        messages=[...]
    )
    
    # å¾®è°ƒæ¨¡å‹
    finetuned_response = client.chat.completions.create(
        model="ft-your-model",
        messages=[...]
    )
    
    print("=== åŸæ¨¡å‹ ===")
    print(base_response.choices[0].message.content)
    
    print("\n=== å¾®è°ƒæ¨¡å‹ ===")
    print(finetuned_response.choices[0].message.content)
    
    # äººå·¥æˆ–è‡ªåŠ¨è¯„ä¼°å“ªä¸ªæ›´å¥½
    winner = input("å“ªä¸ªæ›´å¥½ï¼Ÿ(1=åŸæ¨¡å‹, 2=å¾®è°ƒæ¨¡å‹): ")
    log_ab_result(input_text, winner)
```

---

## ğŸ¯ ç¬¬ä¸‰éƒ¨åˆ†ï¼šæœ¬é¡¹ç›®çš„åº”ç”¨åœºæ™¯

### åœºæ™¯1ï¼šå°è¯´æƒ…èŠ‚æ ‡æ³¨

**ç›®æ ‡**ï¼šè®©æ¨¡å‹å­¦ä¼šè¯†åˆ«æ•…äº‹æ¨è¿›ã€è®¾å®šã€é“å…·ã€æ—¶ç©ºå˜åŒ–

#### è®­ç»ƒæ•°æ®ç”Ÿæˆ

```python
# 1. ä½¿ç”¨ç¬¬ä¸€ç« åˆ†æ®µ.mdä½œä¸ºç¤ºä¾‹
gold_standard = parse_markdown("åˆ†æèµ„æ–™/æœ‰åŸå°è¯´/01_æœ«å“¥è¶…å‡¡å…¬è·¯/novel/ç¬¬ä¸€ç« åˆ†æ®µ.md")

# 2. ç”¨Few-shotè®©LLMç”Ÿæˆæ›´å¤šæ ‡æ³¨
for chapter in novel_chapters[1:]:  # ç¬¬2-50ç« 
    annotation = llm.generate_with_few_shot(
        examples=gold_standard[:3],  # ä½¿ç”¨å‰3ä¸ªç¤ºä¾‹
        input_text=chapter
    )
    
    # 3. äººå·¥æŠ½æŸ¥20%
    if random.random() < 0.2:
        annotation = human_review(annotation)
    
    training_data.append(annotation)
```

---

#### å¾®è°ƒæµç¨‹

```python
# 1. å‡†å¤‡æ•°æ®
training_examples = []

for chapter, annotation in training_data:
    training_examples.append({
        "messages": [
            {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„å°è¯´åˆ†æå¸ˆï¼Œæ“…é•¿è¯†åˆ«æ•…äº‹ç»“æ„..."},
            {"role": "user", "content": f"è¯·åˆ†æä»¥ä¸‹ç« èŠ‚ï¼š\n\n{chapter}"},
            {"role": "assistant", "content": json.dumps(annotation, ensure_ascii=False)}
        ]
    })

# 2. ä¿å­˜ä¸ºJSONL
save_jsonl(training_examples, "novel_annotation_train.jsonl")

# 3. å¾®è°ƒ
fine_tune_job = client.fine_tuning.jobs.create(
    training_file=upload_file("novel_annotation_train.jsonl"),
    model="deepseek-chat",
    hyperparameters={"n_epochs": 3}
)

# 4. ç­‰å¾…å®Œæˆå¹¶æµ‹è¯•
fine_tuned_model_id = wait_for_completion(fine_tune_job.id)
test_on_new_chapter(fine_tuned_model_id, chapter_51)
```

---

### åœºæ™¯2ï¼šè§£è¯´è¯è´¨é‡è¯„ä¼°

**ç›®æ ‡**ï¼šè®©æ¨¡å‹å­¦ä¼šè¯„ä¼°ç”Ÿæˆçš„è§£è¯´è¯æ˜¯å¦æŠ“ä½å…³é”®æƒ…èŠ‚

#### è®­ç»ƒæ•°æ®ç¤ºä¾‹

```jsonl
{"messages": [
  {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„è§£è¯´è¯è´¨é‡è¯„ä¼°å¸ˆ..."},
  {"role": "user", "content": "åŸæ–‡ï¼š{novel_text}\nè§£è¯´è¯ï¼š{script_text}\nè¯·è¯„ä¼°è´¨é‡ã€‚"},
  {"role": "assistant", "content": "{\"score\": 85, \"reasoning\": \"æŠ“ä½äº†æ ¸å¿ƒè®¾å®šå’Œä¸»è¦æƒ…èŠ‚ï¼Œä½†é—æ¼äº†äºŒå…«å¤§æ å‡çº§è¿™ä¸ªå…³é”®é’©å­...\"}"}
]}
```

---

## ğŸ’° ç¬¬å››éƒ¨åˆ†ï¼šæˆæœ¬ä¼°ç®—

### DeepSeek å¾®è°ƒæˆæœ¬ï¼ˆ2026å¹´ä»·æ ¼ï¼‰

| é¡¹ç›® | æˆæœ¬ | è¯´æ˜ |
|------|------|------|
| **æ•°æ®ç”Ÿæˆ** | $1-5 | ç”¨åŸºç¡€æ¨¡å‹ç”Ÿæˆ1000æ¡è®­ç»ƒæ•°æ® |
| **å¾®è°ƒè®­ç»ƒ** | $10-50 | å–å†³äºæ•°æ®é‡å’Œè®­ç»ƒè½®æ•° |
| **æ¨¡å‹å­˜å‚¨** | $2/æœˆ | ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹ |
| **æ¨ç†ä½¿ç”¨** | $0.27/1M tokens | ä½¿ç”¨å¾®è°ƒæ¨¡å‹çš„æˆæœ¬ |

**æ€»æˆæœ¬ï¼ˆé¦–æ¬¡ï¼‰**ï¼šçº¦ $20-100  
**æœˆåº¦æˆæœ¬**ï¼š$2 å­˜å‚¨è´¹ + æ¨ç†è´¹ç”¨

---

## âš ï¸ ç¬¬äº”éƒ¨åˆ†ï¼šå¸¸è§é™·é˜±ä¸æ³¨æ„äº‹é¡¹

### é™·é˜±1ï¼šæ•°æ®è´¨é‡å·®

```python
# âŒ é”™è¯¯åšæ³•ï¼šä¸åŠ å®¡æ ¸åœ°æ‰¹é‡ç”Ÿæˆ
for text in texts:
    annotation = llm.generate(text)
    training_data.append(annotation)  # å¯èƒ½æœ‰å¾ˆå¤šé”™è¯¯

# âœ… æ­£ç¡®åšæ³•ï¼šè´¨é‡æ§åˆ¶
for text in texts:
    annotation = llm.generate(text)
    
    # è‡ªåŠ¨è´¨é‡æ£€æŸ¥
    if quality_score(annotation) < 0.8:
        annotation = human_review(annotation)
    
    training_data.append(annotation)
```

---

### é™·é˜±2ï¼šè¿‡æ‹Ÿåˆ

**ç°è±¡**ï¼šè®­ç»ƒé›†å‡†ç¡®ç‡95%ï¼Œæµ‹è¯•é›†å‡†ç¡®ç‡60%

**åŸå› **ï¼š
- è®­ç»ƒæ•°æ®å¤ªå°‘
- è®­ç»ƒè½®æ•°å¤ªå¤š
- æ•°æ®å¤šæ ·æ€§ä¸è¶³

**è§£å†³**ï¼š
```python
# 1. å¢åŠ æ•°æ®å¤šæ ·æ€§
training_data = diversify_data(training_data)

# 2. å‡å°‘è®­ç»ƒè½®æ•°
hyperparameters={"n_epochs": 2}  # ä»5è½®é™åˆ°2è½®

# 3. ä½¿ç”¨æ­£åˆ™åŒ–
hyperparameters={"dropout": 0.1}
```

---

### é™·é˜±3ï¼šè¯„ä¼°ä¸å……åˆ†

```python
# âŒ åªçœ‹è‡ªåŠ¨æŒ‡æ ‡
print(f"å‡†ç¡®ç‡: {accuracy}")  # 85%ï¼Œçœ‹èµ·æ¥ä¸é”™

# âœ… ç»“åˆäººå·¥è¯„ä¼°
auto_metrics = calculate_metrics(predictions)
human_scores = human_evaluate(samples)

print(f"è‡ªåŠ¨å‡†ç¡®ç‡: {auto_metrics['accuracy']}")
print(f"äººå·¥å¹³å‡åˆ†: {human_scores['avg_score']}")
print(f"å¸¸è§é—®é¢˜: {human_scores['issues']}")
```

---

## ğŸš€ ç¬¬å…­éƒ¨åˆ†ï¼šå¿«é€Ÿå¼€å§‹æ¸…å•

### ç¬¬1æ­¥ï¼šå‡†å¤‡ç¯å¢ƒ

```bash
# å®‰è£…ä¾èµ–
pip install openai datasets pandas

# è®¾ç½®APIå¯†é’¥
export DEEPSEEK_API_KEY="your_key"
```

---

### ç¬¬2æ­¥ï¼šæ”¶é›†è®­ç»ƒæ•°æ®

```python
# æ–¹æ¡ˆAï¼šæ‰‹å·¥æ ‡æ³¨ï¼ˆ5-10ä¸ªé«˜è´¨é‡ç¤ºä¾‹ï¼‰
examples = manually_annotate(samples[:10])

# æ–¹æ¡ˆBï¼šLLMè¾…åŠ©ï¼ˆç”¨ç¤ºä¾‹ç”Ÿæˆ100-1000æ¡ï¼‰
training_data = generate_with_few_shot(examples, all_samples)
```

---

### ç¬¬3æ­¥ï¼šå‡†å¤‡æ•°æ®æ–‡ä»¶

```python
# è½¬æ¢ä¸ºå¾®è°ƒæ ¼å¼
formatted_data = [
    {
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": input_text},
            {"role": "assistant", "content": expected_output}
        ]
    }
    for input_text, expected_output in training_data
]

# ä¿å­˜
save_jsonl(formatted_data, "train.jsonl")
```

---

### ç¬¬4æ­¥ï¼šå¯åŠ¨å¾®è°ƒ

```python
from openai import OpenAI

client = OpenAI(
    api_key=os.getenv("DEEPSEEK_API_KEY"),
    base_url="https://api.deepseek.com"
)

# ä¸Šä¼ æ•°æ®
with open("train.jsonl", "rb") as f:
    file = client.files.create(file=f, purpose="fine-tune")

# å¯åŠ¨å¾®è°ƒ
job = client.fine_tuning.jobs.create(
    training_file=file.id,
    model="deepseek-chat"
)

print(f"å¾®è°ƒä»»åŠ¡ID: {job.id}")
```

---

### ç¬¬5æ­¥ï¼šç­‰å¾…å¹¶æµ‹è¯•

```python
# ç­‰å¾…å®Œæˆ
model_id = wait_for_fine_tune(job.id)

# æµ‹è¯•
test_response = client.chat.completions.create(
    model=model_id,
    messages=[
        {"role": "user", "content": "æµ‹è¯•è¾“å…¥..."}
    ]
)

print(test_response.choices[0].message.content)
```

---

## ğŸ“š ç¬¬ä¸ƒéƒ¨åˆ†ï¼šæ¨èå­¦ä¹ èµ„æº

### å®˜æ–¹æ–‡æ¡£

| èµ„æº | é“¾æ¥ | è¯´æ˜ |
|------|------|------|
| OpenAI Fine-tuning Guide | https://platform.openai.com/docs/guides/fine-tuning | APIå¾®è°ƒå®˜æ–¹æ•™ç¨‹ |
| DeepSeek API Docs | https://platform.deepseek.com/api-docs | DeepSeek APIæ–‡æ¡£ |
| Hugging Face Tutorials | https://huggingface.co/docs/transformers | å¼€æºæ¨¡å‹å¾®è°ƒ |

### å®æˆ˜æ•™ç¨‹

| èµ„æº | éš¾åº¦ | è¯´æ˜ |
|------|------|------|
| "Fine-tuning GPT for Classification" | â­â­ | åˆ†ç±»ä»»åŠ¡å…¥é—¨ |
| "Custom Chatbot with Fine-tuning" | â­â­â­ | å¯¹è¯ä»»åŠ¡å®æˆ˜ |
| "LoRA Fine-tuning Guide" | â­â­â­â­ | æœ¬åœ°å¾®è°ƒè¿›é˜¶ |

---

## ğŸ“ æ€»ç»“

### å…³é”®è¦ç‚¹

1. **å¾®è°ƒ â‰  ä»é›¶è®­ç»ƒ**
   - åŸºäºå·²æœ‰æ¨¡å‹ï¼ˆå¦‚DeepSeekï¼‰
   - åªéœ€å°‘é‡æ•°æ®ï¼ˆå‡ ç™¾åˆ°å‡ åƒæ¡ï¼‰
   - æˆæœ¬ä½ã€æ—¶é—´çŸ­

2. **æ•°æ®è´¨é‡ > æ•°æ®æ•°é‡**
   - 100æ¡é«˜è´¨é‡æ•°æ® > 1000æ¡ä½è´¨é‡æ•°æ®
   - ä¸€å®šè¦äººå·¥å®¡æ ¸å…³é”®æ ·æœ¬

3. **è¿­ä»£æ”¹è¿›**
   - å…ˆç”¨å°‘é‡æ•°æ®å¿«é€Ÿæµ‹è¯•
   - æ ¹æ®æ•ˆæœè°ƒæ•´æ•°æ®å’Œå‚æ•°
   - é€æ­¥æ‰©å¤§è§„æ¨¡

4. **è¯„ä¼°è¦å…¨é¢**
   - ä¸èƒ½åªçœ‹è‡ªåŠ¨æŒ‡æ ‡
   - äººå·¥è¯„ä¼°æ›´é‡è¦
   - A/Bæµ‹è¯•å¯¹æ¯”åŸºç¡€æ¨¡å‹

---

## ğŸ”— ç›¸å…³æ–‡æ¡£

- `docs/architecture/LAYERED_ALIGNMENT_DESIGN.md` - v4.0 å¯¹é½æ¶æ„
- `docs/maintenance/PROJECT_OPTIMIZATION_V2.1.md` - æ•°æ®ç»“æ„ä¼˜åŒ–
- `src/prompts/novel_segmentation.yaml` - å°è¯´åˆ†æ®µPromptç¤ºä¾‹

---

**å‡†å¤‡å¥½å¼€å§‹äº†å—ï¼Ÿ** ä»æ”¶é›†10ä¸ªé«˜è´¨é‡æ ‡æ³¨ç¤ºä¾‹å¼€å§‹ï¼ğŸš€

---
*æœ€åæ›´æ–°: 2026-02-05*
