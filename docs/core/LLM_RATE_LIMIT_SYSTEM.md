# LLMå¹¶å‘è°ƒç”¨ç®¡ç†ç³»ç»Ÿ

## ğŸ“‹ æ¦‚è¿°

ç»Ÿä¸€çš„LLMè°ƒç”¨ç®¡ç†ç³»ç»Ÿï¼Œæä¾›ï¼š
- âœ… å¤šæä¾›å•†é™æµè§„åˆ™ç®¡ç†
- âœ… è‡ªåŠ¨é™æµæ£€æµ‹ä¸ç­‰å¾…
- âœ… æ™ºèƒ½é‡è¯•ç­–ç•¥ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
- âœ… å¹¶å‘æ§åˆ¶
- âœ… ä½¿ç”¨ç»Ÿè®¡è¿½è¸ª
- âœ… è‡ªåŠ¨æµ‹è¯•å·¥å…·

---

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          LLMCallManagerï¼ˆå…¨å±€å•ä¾‹ï¼‰          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  - é…ç½®ç®¡ç†ï¼ˆåŠ è½½/ä¿å­˜/æ›´æ–°ï¼‰                â”‚
â”‚  - é™æµå™¨ç®¡ç†ï¼ˆæ¯ä¸ªæ¨¡å‹ä¸€ä¸ªé™æµå™¨ï¼‰          â”‚
â”‚  - ç»Ÿä¸€è°ƒç”¨æ¥å£ï¼ˆå¸¦é™æµ+é‡è¯•ï¼‰               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
       â”‚               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚ RateLimiter â”‚ â”‚ RetryLogic â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ - QPMé™åˆ¶   â”‚ â”‚ - æŒ‡æ•°é€€é¿  â”‚
â”‚ - QPDé™åˆ¶   â”‚ â”‚ - é”™è¯¯è¯†åˆ«  â”‚
â”‚ - TPMé™åˆ¶   â”‚ â”‚ - æ™ºèƒ½å»¶è¿Ÿ  â”‚
â”‚ - å¹¶å‘æ§åˆ¶   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ æ ¸å¿ƒç»„ä»¶

### 1. `LLMRateLimitConfig`
**é™æµé…ç½®æ¨¡å‹**

```python
@dataclass
class LLMRateLimitConfig:
    provider: str  # æä¾›å•†ï¼ˆanthropic, deepseek, openaiï¼‰
    model: str  # æ¨¡å‹åç§°
    
    # é™æµè§„åˆ™
    requests_per_minute: Optional[int] = None  # QPM
    requests_per_day: Optional[int] = None  # QPD
    tokens_per_minute: Optional[int] = None  # TPM
    tokens_per_day: Optional[int] = None  # TPD
    
    # å¹¶å‘æ§åˆ¶
    max_concurrent: int = 1
    
    # é‡è¯•ç­–ç•¥
    max_retries: int = 3
    base_retry_delay: float = 2.0
    max_retry_delay: float = 60.0
    
    # æµ‹è¯•çŠ¶æ€
    is_tested: bool = False
    last_test_date: Optional[str] = None
    test_notes: str = ""
```

### 2. `RateLimiter`
**é™æµå™¨ï¼ˆæ»‘åŠ¨çª—å£ç®—æ³•ï¼‰**

åŠŸèƒ½ï¼š
- è·Ÿè¸ªæ—¶é—´çª—å£å†…çš„è¯·æ±‚æ•°å’Œtokenæ•°
- ç¡®ä¿ä¸è¶…è¿‡QPM/QPD/TPM/TPDé™åˆ¶
- å¹¶å‘æ•°æ§åˆ¶
- è‡ªåŠ¨æ¸…ç†è¿‡æœŸè®°å½•

æ ¸å¿ƒæ–¹æ³•ï¼š
```python
async def acquire(estimated_tokens: int) -> bool:
    """è¯·æ±‚æ‰§è¡Œæƒé™"""
    # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶
    # å¦‚æœæœªè¶…è¿‡ï¼Œè®°å½•å¹¶è¿”å›True
    # å¦‚æœè¶…è¿‡ï¼Œè¿”å›False

async def release():
    """é‡Šæ”¾æ‰§è¡Œæƒé™"""
```

### 3. `LLMCallManager`
**è°ƒç”¨ç®¡ç†å™¨ï¼ˆå…¨å±€å•ä¾‹ï¼‰**

åŠŸèƒ½ï¼š
- é…ç½®ç®¡ç†ï¼ˆåŠ è½½/ä¿å­˜/æ›´æ–°ï¼‰
- ä¸ºæ¯ä¸ªæ¨¡å‹åˆ›å»ºé™æµå™¨
- æä¾›ç»Ÿä¸€çš„è°ƒç”¨æ¥å£
- è‡ªåŠ¨é‡è¯•ä¸é”™è¯¯å¤„ç†

æ ¸å¿ƒæ–¹æ³•ï¼š
```python
async def call_with_rate_limit(
    func: Callable,
    provider: str,
    model: str,
    estimated_tokens: int = 1000,
    *args, **kwargs
) -> Any:
    """å¸¦é™æµæ§åˆ¶çš„LLMè°ƒç”¨"""
    # 1. è·å–é…ç½®å’Œé™æµå™¨
    # 2. ç­‰å¾…è·å–æ‰§è¡Œæƒé™ï¼ˆé˜»å¡å¼ï¼‰
    # 3. æ‰§è¡Œå‡½æ•°ï¼ˆå¸¦é‡è¯•ï¼‰
    # 4. é‡Šæ”¾æ‰§è¡Œæƒé™
```

---

## ğŸ“¦ é¢„å®šä¹‰é…ç½®

### Anthropic Claude
```python
{
    "provider": "anthropic",
    "model": "claude-3-5-sonnet-20241022",
    "requests_per_minute": 50,  # å¾…æµ‹è¯•éªŒè¯
    "tokens_per_minute": 40000,  # å¾…æµ‹è¯•éªŒè¯
    "max_concurrent": 3,
    "max_retries": 3,
    "base_retry_delay": 2.0
}
```

### DeepSeek
```python
{
    "provider": "deepseek",
    "model": "deepseek-chat",
    "requests_per_minute": 60,  # å¾…æµ‹è¯•éªŒè¯
    "max_concurrent": 2,
    "max_retries": 3,
    "base_retry_delay": 3.0
}
```

### OpenAI GPT-4
```python
{
    "provider": "openai",
    "model": "gpt-4",
    "requests_per_minute": 500,  # ä»˜è´¹è´¦æˆ·
    "requests_per_day": 10000,
    "tokens_per_minute": 10000,
    "max_concurrent": 5
}
```

### Conservativeï¼ˆä¿å®ˆé…ç½®ï¼‰
ç”¨äºæœªæµ‹è¯•çš„æ¨¡å‹ï¼š
```python
{
    "requests_per_minute": 10,
    "max_concurrent": 1,
    "max_retries": 5,
    "base_retry_delay": 5.0
}
```

---

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### 1. åŸºæœ¬ä½¿ç”¨

```python
from src.core.llm_rate_limiter import get_llm_manager

# è·å–å…¨å±€ç®¡ç†å™¨
manager = get_llm_manager()

# å®šä¹‰ä½ çš„APIè°ƒç”¨å‡½æ•°
def my_llm_call():
    # å®é™…çš„LLMè°ƒç”¨é€»è¾‘
    return client.messages.create(...)

# ä½¿ç”¨ç®¡ç†å™¨è°ƒç”¨ï¼ˆè‡ªåŠ¨é™æµ+é‡è¯•ï¼‰
result = await manager.call_with_rate_limit(
    func=my_llm_call,
    provider="anthropic",
    model="claude-3-5-sonnet-20241022",
    estimated_tokens=1000  # é¢„ä¼°tokenä½¿ç”¨é‡
)
```

### 2. åœ¨Workflowä¸­ä½¿ç”¨

**åŸä»£ç **ï¼ˆç›´æ¥è°ƒç”¨ï¼‰ï¼š
```python
seg_output = self.novel_segmenter.execute(
    chapter_content=chapter_content,
    chapter_number=chapter.number,
    chapter_title=chapter.title
)
```

**ä¼˜åŒ–å**ï¼ˆä½¿ç”¨ç®¡ç†å™¨ï¼‰ï¼š
```python
from src.core.llm_rate_limiter import get_llm_manager

manager = get_llm_manager()

seg_output = await manager.call_with_rate_limit(
    func=self.novel_segmenter.execute,
    provider="deepseek",  # ä»configè·å–
    model="deepseek-chat",
    estimated_tokens=2000,  # æ ¹æ®ç« èŠ‚é•¿åº¦ä¼°ç®—
    chapter_content=chapter_content,
    chapter_number=chapter.number,
    chapter_title=chapter.title
)
```

### 3. æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯

```python
# è·å–æ‰€æœ‰é™æµå™¨çš„ç»Ÿè®¡ä¿¡æ¯
stats = manager.get_all_stats()

for model, stat in stats.items():
    print(f"{model}:")
    print(f"  å½“å‰å¹¶å‘: {stat['current_concurrent']}")
    print(f"  æœ€è¿‘1åˆ†é’Ÿè¯·æ±‚: {stat['requests_last_minute']}")
    print(f"  æœ€è¿‘1å¤©è¯·æ±‚: {stat['requests_last_day']}")
    print(f"  æœ€è¿‘1åˆ†é’Ÿtokens: {stat['tokens_last_minute']}")
```

### 4. æ›´æ–°é…ç½®

```python
# æ ¹æ®æµ‹è¯•ç»“æœæ›´æ–°é…ç½®
manager.update_config(
    "anthropic_claude",
    requests_per_minute=80,  # æ›´æ–°QPM
    is_tested=True,
    last_test_date="2026-02-10",
    test_notes="æµ‹è¯•éªŒè¯ï¼šQPM=80å¯ç¨³å®šè¿è¡Œ"
)
```

---

## ğŸ§ª é™æµè§„åˆ™æµ‹è¯•

### æµ‹è¯•å·¥å…·ä½¿ç”¨

```bash
# è¿è¡Œäº¤äº’å¼æµ‹è¯•å·¥å…·
python3 scripts/test/test_llm_rate_limits.py
```

### æµ‹è¯•æµç¨‹

1. **é€‰æ‹©æµ‹è¯•æ¨¡å¼**
   - å¿«é€Ÿæµ‹è¯•ï¼ˆä½¿ç”¨mockæ•°æ®ï¼‰
   - å•ä¸ªæä¾›å•†æµ‹è¯•ï¼ˆéœ€è¦å®é™…APIï¼‰
   - æŸ¥çœ‹å½“å‰é…ç½®

2. **è‡ªåŠ¨æµ‹è¯•é€»è¾‘**
   - é€æ¸åŠ å¿«è¯·æ±‚é¢‘ç‡
   - ç›´åˆ°è§¦å‘é™æµ
   - è®°å½•æˆåŠŸ/å¤±è´¥æ¬¡æ•°
   - è®¡ç®—ä¼°ç®—çš„QPM
   - ç»™å‡ºå»ºè®®é…ç½®

3. **æµ‹è¯•ç»“æœ**
   ```json
   {
     "provider": "anthropic",
     "model": "claude-3-5-sonnet-20241022",
     "test_date": "2026-02-10T12:00:00",
     "successful_requests": 45,
     "rate_limited_requests": 5,
     "estimated_qpm": 48,
     "suggested_qpm": 38,
     "notes": "è§¦å‘5æ¬¡é™æµï¼Œå»ºè®®QPMè®¾ç½®ä¸º38"
   }
   ```

4. **æ›´æ–°é…ç½®**
   - å·¥å…·ä¼šè‡ªåŠ¨è¯¢é—®æ˜¯å¦æ›´æ–°é…ç½®
   - æ›´æ–°åçš„é…ç½®æŒä¹…åŒ–åˆ°`data/llm_configs.json`

### å®é™…APIæµ‹è¯•ï¼ˆç¤ºä¾‹ï¼‰

```python
# åˆ›å»ºå®é™…çš„APIè°ƒç”¨å‡½æ•°
def test_anthropic():
    from anthropic import Anthropic
    client = Anthropic(api_key="...")
    
    response = client.messages.create(
        model="claude-3-5-sonnet-20241022",
        max_tokens=100,
        messages=[{"role": "user", "content": "Hi"}]
    )
    return response

# æµ‹è¯•
tester = LLMRateLimitTester()
result = await tester.test_provider(
    provider="anthropic",
    model="claude-3-5-sonnet-20241022",
    test_func=test_anthropic,
    test_duration=60,  # æµ‹è¯•60ç§’
    ramp_up_delay=2.0  # åˆå§‹å»¶è¿Ÿ2ç§’
)

# æ›´æ–°é…ç½®
tester.update_configs_from_test()
```

---

## ğŸ“Š æµ‹è¯•ç»“æœç®¡ç†

### æµ‹è¯•è®°å½•æ ¼å¼

```json
{
  "anthropic_claude": {
    "provider": "anthropic",
    "model": "claude-3-5-sonnet-20241022",
    "test_date": "2026-02-10T12:00:00",
    "test_duration_seconds": 60,
    "total_requests": 50,
    "successful_requests": 45,
    "rate_limited_requests": 5,
    "other_errors": 0,
    "estimated_qpm": 48,
    "suggested_qpm": 38,
    "notes": "è§¦å‘5æ¬¡é™æµï¼Œå»ºè®®QPM=38"
  }
}
```

### æµ‹è¯•ç»“æœæ–‡ä»¶

- **é…ç½®æ–‡ä»¶**: `data/llm_configs.json`
- **æµ‹è¯•ç»“æœ**: `data/llm_rate_limit_test_results.json`

---

## ğŸ”„ é›†æˆåˆ°ç°æœ‰Workflow

### Step 1: å¯¼å…¥ç®¡ç†å™¨

```python
from src.core.llm_rate_limiter import get_llm_manager

class NovelProcessingWorkflow(BaseWorkflow):
    def __init__(self):
        super().__init__()
        # ...å…¶ä»–åˆå§‹åŒ–
        
        # åˆå§‹åŒ–LLMè°ƒç”¨ç®¡ç†å™¨
        self.llm_manager = get_llm_manager()
```

### Step 2: æ›¿æ¢ç›´æ¥è°ƒç”¨

**åŸæ–¹æ³•**ï¼ˆåœ¨_segment_single_chapterä¸­ï¼‰ï¼š
```python
seg_output = self.novel_segmenter.execute(
    chapter_content=chapter_content,
    chapter_number=chapter.number,
    chapter_title=chapter.title,
    provider=workflow_config.segmentation_provider
)
```

**ä¼˜åŒ–å**ï¼š
```python
seg_output = await self.llm_manager.call_with_rate_limit(
    func=self.novel_segmenter.execute,
    provider="deepseek",  # æˆ–ä»configè·å–
    model="deepseek-chat",
    estimated_tokens=len(chapter_content) * 2,  # æ ¹æ®å†…å®¹é•¿åº¦ä¼°ç®—
    chapter_content=chapter_content,
    chapter_number=chapter.number,
    chapter_title=chapter.title,
    provider=workflow_config.segmentation_provider
)
```

### Step 3: ç§»é™¤æ—§çš„é‡è¯•é€»è¾‘

ç°åœ¨é‡è¯•é€»è¾‘ç”±`LLMCallManager`ç»Ÿä¸€ç®¡ç†ï¼Œå¯ä»¥åˆ é™¤workflowä¸­çš„`_retry_with_backoff`æ–¹æ³•ã€‚

---

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. Tokenä¼°ç®—

å‡†ç¡®çš„tokenä¼°ç®—å¯ä»¥æé«˜TPMé™æµçš„å‡†ç¡®æ€§ï¼š

```python
def estimate_tokens(text: str) -> int:
    """ä¼°ç®—æ–‡æœ¬çš„tokenæ•°é‡"""
    # ç®€å•ä¼°ç®—ï¼šä¸­æ–‡1å­—â‰ˆ1.5tokensï¼Œè‹±æ–‡1è¯â‰ˆ1.3tokens
    chinese_chars = len([c for c in text if '\u4e00' <= c <= '\u9fff'])
    other_chars = len(text) - chinese_chars
    
    return int(chinese_chars * 1.5 + other_chars * 0.3)

# ä½¿ç”¨
estimated_tokens = estimate_tokens(chapter_content)
```

### 2. æ‰¹é‡æ“ä½œä¼˜åŒ–

å¯¹äºæ‰¹é‡æ“ä½œï¼Œä½¿ç”¨ç®¡ç†å™¨ä¼šè‡ªåŠ¨æ’é˜Ÿå’Œé™æµï¼š

```python
tasks = []
for chapter in chapters:
    task = self.llm_manager.call_with_rate_limit(
        func=self.novel_segmenter.execute,
        provider="deepseek",
        model="deepseek-chat",
        estimated_tokens=estimate_tokens(chapter.content),
        chapter_content=chapter.content,
        ...
    )
    tasks.append(task)

# å¹¶å‘æ‰§è¡Œï¼ˆç®¡ç†å™¨ä¼šè‡ªåŠ¨é™æµï¼‰
results = await asyncio.gather(*tasks)
```

### 3. é…ç½®è°ƒä¼˜

æ ¹æ®å®é™…æµ‹è¯•ç»“æœè°ƒæ•´é…ç½®ï¼š

```python
# ä¿å®ˆé…ç½®ï¼ˆé«˜æˆåŠŸç‡ï¼Œä½é€Ÿåº¦ï¼‰
config = {
    "requests_per_minute": 30,
    "max_concurrent": 1,
    "base_retry_delay": 3.0
}

# æ¿€è¿›é…ç½®ï¼ˆé«˜é€Ÿåº¦ï¼Œå¯èƒ½è§¦å‘é™æµï¼‰
config = {
    "requests_per_minute": 100,
    "max_concurrent": 5,
    "base_retry_delay": 1.0
}

# å‡è¡¡é…ç½®ï¼ˆæ¨èï¼‰
config = {
    "requests_per_minute": 50,
    "max_concurrent": 2-3,
    "base_retry_delay": 2.0
}
```

---

## ğŸ› æ•…éšœæ’æŸ¥

### é—®é¢˜1: ä»ç„¶é¢‘ç¹è§¦å‘é™æµ

**åŸå› **ï¼š
- é…ç½®çš„QPMè¿‡é«˜
- å¤šä¸ªè¿›ç¨‹åŒæ—¶ä½¿ç”¨åŒä¸€API key

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. é™ä½QPMé…ç½®
2. å¢åŠ è¯·æ±‚å»¶è¿Ÿ
3. æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–è¿›ç¨‹åœ¨ä½¿ç”¨API

### é—®é¢˜2: å¤„ç†é€Ÿåº¦å¤ªæ…¢

**åŸå› **ï¼š
- é…ç½®è¿‡äºä¿å®ˆ
- å¹¶å‘æ•°å¤ªä½

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. è¿è¡Œæµ‹è¯•å·¥å…·ï¼Œè·å–å®é™…é™æµé˜ˆå€¼
2. æ ¹æ®æµ‹è¯•ç»“æœè°ƒé«˜QPM
3. å¢åŠ å¹¶å‘æ•°

### é—®é¢˜3: é…ç½®æœªç”Ÿæ•ˆ

**åŸå› **ï¼š
- é…ç½®æ–‡ä»¶è·¯å¾„é”™è¯¯
- é…ç½®æœªä¿å­˜

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# æ£€æŸ¥é…ç½®
manager = get_llm_manager()
config = manager.get_config("anthropic", "claude-3-5-sonnet-20241022")
print(config)

# æ‰‹åŠ¨ä¿å­˜é…ç½®
manager._save_configs()
```

---

## ğŸ“ æœ€ä½³å®è·µ

### 1. æµ‹è¯•æ–°æ¨¡å‹

æ¯æ¬¡ä½¿ç”¨æ–°æ¨¡å‹å‰ï¼Œå…ˆè¿è¡Œæµ‹è¯•ï¼š

```bash
python3 scripts/test/test_llm_rate_limits.py
```

### 2. å®šæœŸæ›´æ–°é…ç½®

APIé™æµè§„åˆ™å¯èƒ½å˜åŒ–ï¼Œå»ºè®®ï¼š
- æ¯æœˆé‡æ–°æµ‹è¯•ä¸€æ¬¡
- è®°å½•æµ‹è¯•æ—¥æœŸå’Œç»“æœ
- æ›´æ–°é…ç½®æ–‡ä»¶

### 3. ç›‘æ§ä½¿ç”¨æƒ…å†µ

å®šæœŸæ£€æŸ¥ç»Ÿè®¡ä¿¡æ¯ï¼š

```python
# åœ¨workflowç»“æŸæ—¶è¾“å‡ºç»Ÿè®¡
stats = self.llm_manager.get_all_stats()
logger.info(f"LLMè°ƒç”¨ç»Ÿè®¡: {stats}")
```

### 4. é”™è¯¯æ—¥å¿—åˆ†æ

å…³æ³¨æ—¥å¿—ä¸­çš„é™æµè­¦å‘Šï¼š

```
ğŸš« æ£€æµ‹åˆ°APIé™æµ
âš ï¸ æ‰§è¡Œå¤±è´¥ï¼ˆç¬¬1/4æ¬¡å°è¯•ï¼‰: Error code: 403
â³ ç­‰å¾…4.0ç§’åé‡è¯•...
```

å¦‚æœé¢‘ç¹å‡ºç°ï¼Œéœ€è¦è°ƒæ•´é…ç½®ã€‚

---

## ğŸ¯ ä¸‹ä¸€æ­¥è®¡åˆ’

### çŸ­æœŸ
- âœ… å®Œæˆæ ¸å¿ƒç³»ç»Ÿå®ç°
- â³ æµ‹è¯•æ‰€æœ‰é¢„å®šä¹‰é…ç½®
- â³ é›†æˆåˆ°NovelProcessingWorkflow

### ä¸­æœŸ
- è‡ªé€‚åº”é™æµï¼ˆæ ¹æ®æˆåŠŸç‡åŠ¨æ€è°ƒæ•´ï¼‰
- æˆæœ¬è¿½è¸ªä¸é¢„ç®—æ§åˆ¶
- Web Dashboardå¯è§†åŒ–

### é•¿æœŸ
- å¤šè´¦æˆ·è´Ÿè½½å‡è¡¡
- åˆ†å¸ƒå¼é™æµï¼ˆå¤šæœºå™¨ååŒï¼‰
- æœºå™¨å­¦ä¹ é¢„æµ‹æœ€ä¼˜é…ç½®

---

*æœ€åæ›´æ–°: 2026-02-10*
*ç‰ˆæœ¬: 1.0.0*
