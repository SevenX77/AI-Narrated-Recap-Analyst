# LLMç®¡ç†å™¨é›†æˆæŒ‡å—

## ğŸ¯ é›†æˆç›®æ ‡

å°†`LLMCallManager`é›†æˆåˆ°`NovelProcessingWorkflow`ï¼Œå®ç°ï¼š
1. ç»Ÿä¸€çš„é™æµæ§åˆ¶
2. è‡ªåŠ¨é‡è¯•æœºåˆ¶
3. å¤šæ¨¡å‹é…ç½®æ”¯æŒ
4. å¹¶å‘æ™ºèƒ½ç®¡ç†

---

## ğŸ“ é›†æˆæ­¥éª¤

### Step 1: æ›´æ–°Workflowåˆå§‹åŒ–

```python
from src.core.llm_rate_limiter import get_llm_manager

class NovelProcessingWorkflow(BaseWorkflow):
    def __init__(self):
        super().__init__()
        
        # åˆå§‹åŒ–å·¥å…·
        self.novel_importer = NovelImporter()
        # ... å…¶ä»–å·¥å…·
        
        # åˆå§‹åŒ–LLMè°ƒç”¨ç®¡ç†å™¨
        self.llm_manager = get_llm_manager()
        
        logger.info(f"âœ… {self.name} åˆå§‹åŒ–å®Œæˆ")
```

### Step 2: æ›¿æ¢åˆ†æ®µè°ƒç”¨

**åŸä»£ç ** (`_segment_single_chapter`):
```python
# ä½¿ç”¨è‡ªå®šä¹‰é‡è¯•é€»è¾‘
if workflow_config.retry_on_error:
    seg_output = await self._retry_with_backoff(
        self.novel_segmenter.execute,
        chapter_content=chapter_content,
        chapter_number=chapter.number,
        chapter_title=chapter.title,
        provider=workflow_config.segmentation_provider,
        max_retries=workflow_config.max_retries,
        base_delay=workflow_config.retry_delay
    )
else:
    seg_output = self.novel_segmenter.execute(...)

# æ‰‹åŠ¨å»¶è¿Ÿ
if workflow_config.request_delay > 0:
    await asyncio.sleep(workflow_config.request_delay)
```

**ä¼˜åŒ–å**:
```python
# ä½¿ç”¨LLMç®¡ç†å™¨ï¼ˆè‡ªåŠ¨é™æµ+é‡è¯•+å»¶è¿Ÿï¼‰
seg_output = await self.llm_manager.call_with_rate_limit(
    func=self.novel_segmenter.execute,
    provider=workflow_config.segmentation_provider,  # "deepseek"
    model="deepseek-chat",  # æˆ–ä»configè·å–
    estimated_tokens=self._estimate_tokens(chapter_content),
    chapter_content=chapter_content,
    chapter_number=chapter.number,
    chapter_title=chapter.title,
    provider=workflow_config.segmentation_provider
)
```

### Step 3: æ›¿æ¢æ ‡æ³¨è°ƒç”¨

**åŸä»£ç ** (`_annotate_single_chapter`):
```python
if workflow_config.retry_on_error:
    result = await self._retry_with_backoff(...)
else:
    result = self.novel_annotator.execute(...)

if workflow_config.request_delay > 0:
    await asyncio.sleep(workflow_config.request_delay)
```

**ä¼˜åŒ–å**:
```python
result = await self.llm_manager.call_with_rate_limit(
    func=self.novel_annotator.execute,
    provider=workflow_config.annotation_provider,
    model="deepseek-chat",
    estimated_tokens=self._estimate_tokens_for_annotation(segmentation_result),
    segmentation_result=segmentation_result,
    enable_functional_tags=workflow_config.enable_functional_tags,
    provider=workflow_config.annotation_provider
)
```

### Step 4: æ·»åŠ Tokenä¼°ç®—æ–¹æ³•

```python
def _estimate_tokens(self, text: str) -> int:
    """
    ä¼°ç®—æ–‡æœ¬çš„tokenæ•°é‡
    
    ä¸­æ–‡: 1å­— â‰ˆ 1.5 tokens
    è‹±æ–‡: 1è¯ â‰ˆ 1.3 tokens
    """
    chinese_chars = len([c for c in text if '\u4e00' <= c <= '\u9fff'])
    other_chars = len(text) - chinese_chars
    
    # ä¿å®ˆä¼°ç®—ï¼ˆåŒ…å«è¾“å‡ºtokensï¼‰
    input_tokens = int(chinese_chars * 1.5 + other_chars * 0.3)
    output_tokens = int(input_tokens * 0.2)  # å‡è®¾è¾“å‡ºæ˜¯è¾“å…¥çš„20%
    
    return input_tokens + output_tokens

def _estimate_tokens_for_annotation(self, seg_result: ParagraphSegmentationResult) -> int:
    """ä¼°ç®—æ ‡æ³¨ä»»åŠ¡çš„tokenæ•°é‡"""
    total_chars = sum(len(p.content) for p in seg_result.paragraphs)
    return self._estimate_tokens("x" * total_chars)
```

### Step 5: åˆ é™¤æ—§çš„é‡è¯•é€»è¾‘

ç°åœ¨å¯ä»¥åˆ é™¤`_retry_with_backoff`æ–¹æ³•ï¼Œå› ä¸ºLLMç®¡ç†å™¨å·²æä¾›ç»Ÿä¸€çš„é‡è¯•åŠŸèƒ½ã€‚

```python
# âŒ åˆ é™¤è¿™ä¸ªæ–¹æ³•
async def _retry_with_backoff(self, func, *args, max_retries=3, base_delay=2.0, **kwargs):
    ...
```

### Step 6: ç®€åŒ–Config

ç°åœ¨è¿™äº›é…ç½®é¡¹å¯ä»¥ç§»é™¤ï¼ˆç”±LLMç®¡ç†å™¨ç»Ÿä¸€ç®¡ç†ï¼‰ï¼š

```python
# âŒ å¯ä»¥ç§»é™¤ï¼ˆå¯é€‰ï¼‰
class NovelProcessingConfig:
    retry_on_error: bool = True
    max_retries: int = 3
    retry_delay: float = 2.0
    request_delay: float = 1.5
```

ä¿ç•™è¿™äº›é…ç½®ä¹Ÿå¯ä»¥ï¼Œä½œä¸ºoverrideé€‰é¡¹ã€‚

---

## ğŸ”„ å®Œæ•´ä»£ç ç¤ºä¾‹

### ä¿®æ”¹åçš„`_segment_single_chapter`

```python
async def _segment_single_chapter(
    self,
    chapter: ChapterInfo,
    novel_content: str,
    workflow_config: NovelProcessingConfig
) -> ParagraphSegmentationResult:
    """åˆ†æ®µå•ä¸ªç« èŠ‚ï¼ˆä½¿ç”¨LLMç®¡ç†å™¨ï¼‰"""
    logger.info(f"   å¤„ç†ç« èŠ‚ {chapter.number}: {chapter.title}")
    
    # æå–ç« èŠ‚å†…å®¹
    lines = novel_content.split('\n')
    end_line = chapter.end_line if chapter.end_line is not None else len(lines)
    chapter_content = '\n'.join(lines[chapter.start_line:end_line])
    
    # ä½¿ç”¨LLMç®¡ç†å™¨è°ƒç”¨ï¼ˆè‡ªåŠ¨é™æµ+é‡è¯•ï¼‰
    seg_output = await self.llm_manager.call_with_rate_limit(
        func=self.novel_segmenter.execute,
        provider=workflow_config.segmentation_provider,
        model="deepseek-chat",  # æˆ–ä»configè·å–
        estimated_tokens=self._estimate_tokens(chapter_content),
        chapter_content=chapter_content,
        chapter_number=chapter.number,
        chapter_title=chapter.title,
        provider=workflow_config.segmentation_provider
    )
    
    return seg_output.json_result
```

---

## âš™ï¸ é…ç½®æ–‡ä»¶ç®¡ç†

### é…ç½®æ–‡ä»¶ä½ç½®
```
data/llm_configs.json
```

### é…ç½®æ–‡ä»¶æ ¼å¼

```json
{
  "anthropic_claude": {
    "provider": "anthropic",
    "model": "claude-3-5-sonnet-20241022",
    "requests_per_minute": 50,
    "tokens_per_minute": 40000,
    "max_concurrent": 3,
    "max_retries": 3,
    "base_retry_delay": 2.0,
    "is_tested": false,
    "last_test_date": null,
    "test_notes": "é»˜è®¤é…ç½®ï¼Œå¾…æµ‹è¯•éªŒè¯"
  },
  "deepseek_chat": {
    "provider": "deepseek",
    "model": "deepseek-chat",
    "requests_per_minute": 60,
    "max_concurrent": 2,
    "max_retries": 3,
    "base_retry_delay": 3.0,
    "is_tested": false,
    "test_notes": "é»˜è®¤é…ç½®ï¼Œå¾…æµ‹è¯•éªŒè¯"
  }
}
```

### æ‰‹åŠ¨ç¼–è¾‘é…ç½®

å¯ä»¥ç›´æ¥ç¼–è¾‘`data/llm_configs.json`ï¼Œä¸‹æ¬¡è¿è¡Œæ—¶ä¼šè‡ªåŠ¨åŠ è½½ã€‚

---

## ğŸ§ª æµ‹è¯•ä¸éªŒè¯

### 1. è¿è¡Œé›†æˆæ¼”ç¤º

```bash
python3 scripts/test/test_llm_manager_integration.py
```

è¾“å‡ºç¤ºä¾‹ï¼š
```
âœ… è¯·æ±‚1: Response to: Test prompt 1...
âœ… è¯·æ±‚2: Response to: Test prompt 2...
ğŸš« æ£€æµ‹åˆ°APIé™æµ
âš ï¸ æ‰§è¡Œå¤±è´¥ï¼ˆç¬¬1/4æ¬¡å°è¯•ï¼‰: Error code: 403
â³ ç­‰å¾…4.0ç§’åé‡è¯•...
âœ… è¯·æ±‚3: Response to: Test prompt 3...

ğŸ“Š æˆåŠŸç‡: 10/10
```

### 2. æµ‹è¯•å®é™…APIé™æµ

éœ€è¦åˆ›å»ºå®é™…çš„APIè°ƒç”¨å‡½æ•°ï¼š

```python
# åœ¨scripts/test/test_llm_rate_limits.pyä¸­å®ç°

def test_deepseek_actual():
    """æµ‹è¯•DeepSeekå®é™…é™æµ"""
    from openai import OpenAI
    
    client = OpenAI(
        api_key=os.getenv("DEEPSEEK_API_KEY"),
        base_url="https://api.deepseek.com"
    )
    
    response = client.chat.completions.create(
        model="deepseek-chat",
        messages=[{"role": "user", "content": "ä½ å¥½"}],
        max_tokens=50
    )
    
    return response

# è¿è¡Œæµ‹è¯•
tester = LLMRateLimitTester()
result = await tester.test_provider(
    provider="deepseek",
    model="deepseek-chat",
    test_func=test_deepseek_actual,
    test_duration=120,  # æµ‹è¯•2åˆ†é’Ÿ
    ramp_up_delay=3.0
)
```

### 3. æŸ¥çœ‹æµ‹è¯•ç»“æœ

æµ‹è¯•å®Œæˆåä¼šç”Ÿæˆï¼š
- `data/llm_configs.json` - æ›´æ–°åçš„é…ç½®
- `data/llm_rate_limit_test_results.json` - æµ‹è¯•ç»“æœè®°å½•

---

## ğŸ“ˆ ä¸åŒæ¨¡å‹çš„ç­–ç•¥

### DeepSeekï¼ˆå½“å‰ä¸»åŠ›ï¼‰
```python
{
    "requests_per_minute": 30,  # ä¿å®ˆä¼°è®¡
    "max_concurrent": 1,  # ä¸²è¡ŒåŒ–
    "base_retry_delay": 3.0,  # è¾ƒé•¿å»¶è¿Ÿ
    "test_notes": "ç»éªŒå€¼ï¼šQPM>30ä¼šé¢‘ç¹è§¦å‘é™æµ"
}
```

### Claudeï¼ˆAnthropicï¼‰
```python
{
    "requests_per_minute": 50,  # ä»˜è´¹è´¦æˆ·
    "tokens_per_minute": 40000,
    "max_concurrent": 3,
    "base_retry_delay": 2.0
}
```

### GPT-4ï¼ˆOpenAIï¼‰
```python
{
    "requests_per_minute": 500,  # ä»˜è´¹è´¦æˆ·
    "tokens_per_minute": 10000,
    "max_concurrent": 5,
    "base_retry_delay": 1.0
}
```

---

## ğŸ’¡ ä½¿ç”¨å»ºè®®

### 1. é¦–æ¬¡ä½¿ç”¨æ–°æ¨¡å‹

```bash
# 1. è¿è¡Œæµ‹è¯•å·¥å…·
python3 scripts/test/test_llm_rate_limits.py

# 2. é€‰æ‹©"æµ‹è¯•å•ä¸ªæä¾›å•†"
# 3. è¾“å…¥å®é™…çš„APIè°ƒç”¨å‡½æ•°
# 4. ç­‰å¾…æµ‹è¯•å®Œæˆ
# 5. æ ¹æ®å»ºè®®æ›´æ–°é…ç½®
```

### 2. ç›‘æ§ä½¿ç”¨æƒ…å†µ

åœ¨workflowç»“æŸæ—¶è¾“å‡ºç»Ÿè®¡ï¼š

```python
# åœ¨runæ–¹æ³•æœ€å
stats = self.llm_manager.get_all_stats()
logger.info(f"ğŸ“Š LLMè°ƒç”¨ç»Ÿè®¡: {json.dumps(stats, indent=2)}")
```

### 3. è°ƒä¼˜ç­–ç•¥

| è§‚å¯Ÿåˆ°çš„ç°è±¡ | è°ƒæ•´å»ºè®® |
|------------|---------|
| é¢‘ç¹è§¦å‘é™æµ | é™ä½QPMæˆ–max_concurrent |
| å¤„ç†é€Ÿåº¦å¤ªæ…¢ | æé«˜QPMæˆ–max_concurrent |
| æˆåŠŸç‡>99% | å¯ä»¥æ›´æ¿€è¿›é…ç½® |
| æˆåŠŸç‡<90% | éœ€è¦æ›´ä¿å®ˆé…ç½® |

---

## ğŸ”§ é«˜çº§åŠŸèƒ½

### 1. è‡ªå®šä¹‰é™æµç­–ç•¥

```python
# åˆ›å»ºè‡ªå®šä¹‰é…ç½®
custom_config = LLMRateLimitConfig(
    provider="my_provider",
    model="my_model",
    requests_per_minute=20,
    max_concurrent=1,
    max_retries=5,
    base_retry_delay=5.0,
    rate_limit_errors=["403", "429", "quota_exceeded"]
)

# æ·»åŠ åˆ°ç®¡ç†å™¨
manager = get_llm_manager()
manager.configs["my_provider_my_model"] = custom_config
manager.limiters["my_provider_my_model"] = RateLimiter(custom_config)
manager._save_configs()
```

### 2. å®æ—¶è°ƒæ•´é…ç½®

```python
# è¿è¡Œæ—¶åŠ¨æ€è°ƒæ•´
manager.update_config(
    "deepseek_chat",
    requests_per_minute=40,  # é™ä½QPM
    max_concurrent=1  # ä¸²è¡ŒåŒ–
)
```

### 3. å¤šè´¦æˆ·æ”¯æŒ

```python
# ä¸ºåŒä¸€æ¨¡å‹é…ç½®å¤šä¸ªè´¦æˆ·
configs = {
    "deepseek_account1": LLMRateLimitConfig(
        provider="deepseek",
        model="deepseek-chat",
        requests_per_minute=60,
        ...
    ),
    "deepseek_account2": LLMRateLimitConfig(
        provider="deepseek",
        model="deepseek-chat",
        requests_per_minute=60,
        ...
    )
}

# è½®è¯¢ä½¿ç”¨
account = f"deepseek_account{(i % 2) + 1}"
result = await manager.call_with_rate_limit(
    func=...,
    provider="deepseek",
    model=account,
    ...
)
```

---

## ğŸš€ å®æˆ˜æ¡ˆä¾‹

### æ¡ˆä¾‹1: å¤„ç†100ç« å°è¯´

**åŸæ–¹æ¡ˆ**ï¼ˆæ— ç®¡ç†å™¨ï¼‰:
- é…ç½®: å¹¶å‘=3, æ— é‡è¯•
- ç»“æœ: é¢‘ç¹è§¦å‘é™æµï¼ŒæˆåŠŸç‡40%
- è€—æ—¶: 10åˆ†é’Ÿ

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼ˆä½¿ç”¨ç®¡ç†å™¨ï¼‰:
```python
config = NovelProcessingConfig(
    max_concurrent_chapters=2,  # Workflowå±‚å¹¶å‘
    # LLMç®¡ç†å™¨ä¼šè¿›ä¸€æ­¥æ§åˆ¶å®é™…å¹¶å‘
)

# LLMé…ç½®ï¼ˆè‡ªåŠ¨åŠ è½½ï¼‰
llm_config = {
    "requests_per_minute": 30,
    "max_concurrent": 1,  # LLMå±‚å¹¶å‘
    "max_retries": 3
}
```

- ç»“æœ: è‡ªåŠ¨é‡è¯•ï¼ŒæˆåŠŸç‡95%+
- è€—æ—¶: 15-20åˆ†é’Ÿ

### æ¡ˆä¾‹2: æ··åˆä½¿ç”¨å¤šä¸ªæ¨¡å‹

```python
# åˆ†æ®µä½¿ç”¨DeepSeekï¼ˆä¾¿å®œï¼‰
seg_output = await self.llm_manager.call_with_rate_limit(
    func=self.novel_segmenter.execute,
    provider="deepseek",
    model="deepseek-chat",
    ...
)

# æ ‡æ³¨ä½¿ç”¨Claudeï¼ˆè´¨é‡é«˜ï¼‰
ann_output = await self.llm_manager.call_with_rate_limit(
    func=self.novel_annotator.execute,
    provider="anthropic",
    model="claude-3-5-sonnet-20241022",
    ...
)

# ç®¡ç†å™¨ä¼šä¸ºæ¯ä¸ªæ¨¡å‹ç‹¬ç«‹ç®¡ç†é™æµ
```

---

## ğŸ“Š ç›‘æ§ä¸ç»Ÿè®¡

### å®æ—¶ç›‘æ§

```python
# æ¯å¤„ç†10ç« è¾“å‡ºä¸€æ¬¡ç»Ÿè®¡
if len(result.segmentation_results) % 10 == 0:
    stats = self.llm_manager.get_all_stats()
    logger.info(f"ğŸ“Š å½“å‰LLMä½¿ç”¨æƒ…å†µ:")
    for model, stat in stats.items():
        logger.info(f"  {model}:")
        logger.info(f"    å¹¶å‘: {stat['current_concurrent']}")
        logger.info(f"    æœ€è¿‘1åˆ†é’Ÿè¯·æ±‚: {stat['requests_last_minute']}")
```

### æœ€ç»ˆæŠ¥å‘Š

```python
# åœ¨workflowç»“æŸæ—¶ç”ŸæˆLLMä½¿ç”¨æŠ¥å‘Š
def _generate_llm_usage_report(self, processing_dir: str):
    """ç”ŸæˆLLMä½¿ç”¨æŠ¥å‘Š"""
    stats = self.llm_manager.get_all_stats()
    
    report = f"""# LLMä½¿ç”¨ç»Ÿè®¡æŠ¥å‘Š

## å„æ¨¡å‹ä½¿ç”¨æƒ…å†µ

"""
    for model, stat in stats.items():
        report += f"""### {model}
- å½“å‰å¹¶å‘: {stat['current_concurrent']}
- æœ€è¿‘1åˆ†é’Ÿè¯·æ±‚: {stat['requests_last_minute']}
- æœ€è¿‘1å¤©è¯·æ±‚: {stat['requests_last_day']}
- æœ€è¿‘1åˆ†é’Ÿtokens: {stat['tokens_last_minute']}

"""
    
    filepath = Path(processing_dir) / "reports" / "llm_usage_report.md"
    filepath.write_text(report, encoding='utf-8')
```

---

## âš™ï¸ é…ç½®æ–‡ä»¶ç¤ºä¾‹

### ç”Ÿäº§ç¯å¢ƒé…ç½®

```json
{
  "deepseek_chat": {
    "provider": "deepseek",
    "model": "deepseek-chat",
    "requests_per_minute": 30,
    "max_concurrent": 1,
    "max_retries": 3,
    "base_retry_delay": 3.0,
    "max_retry_delay": 30.0,
    "rate_limit_errors": ["403", "429", "rate limit"],
    "is_tested": true,
    "last_test_date": "2026-02-10",
    "test_notes": "å®é™…æµ‹è¯•ï¼šQPM>30è§¦å‘é™æµï¼Œå»ºè®®QPM=30"
  }
}
```

### æµ‹è¯•ç¯å¢ƒé…ç½®

```json
{
  "deepseek_chat": {
    "requests_per_minute": 10,
    "max_concurrent": 1,
    "max_retries": 5,
    "base_retry_delay": 5.0,
    "test_notes": "æµ‹è¯•ç¯å¢ƒï¼šæåº¦ä¿å®ˆé…ç½®"
  }
}
```

---

## ğŸ“ å…³é”®æ¦‚å¿µ

### QPM vs max_concurrent

- **QPM (Queries Per Minute)**: æ¯åˆ†é’Ÿæœ€å¤§è¯·æ±‚æ•°ï¼ˆAPIæä¾›å•†é™åˆ¶ï¼‰
- **max_concurrent**: æœ€å¤§å¹¶å‘æ•°ï¼ˆåŒæ—¶è¿›è¡Œçš„è¯·æ±‚æ•°ï¼‰

**å…³ç³»**ï¼š
```
å®é™…QPM = min(é…ç½®QPM, max_concurrent * 60/å¹³å‡å“åº”æ—¶é—´)

ä¾‹å¦‚ï¼š
- é…ç½®QPM=60
- max_concurrent=3
- å¹³å‡å“åº”æ—¶é—´=10ç§’
- å®é™…QPM = min(60, 3 * 60/10) = min(60, 18) = 18
```

**å»ºè®®**ï¼š
- QPMè®¾ç½®ä¸ºAPIé™åˆ¶çš„80%ï¼ˆç•™ä½™é‡ï¼‰
- max_concurrentæ ¹æ®å¹³å‡å“åº”æ—¶é—´è°ƒæ•´

### æ»‘åŠ¨çª—å£ vs ä»¤ç‰Œæ¡¶

å½“å‰å®ç°ä½¿ç”¨**æ»‘åŠ¨çª—å£ç®—æ³•**ï¼š
- è®°å½•æœ€è¿‘60ç§’çš„æ‰€æœ‰è¯·æ±‚
- è®¡ç®—çª—å£å†…è¯·æ±‚æ•°æ˜¯å¦è¶…è¿‡QPM
- ä¼˜ç‚¹ï¼šç²¾ç¡®
- ç¼ºç‚¹ï¼šå†…å­˜å ç”¨ç¨é«˜

å¦‚æœéœ€è¦æ›´é«˜æ€§èƒ½ï¼Œå¯æ”¹ç”¨**ä»¤ç‰Œæ¡¶ç®—æ³•**ã€‚

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆè®¾ç½®äº†QPM=60ï¼Œä½†å®é™…åªæœ‰20ï¼Ÿ

A: æ£€æŸ¥ä»¥ä¸‹å› ç´ ï¼š
1. `max_concurrent`æ˜¯å¦å¤ªä½ï¼Ÿ
2. å¹³å‡å“åº”æ—¶é—´æ˜¯å¦å¾ˆé•¿ï¼Ÿ
3. æ˜¯å¦æœ‰å…¶ä»–é™åˆ¶ï¼ˆTPM, QPDï¼‰ï¼Ÿ

### Q2: å¦‚ä½•çŸ¥é“å½“å‰é…ç½®æ˜¯å¦åˆç†ï¼Ÿ

A: è¿è¡Œæµ‹è¯•å·¥å…·ï¼š
```bash
python3 scripts/test/test_llm_rate_limits.py
```

### Q3: é…ç½®æ›´æ–°åä¸ç”Ÿæ•ˆï¼Ÿ

A: éœ€è¦é‡å¯è¿›ç¨‹ï¼Œæˆ–è°ƒç”¨ï¼š
```python
manager._load_configs()  # é‡æ–°åŠ è½½é…ç½®
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- `src/core/llm_rate_limiter.py` - æ ¸å¿ƒå®ç°
- `scripts/test/test_llm_rate_limits.py` - æµ‹è¯•å·¥å…·
- `scripts/test/test_llm_manager_integration.py` - é›†æˆæ¼”ç¤º
- `docs/core/LLM_RATE_LIMIT_SYSTEM.md` - ç³»ç»Ÿæ–‡æ¡£

---

*æœ€åæ›´æ–°: 2026-02-10*
*ç‰ˆæœ¬: 1.0.0*
