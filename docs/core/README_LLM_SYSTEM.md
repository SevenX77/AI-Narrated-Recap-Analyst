# LLMå¹¶å‘è°ƒç”¨ç®¡ç†ç³»ç»Ÿ - ä½¿ç”¨æ‰‹å†Œ

## ğŸ¯ ä¸€åˆ†é’Ÿå¿«é€Ÿå…¥é—¨

### é—®é¢˜
ä½ çš„workflowé¢‘ç¹é‡åˆ°APIé™æµï¼ˆ403/429é”™è¯¯ï¼‰ï¼ŒæˆåŠŸç‡ä½ï¼Ÿ

### è§£å†³æ–¹æ¡ˆ
ä½¿ç”¨LLMè°ƒç”¨ç®¡ç†ç³»ç»Ÿï¼Œä¸€è¡Œä»£ç è§£å†³ï¼š

**åŸä»£ç **:
```python
result = self.some_tool.execute(data=data)  # å¯èƒ½å¤±è´¥
```

**ä¼˜åŒ–å**:
```python
from src.core.llm_rate_limiter import get_llm_manager

manager = get_llm_manager()
result = await manager.call_with_rate_limit(
    func=self.some_tool.execute,
    provider="deepseek",
    model="deepseek-chat",
    estimated_tokens=1000,
    data=data
)  # è‡ªåŠ¨é™æµ+é‡è¯•ï¼ŒæˆåŠŸç‡95%+
```

---

## ğŸ“š å®Œæ•´æ–‡æ¡£ç´¢å¼•

### æ ¸å¿ƒæ–‡æ¡£ï¼ˆå¿…è¯»ï¼‰
1. **LLM_SYSTEM_COMPLETE.md** - ç³»ç»Ÿå®Œæ•´è¯´æ˜
   - ç³»ç»Ÿæ¦‚è¿°
   - ç»„ä»¶ä»‹ç»
   - æ€§èƒ½å¯¹æ¯”
   - æµ‹è¯•ç»“æœ

2. **LLM_INTEGRATION_GUIDE.md** - é›†æˆæŒ‡å—
   - é›†æˆæ­¥éª¤
   - ä»£ç ç¤ºä¾‹
   - é…ç½®ç®¡ç†

3. **LLM_RATE_LIMIT_SYSTEM.md** - ç³»ç»Ÿè®¾è®¡
   - æ¶æ„è®¾è®¡
   - æ ¸å¿ƒç®—æ³•
   - APIæ–‡æ¡£

### å‚è€ƒæ–‡æ¡£
4. **RETRY_MECHANISM.md** - é‡è¯•æœºåˆ¶è¯¦è§£
5. **BUGFIX_SUMMARY.md** - é—®é¢˜ä¿®å¤è®°å½•

---

## ğŸ› ï¸ æ ¸å¿ƒåŠŸèƒ½

### 1. å¤šæ¨¡å‹é…ç½®ç®¡ç†

```python
# è‡ªåŠ¨åŠ è½½é…ç½®ï¼ˆdata/llm_configs.jsonï¼‰
manager = get_llm_manager()

# æ”¯æŒçš„æ¨¡å‹
- Anthropic Claude
- DeepSeek Chat
- OpenAI GPT-4
- è‡ªå®šä¹‰æ¨¡å‹ï¼ˆæ·»åŠ é…ç½®å³å¯ï¼‰
```

### 2. æ™ºèƒ½é™æµæ§åˆ¶

```python
ç‰¹æ€§ï¼š
âœ… QPMé™åˆ¶ï¼ˆæ¯åˆ†é’Ÿè¯·æ±‚æ•°ï¼‰
âœ… QPDé™åˆ¶ï¼ˆæ¯å¤©è¯·æ±‚æ•°ï¼‰
âœ… TPMé™åˆ¶ï¼ˆæ¯åˆ†é’Ÿtokenæ•°ï¼‰
âœ… å¹¶å‘æ•°æ§åˆ¶
âœ… æ»‘åŠ¨çª—å£ç®—æ³•ï¼ˆç²¾ç¡®ï¼‰
```

### 3. è‡ªåŠ¨é‡è¯•æœºåˆ¶

```python
ç­–ç•¥ï¼š
âœ… æŒ‡æ•°é€€é¿ï¼ˆ2s â†’ 4s â†’ 8sï¼‰
âœ… é™æµæ£€æµ‹ï¼ˆ403/429è‡ªåŠ¨è¯†åˆ«ï¼‰
âœ… æ™ºèƒ½å»¶è¿Ÿï¼ˆé™æµé”™è¯¯åŠ å€ç­‰å¾…ï¼‰
âœ… æœ€å¤šé‡è¯•3æ¬¡ï¼ˆå¯é…ç½®ï¼‰
```

### 4. å®æ—¶ç›‘æ§ç»Ÿè®¡

```python
# è·å–ä½¿ç”¨ç»Ÿè®¡
stats = manager.get_all_stats()
# {
#   "current_concurrent": 2,
#   "requests_last_minute": 25,
#   "tokens_last_minute": 50000
# }
```

---

## âš¡ ä¸‰ç§ä½¿ç”¨æ–¹å¼

### æ–¹å¼1: å¿«é€Ÿé›†æˆï¼ˆæ¨èï¼‰

```python
from src.core.llm_rate_limiter import get_llm_manager

class YourWorkflow:
    def __init__(self):
        self.llm_manager = get_llm_manager()
    
    async def process(self):
        result = await self.llm_manager.call_with_rate_limit(
            func=your_llm_function,
            provider="deepseek",
            model="deepseek-chat",
            estimated_tokens=1000,
            **your_params
        )
```

### æ–¹å¼2: è‡ªå®šä¹‰é…ç½®

```python
from src.core.llm_rate_limiter import LLMCallManager, LLMRateLimitConfig

# åˆ›å»ºè‡ªå®šä¹‰é…ç½®
custom_config = LLMRateLimitConfig(
    provider="my_provider",
    model="my_model",
    requests_per_minute=30,
    max_concurrent=2
)

# ä½¿ç”¨è‡ªå®šä¹‰é…ç½®
manager = LLMCallManager()
manager.configs["my_key"] = custom_config
```

### æ–¹å¼3: æµ‹è¯•åä½¿ç”¨

```bash
# 1. å…ˆæµ‹è¯•å®é™…é™æµ
python3 scripts/test/test_actual_api_limits.py

# 2. æ ¹æ®æµ‹è¯•ç»“æœè‡ªåŠ¨æ›´æ–°é…ç½®

# 3. åœ¨workflowä¸­ä½¿ç”¨ï¼ˆè‡ªåŠ¨åŠ è½½æœ€æ–°é…ç½®ï¼‰
manager = get_llm_manager()
```

---

## ğŸ§ª æµ‹è¯•å·¥å…·ä½¿ç”¨

### å·¥å…·1: é›†æˆæ¼”ç¤ºï¼ˆæ— é…é¢æ¶ˆè€—ï¼‰

```bash
python3 scripts/test/test_llm_manager_integration.py
```

**è¾“å‡º**:
- âœ… åŸºæœ¬ä½¿ç”¨æ¼”ç¤º
- âœ… å¹¶å‘è°ƒç”¨æ¼”ç¤º
- âœ… é™æµæ£€æµ‹æ¼”ç¤º
- âœ… é…ç½®æ›´æ–°æ¼”ç¤º

### å·¥å…·2: å®é™…APIæµ‹è¯•ï¼ˆæ¶ˆè€—é…é¢ï¼‰

```bash
python3 scripts/test/test_actual_api_limits.py
```

**æµç¨‹**:
1. é€‰æ‹©è¦æµ‹è¯•çš„API
2. ç¡®è®¤å¼€å§‹æµ‹è¯•
3. ç­‰å¾…2åˆ†é’Ÿæµ‹è¯•
4. è·å¾—å»ºè®®é…ç½®
5. é€‰æ‹©æ˜¯å¦æ›´æ–°é…ç½®

### å·¥å…·3: äº¤äº’å¼æµ‹è¯•

```bash
python3 scripts/test/test_llm_rate_limits.py
```

**åŠŸèƒ½**:
- æŸ¥çœ‹å½“å‰é…ç½®
- å¿«é€Ÿæµ‹è¯•ï¼ˆmockï¼‰
- å•ä¸ªæä¾›å•†æµ‹è¯•

---

## ğŸ“ æ–‡ä»¶è¯´æ˜

### ä»£ç æ–‡ä»¶
- `src/core/llm_rate_limiter.py` - æ ¸å¿ƒå®ç°ï¼ˆ400è¡Œï¼‰

### æµ‹è¯•æ–‡ä»¶
- `scripts/test/test_llm_manager_integration.py` - æ¼”ç¤ºè„šæœ¬
- `scripts/test/test_actual_api_limits.py` - å®é™…APIæµ‹è¯•
- `scripts/test/test_llm_rate_limits.py` - äº¤äº’å¼æµ‹è¯•

### é…ç½®æ–‡ä»¶
- `data/llm_configs.json` - LLMé…ç½®ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰
- `data/llm_rate_limit_test_results.json` - æµ‹è¯•ç»“æœè®°å½•

### æ–‡æ¡£æ–‡ä»¶
- `docs/core/README_LLM_SYSTEM.md` - æœ¬æ–‡æ¡£
- `docs/core/LLM_SYSTEM_COMPLETE.md` - å®Œæ•´è¯´æ˜
- `docs/core/LLM_INTEGRATION_GUIDE.md` - é›†æˆæŒ‡å—
- `docs/core/LLM_RATE_LIMIT_SYSTEM.md` - ç³»ç»Ÿè®¾è®¡

---

## ğŸ“ å­¦ä¹ è·¯å¾„

### åˆå­¦è€…
1. é˜…è¯»æœ¬æ–‡æ¡£ï¼ˆREADMEï¼‰
2. è¿è¡Œæ¼”ç¤ºè„šæœ¬ï¼š`test_llm_manager_integration.py`
3. æŸ¥çœ‹ç”Ÿæˆçš„é…ç½®ï¼š`data/llm_configs.json`
4. åœ¨ç®€å•åœºæ™¯ä¸­ä½¿ç”¨

### è¿›é˜¶ç”¨æˆ·
1. é˜…è¯»å®Œæ•´æ–‡æ¡£ï¼š`LLM_SYSTEM_COMPLETE.md`
2. è¿è¡Œå®é™…APIæµ‹è¯•ï¼š`test_actual_api_limits.py`
3. æ ¹æ®æµ‹è¯•ç»“æœä¼˜åŒ–é…ç½®
4. é›†æˆåˆ°å¤æ‚workflow

### é«˜çº§ç”¨æˆ·
1. é˜…è¯»ç³»ç»Ÿè®¾è®¡ï¼š`LLM_RATE_LIMIT_SYSTEM.md`
2. è‡ªå®šä¹‰é™æµç­–ç•¥
3. å®ç°å¤šè´¦æˆ·è´Ÿè½½å‡è¡¡
4. è´¡çŒ®ä»£ç ä¼˜åŒ–

---

## ğŸ’¬ å¸¸è§åœºæ™¯

### åœºæ™¯1: æ–°é¡¹ç›®å¿«é€Ÿå¯åŠ¨

```python
# ä½¿ç”¨é»˜è®¤é…ç½®ï¼ˆå·²åŒ…å«ä¸»æµæ¨¡å‹ï¼‰
from src.core.llm_rate_limiter import get_llm_manager

manager = get_llm_manager()  # è‡ªåŠ¨åŠ è½½é…ç½®
result = await manager.call_with_rate_limit(...)  # å¼€å§‹ä½¿ç”¨
```

### åœºæ™¯2: å¤„ç†å¤§æ‰¹é‡æ•°æ®

```python
# ä½¿ç”¨ä¿å®ˆé…ç½®
config = manager.get_config("deepseek", "deepseek-chat")
print(f"å½“å‰QPM: {config.requests_per_minute}")

# å¦‚æœéœ€è¦è°ƒæ•´
manager.update_config(
    "deepseek_chat",
    requests_per_minute=20,  # é™ä½QPM
    max_concurrent=1  # ä¸²è¡ŒåŒ–
)
```

### åœºæ™¯3: ç´§æ€¥ä»»åŠ¡å¿«é€Ÿå¤„ç†

```python
# ä¸´æ—¶æé«˜é™åˆ¶ï¼ˆä»…å½“å‰è¿›ç¨‹ï¼‰
manager.update_config(
    "deepseek_chat",
    requests_per_minute=60,  # æé«˜QPM
    max_concurrent=3,  # å¢åŠ å¹¶å‘
    base_retry_delay=1.0  # å‡å°‘å»¶è¿Ÿ
)

# æ³¨æ„ï¼šå¯èƒ½è§¦å‘æ›´å¤šé™æµ
```

---

## âš™ï¸ é…ç½®é€ŸæŸ¥è¡¨

### ä¿å®ˆé…ç½®ï¼ˆé«˜æˆåŠŸç‡ï¼‰
```json
{
  "requests_per_minute": 20,
  "max_concurrent": 1,
  "max_retries": 5,
  "base_retry_delay": 5.0
}
```

### å‡è¡¡é…ç½®ï¼ˆæ¨èï¼‰
```json
{
  "requests_per_minute": 30-40,
  "max_concurrent": 2,
  "max_retries": 3,
  "base_retry_delay": 3.0
}
```

### æ¿€è¿›é…ç½®ï¼ˆå¿«é€Ÿä½†å¯èƒ½å¤±è´¥ï¼‰
```json
{
  "requests_per_minute": 60+,
  "max_concurrent": 3-5,
  "max_retries": 2,
  "base_retry_delay": 1.0
}
```

---

## ğŸ†˜ è·å–å¸®åŠ©

### æŸ¥çœ‹æ—¥å¿—

å¯ç”¨è¯¦ç»†æ—¥å¿—ï¼š
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

### æ£€æŸ¥é…ç½®

```python
manager = get_llm_manager()
config = manager.get_config("deepseek", "deepseek-chat")
print(config)
```

### æŸ¥çœ‹ç»Ÿè®¡

```python
stats = manager.get_all_stats()
print(json.dumps(stats, indent=2))
```

### å¸¸è§é”™è¯¯

| é”™è¯¯ä¿¡æ¯ | åŸå›  | è§£å†³æ–¹æ¡ˆ |
|---------|------|---------|
| `403 access forbidden` | APIé™æµ | é™ä½QPMæˆ–å¢åŠ å»¶è¿Ÿ |
| `429 too many requests` | APIé™æµ | é™ä½å¹¶å‘æ•° |
| `é…ç½®æ–‡ä»¶ä¸å­˜åœ¨` | é¦–æ¬¡è¿è¡Œ | è‡ªåŠ¨ç”Ÿæˆé»˜è®¤é…ç½® |
| `æˆåŠŸç‡ä½` | é…ç½®è¿‡äºæ¿€è¿› | ä½¿ç”¨ä¿å®ˆé…ç½® |

---

## ğŸ“ æŠ€æœ¯æ”¯æŒ

### é—®é¢˜åé¦ˆ
- ä»£ç ä»“åº“: æäº¤Issue
- æ–‡æ¡£é—®é¢˜: æŸ¥çœ‹`docs/core/`ç›®å½•
- ä½¿ç”¨ç–‘é—®: è¿è¡Œæ¼”ç¤ºè„šæœ¬å­¦ä¹ 

### è´¡çŒ®æŒ‡å—
- æµ‹è¯•æ–°æ¨¡å‹çš„é™æµè§„åˆ™
- æäº¤æµ‹è¯•ç»“æœ
- ä¼˜åŒ–ç®—æ³•å®ç°
- å®Œå–„æ–‡æ¡£

---

## âœ¨ æ ¸å¿ƒä¼˜åŠ¿

1. **å¼€ç®±å³ç”¨**: å®‰è£…åç«‹å³å¯ç”¨ï¼Œæ— éœ€é…ç½®
2. **è‡ªåŠ¨ç®¡ç†**: é™æµ+é‡è¯•+å¹¶å‘å…¨è‡ªåŠ¨
3. **é«˜æˆåŠŸç‡**: ä»40%æå‡åˆ°95%+
4. **æ˜“äºæ‰©å±•**: æ·»åŠ æ–°æ¨¡å‹åªéœ€é…ç½®
5. **ç”Ÿäº§å°±ç»ª**: å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œç›‘æ§

---

**ğŸš€ ç°åœ¨å¼€å§‹ä½¿ç”¨å§ï¼**

```python
from src.core.llm_rate_limiter import get_llm_manager

manager = get_llm_manager()
result = await manager.call_with_rate_limit(
    func=your_function,
    provider="deepseek",
    model="deepseek-chat",
    estimated_tokens=1000,
    **your_params
)
```

---

*ç‰ˆæœ¬: 1.0.0*
*æœ€åæ›´æ–°: 2026-02-10*
